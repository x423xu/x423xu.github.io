<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>visual text review | Xiaoyu&#39;s blog</title>
<meta name="keywords" content="paper-reading">
<meta name="description" content="Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods Abstract Focus on ten prominent tasks that integrate language and vision by discussing their problem formulation, methods, existing datasets, evaluation measures, and compare results with the SOTA methods.
Introduction Multimodal learning models: generate comprehensible but concise and grammatically well-formed descriptions of the visual content, or vice versa by generating the visual content for a given textual description in a natural language of choice, identify objets in the visual content and infer their relationships to reson about, or answer arbitrary questions about them, navigate through and environment by leveraging input from both vision and natural language instructions, translate textual content from one language to another while leveraging the visual content for sense disambiguation, generate stories about the visual content, and so on.">
<meta name="author" content="">
<link rel="canonical" href="https://x423xu.github.io/posts/2022-04-18-visual&#43;text-review/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css" integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://x423xu.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://x423xu.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://x423xu.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://x423xu.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://x423xu.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<script>
    MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true
        },
        options: {
            skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        }
    };

    window.addEventListener('load', (event) => {
        document.querySelectorAll("mjx-container").forEach(function (x) {
            x.parentElement.classList += 'has-jax'
        })
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta property="og:title" content="visual text review" />
<meta property="og:description" content="Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods Abstract Focus on ten prominent tasks that integrate language and vision by discussing their problem formulation, methods, existing datasets, evaluation measures, and compare results with the SOTA methods.
Introduction Multimodal learning models: generate comprehensible but concise and grammatically well-formed descriptions of the visual content, or vice versa by generating the visual content for a given textual description in a natural language of choice, identify objets in the visual content and infer their relationships to reson about, or answer arbitrary questions about them, navigate through and environment by leveraging input from both vision and natural language instructions, translate textual content from one language to another while leveraging the visual content for sense disambiguation, generate stories about the visual content, and so on." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://x423xu.github.io/posts/2022-04-18-visual&#43;text-review/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-04-18T10:52:07+00:00" />
<meta property="article:modified_time" content="2022-04-18T10:52:07+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="visual text review"/>
<meta name="twitter:description" content="Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods Abstract Focus on ten prominent tasks that integrate language and vision by discussing their problem formulation, methods, existing datasets, evaluation measures, and compare results with the SOTA methods.
Introduction Multimodal learning models: generate comprehensible but concise and grammatically well-formed descriptions of the visual content, or vice versa by generating the visual content for a given textual description in a natural language of choice, identify objets in the visual content and infer their relationships to reson about, or answer arbitrary questions about them, navigate through and environment by leveraging input from both vision and natural language instructions, translate textual content from one language to another while leveraging the visual content for sense disambiguation, generate stories about the visual content, and so on."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://x423xu.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "visual text review",
      "item": "https://x423xu.github.io/posts/2022-04-18-visual+text-review/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "visual text review",
  "name": "visual text review",
  "description": "Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods Abstract Focus on ten prominent tasks that integrate language and vision by discussing their problem formulation, methods, existing datasets, evaluation measures, and compare results with the SOTA methods.\nIntroduction Multimodal learning models: generate comprehensible but concise and grammatically well-formed descriptions of the visual content, or vice versa by generating the visual content for a given textual description in a natural language of choice, identify objets in the visual content and infer their relationships to reson about, or answer arbitrary questions about them, navigate through and environment by leveraging input from both vision and natural language instructions, translate textual content from one language to another while leveraging the visual content for sense disambiguation, generate stories about the visual content, and so on.",
  "keywords": [
    "paper-reading"
  ],
  "articleBody": "Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods Abstract Focus on ten prominent tasks that integrate language and vision by discussing their problem formulation, methods, existing datasets, evaluation measures, and compare results with the SOTA methods.\nIntroduction Multimodal learning models: generate comprehensible but concise and grammatically well-formed descriptions of the visual content, or vice versa by generating the visual content for a given textual description in a natural language of choice, identify objets in the visual content and infer their relationships to reson about, or answer arbitrary questions about them, navigate through and environment by leveraging input from both vision and natural language instructions, translate textual content from one language to another while leveraging the visual content for sense disambiguation, generate stories about the visual content, and so on. Applications: visually impaired individuals can be asisted thereby. automatic survelliance autonomous driving human-computer interaction city navigation Task-specific multimodal models image description (Bernardi et al.,2016; Bai \u0026 An, 2018; Hossain et al., 2019) video description generation (Aafaq et al.,2020) visual question answering (Kafle \u0026 Kanan, 2017; Wu et al., 2017) action recognition (Gella \u0026 Keller, 2017) visual semantics (Liu et al., 2019) NLP natual language generation (Gatt \u0026 Krahmer, 2018; Garbacea \u0026 Mei, 2020) NLP commonsense reasoning (Storks et al., 2019) understanding the limitations of the integration of vision and language research (Kafle et al., 2019) Background Computer vision tasks Image as visual information Utilization the tasks where images are used as input the representation of images Tasks Image classification Object localization Object detection Object segmentation Object Identification Instance segmentation Panoptic segmentation Reoresentation self supervised learning (Jing \u0026 Tian, 2019)\nVideo as visual information Utilization knowing the tasks where videos are used as inputs the representation of a video Takss Object tracking Action classification Emotion detection Scene detection Automated editing NLP tasks Tasks understanding language generating language Some of the classical NLP tasks, that are used to comprehend language, are shallow parsing, syntax parsing, semantic role labeling, named entity recognition, entity linking, co-reference resolution, etc.\nRepresentation CV and NLP integration tasks Extension of NLP tasks Visual Description Generation: The goal is to generate a human-readable text snippt that describes the input Visual Storytelling: A sequence of visual inputs is used to generate a narrative summary based on text aligned with them Visual Question Answering: Answer questions about a visual input. Visual Dialog: Aim at creating a meaningful dislog in a natural and conversational language about a visual content. Visual Reeferring Expression: Visual Entailment: An inference task for predicting whether the image semantically entails the text. Multimodal Machine Traslation: Translate from source language to target language by leveraging the visual information as auxiliary modality along with the natural language text in source language. Extension of CV tasks Visual generation: generate visual content by conditioning on input text from a chosen natural language. Visual resoning: It is expected to output a relationship between detected objects by generating an entire visual scene graph. The scene graph is leverage to reason and answer questions about visual content. It can also be used to reason about whether a natural language statement is true or not regarding a visual input. Extension of both NLP and CV tasks Vision-and-Language Navigation: natural language navigation should be interpreted based on visual input. Combine both vision and language. Visual Description Generation and Storytelling generate a textual description when conditioned on visual input\nVisual description generation Aim to generate either a global descriptionor dense captions for a given visual input.\nImage description generation Standard image description generation: generate a sentence-level description of the scene in a given image Dense image description generation: create descriptions at the local object-level in a given image. Image paragraph generation: create paragraphs instead of generating a single simple description. Generated paragraphs are expected to be coherent and contain fine-grained natural language descriptions. Spoken language image description generation: expand the description generation task to work with spoken language, instead o flimiting to only the written forms of language. Stylistic image description generation: add style to the standard image description generation, where the generated descriptions adhere to a specific style. Unseen objects image description generation: leverage images which lack paired descriptions. Generate descriptions for visual object categories previously unseen in image-description corpora. Diverse image description generation: incorporate variety and diversity in the generated captions. Controllable image description generation: select specific objects in an image, defined by a control signal, to generate descriptions. Image caption emendation as generation: build a model to emend both syntactic and semantic errors in the captions. Video description generation Global description generation:\nground sentences that describe actions in the visual information extracted from videos (Motwani \u0026 Mooney, 2012; Regneri et al., 2013) Generate global natural language descriptions for videos with various approaches: leveraging latent topics (Das et al., 2013), corpora knowledge (Krishnamoorthy et al., 2013), graphical models (Rohrbach et al., 2013), and sequence-to-sequence learning (Venugopalan et al., 2015b, 2015a; Donahue et al., 2015; Srivastava et al., 2015; Xu et al., 2016; Ramanishka et al., 2016; Jin et al., 2016), factor graph(Thomason et al., 2014) combines visual detection with language statistics. Seq2seeq based approaches: extra corpora (Venugopalan et al., 2016), soft-attention (Yao et al., 2015), multimodal fusion (Hori et al., 2017), temporal attention (Song et al., 2017), semantic consistency (Gao et al., 2017), residual connections(Li et al., 2019). Incorporation of semantic attributes learned from videos, ensembled-based description generator networks (Shetty et al., 2018), encoder-decoder reconstructors (Wang et al., 2018). Other approaches: combined with entailment generation task (Pasunuru \u0026 Bansal, 2017a), multiple fine-grained actions (Wang et al., 2018b), reinforcement learning (Pasunuru \u0026 Bansal, 2017b), Visual Text correction system (Mazaheri \u0026 Shah, 2018), object relational graph baed encoder, language model decoder (Zhang et al., 2020). Dense video description generation:\nachieve fine-grained video understanding by addressing two sub-problems: (1) localizing events in a video, and (2) generating captions for these localized events. the core challenge, namely the automatic evaluation of video captioning, is still unsolved.\nMovie description generation: input movie clips, align books to movies (Tapaswi et al., 2015; Zhu et al., 2015), movie descriptions (Rohrbach et al., 2015).\nVisual storytelling The task of visual storytelling aims to encode a sequence of images or frames (in the video) to generate a paragraph which is story-like.\nImage storytelling The aim of image storytelling is to generate stories from a sequence of images.\nsemantic coherence is captured in a photo stream. discover semantic embeddings correlations (Yu et al., 2017), incorporated with reinforcement learning (Wang et al., 2018), hierarchically structured reinforced training (Huang et al., 2019), adversarial reward learning Wang et al. (2018a). suffer from repetitiveness, the same objects/events undermine a good story structure. -\u003e inter-sentence diversity was explored with diverse beam search (Hsu et al., 2018). Video storytelling (less explored) In comparison to image storytelling, which only deals with a small sequence of images, the aim of video storytelling is to generate coherent and succinct stories for long videos.\nPinoeer Li et al. (2020) address challenges like diversity in the story and the inherent complexity of video. goal: offer support to people with visual disabilities or technical issues like internet bandwidth limitations. Visual Referring Expression Comprehension and Generation The objective of the task is to ground a natural language expression (e.g. a noun phrase or a longer piece of text) to objects in a visual input.\nImage referring expression comprehension and generation In a natural environment, people use referring expressions to unambiguously identify, indicate, or point to particular objects. This is usually done with a simple phrase or within a larger context (e.g. a sentence). Having a larger context provides better scope for avoiding ambiguity and allows the referential expression to easily map to the target object. However, there can also be other possibilities in which people are asked to describe a target object based on its surrounding objects.\nused to generate, an algorithm generates a referring expression for a given target object which is present in a visual scene approaches: (FitzGerald et al., 2013) tackled the problem from the perspective of density estimation, learn distributions over logical exprssions identifying sets of objects in the world. used to perform comprehension, an algorithm locates in an image the object described by a given referring expression. approaches: Nagaraja et al. (2016) integrate contexts between objects. Multiple Instance Learning (MIL). Hu et al. (2016) leverage a NLP query of the object to localize a target object, integrating spatial configurations and global scene-level contextual information. (Yu et al., 2018) subject appearence, location, and relationship to other objects. (Cirik et al., 2018a) syntactic analysis, build a dynamic computation graph. variational model (Zhang et al., 2018) cross-modal: (Yang et al., 2019) cross-modal relationship inference， 1. highlight objects and relationships connected with a referring, 2. multi-modal semantic contexts. (可以将空间的referring用到depth estimation里面吗)。Recursive Grounding Tree (Hong et al., 2019) binary tree to parse referring expression-\u003e visual reasoning. (Liu et al., 2019), combining visual reasoning with referential expressions. object segmentation based referring expression (Liu et al., 2017). Image referring expression comprehension and generation combination (Mao et al., 2016; Yu et al., 2016) find visual comparison to other objects within an image. (Yu et al., 2017a) a speaker, a listener, and a reinforcer. The speaker generate referring expressions, the listner comprehend referring expressions, the reinforce use a reward function to guide sampling of more discriminative expressions. Video referring expression comprehension and generation Vasudevan et al. (2018) stereo videos to explore temporal-spatial contextual information. Khoreva et al. (2018) language referring expressions to achieve object segmentation. Wang et al. (2020) video grounding with contextual information. Visual question answering, reasoning, and entailment they share the common intention of answering questions when conditioned on a visual input\nVisual question answering The goal of Visual Question Answering (VQA) is to learn a model that comprehends visual content at both the global and local level for finding an association with pairs of questions and answers in the natural language form.\nImage Q\u0026A as Visual Turing Test (Malinowski \u0026 Fritz, 2014; Malinowski et al., 2015; Geman et al., 2015). fill-in-the-blank tasks (Yu et al., 2015), multiple-choice question-answering for images. Address open-ended Image Q\u0026A (Antol et al., 2015; Agrawal et al., 2017), ask free-form natural language question. Binary image Q\u0026A (Zhang et al., 2016). Relate local regions in the images (Zhu et al., 2016) by addressing object-level grounding Interpretability or explainability by overcoming priors (Agrawal et al., 2018). Generate human-interpretable rules that provide better insights (Manjunatha et al., 2019). cycle-consistency (Shah et al., 2019a). outside knowledge (Marino et al., 2019)(Shah et al., 2019b). Multi-task learning, federated learning. Video question answering is to answer natural language questions about videos.\n(Tu et al., 2014) jointly parsing videos with corresponding text to answer queries. open-ended movie Q\u0026A (Tapaswi et al., 2016). fill-in-the-blank questions (Zhu et al., 2017; Mazaheri et al., 2017). (Zeng et al., 2017) free-form Q\u0026A. High-level concept words (Yu et al., 2017b). Attention (Jang et al., 2017). spatio-temporal grounding (Lei et al., 2020) STAGE (Lei et al., 2020), aligned fusion is essential for improving Video Q\u0026A. Visual reasoning is to learn a model that comprehends the visual content by reasoning about it.\nImage reasoning s to answer sophisticated queries by reasoning about the visual world.\n(Johnson et al., 2017a) design diagnostic tests going beyond benchmarks. VQA struggle with comparing the attributes of objects or novel attribute combinations. (Johnson et al., 2017b) program generator. (Hu et al., 2017) predict instance-specific network layouts. Santoro et al. (2017) relation-aware visual features. (Cao et al., 2018) global context reasoning. Mascharka et al. (2018) proposed a set of visual-reasoning primitives. Learning-By-Asking (LBA) (Misra et al., 2018b), mimic natural learning with the goal to make it more data efficient. compositional attention networks (Hudson \u0026 Manning, 2018) explicit and expressive reasoning. neural-symbolic visual question answering (Yi et al., 2018), recover structural scene representation from the image and a program trace from the question. Neuro-Symbolic Concept Learner (NS-CL) (Mao et al., 2019) learns visual concepts, word, and semantic parsing of sentences without explicit supervision. It learns by simply looking at images and reading paired questions and answers. multimodal relation network (Cadène et al., 2019) learn end-to-end reasoning over real images. Aditya et al. (2019) used spatial knowledge to aid visual reasoning, knowledge distillation, relational resoning, probabilistic logical languages. Explainable and explicit neurla modules (Shi et al., 2019) scene graph. Andreas et al. (2016a, 2016b) exploit the compositional linguistic structure of complex questions by forming neural module networks which query about the abstract shapes observed in an image. Compositional question answering (Hudson \u0026 Manning, 2019). Zellers et al. (2019) commonsense knowledge. NLVR (Suhr et al., 2017) and NLVR2 (Suhr et al., 2019) true or false. Video reasoning The goal of COG, Yang et al. (2018), is to address problems related to visual and logical reasoning and memory\nVisual entailment is to learn a model that predicts whether the visual content entails the augmented text along with hypothesis.\nImage entailment Vu et al. (2018), a visually grounded version of the textual entailment where an image is augmented with textual premise and hypothesis. Xie et al. (2019) predicts whether the image semantically entails the text, given image-sentence pairs, the premise is defined by an image. Video entailment (Liu et al., 2020) to infer whether the natural language hypothesis is entailed or contradicted when given a video clip aligned with the subtitles information\nVisual Dialog involves a complex interaction between a human and an artificial agent.\nImage Dialog is to create AI agents that can hold dialog with humans in a natural language of choice about a visual content (Das et al., 2017a), represented by an image. To be more specific, given an image, a history of dialogs, and a question about the image, the goal of an AI agent is to ground the question in the image, infer the context from the history, and then answer the question accurately\n(de Vries et al., 2017) locate an unkown object in the image by asking a swquence of questions. (Mostafazadeh et al., 2017). hold natural-sounding conversations about a shared image. (Lu et al., 2017a) transfer from dialog generation. Seo et al. (2017) attentive memory. (Wu et al., 2018) reinforcement learning and GAN. (Kottur et al., 2018) form explicit and grounded co-reference between nouns and pronouns. (Niu et al., 2019) recursive visual attention. (Zheng et al., 2019) graphical model inference. Guo et al. (2019) builds an image-question-answer synergistic network. (Shekhar et al., 2019) a visually grounded encoder guessing and asking questions. Video dialog is to leverage scene information containing both audio (which can be transcribed as subtitles) and visual frames to hold a dialog (i.e., an exchange) with humans in a natural language of choice about the multimedia content (Alamri et al., 2019b 2019a).\nmultimodal-based video description (Hori et al., 2019). Multimodal Machine Translation (MMT) is to translate natural language sentences that describe visual content (e.g. image) in a source language into a target language by taking the visual content as an additional input to the source language sentences.\nMachine tanslation with image is to translate sentences, that describe an image, in a source language into equivalent sentences in a target language.\nsingle source translation, multisource MMT. multimodal attention (Huang et al., 2016). doubly-attentive decoder incorporated visual features (Calixto et al., 2017). learning to translate, and learning visually grounded representations (Elliott \u0026 Kádár, 2017). noisy image captions for MMT (Schamoni et al., 2018). Machine translation with video (Wang et al., 2019b) is to translate a source language description into the target language equivalent using the video information as additional spatio-temporal context.\nLanguage-to-Vision Generation is to generate visual content given their natural language descriptions.\nLanguage-to-Image Generation sentence-level language-to-image generation: generate images cnditioned on the natural language descriptions. (Mansimov et al., 2016) iteratively draw patches on a canvas, attending to the relavant words in the description. (Reed et al., 2016b) visual concepts could be translated from characters to pixels with conditional gan. (Reed et al., 2016a) what content should be drawn in which location for high quality image generation. (Nguyen et al., 2017) conditioned on image classes. (Dash et al., 2017) condition on both sentence and class information. stackGAN (Zhang et al., 2017, 2019). attention-based GAN (Xu et al., 2018) (Hong et al., 2018) infer the semantic layout of the image. Johnson et al. (2018) used image-specific scene graphs enabling explicitly reasoning about objects and their relationships. Image manipulation TAGAN (Nam et al., 2018) generate semantically manipulated images while preserving text-irrelevant contents. Only regions correspond to the given text are modified. (Zhu et al., 2019) attention generator, discriminator. (Li et al., 2020) designed error correction modules to rectify mismatched attributes and complete the missing contents. Fine-grained image generation (El-Nouby et al., 2018) recurrent image generation, output up to the current step \u0026 past instructions. (Hinz et al., 2019) control location of objects by adding a pathway in an iterative manner. Sequential image generation StoryGAN (Li et al., 2019b) opposite to storytelling. Language-to-Video generation (Li et al., 2018) conditional generative model Vision-and-Language Navigation is to carry out navigation in an environment by interpreting natural language instructions\nImage-and-Language Navigation (Anderson et al., 2018b) an autonomous agent navigate in an environment by interpreting natural language instructions. (Wang et al., 2019a), reinforced cross-modal matching. (Fried et al., 2018) train an action space with an embedded speaker model. Embodied Question Answering (Das et al., 2018a, 2018b). interactive question answering (Gordon et al., 2018). grounded dialog (de Vries et al., 2018) Vison-and-Language pretraining To jointly learn representations using both visual and textual content\nSingle-stream architectures BERT-like (Devlin et al., 2019). VideoBERT (Sun et al., 2019). Bounding Boxes in Text Transformer (B2T2) (Alberti et al., 2019). Unicoder-VL (Li et al., 2020). VL-BERT (Su et al., 2020), VLP (Zhou et al., 2020), OSCAR (Li et al., 202), VinVL (Zhang et al., 2021) can jointly understand and generate from cross-modal data. (Cao et al., 2020) probe. Two-stream architectures two independent encoders for learning visual and text representations. ViLBERT (Lu et al., 2019) and LXMERT (Tan \u0026 Bansal, 2019). -neuro-symbolic reasoning systems (Yi et al., 2018; Vedantam et al., 2019).\n",
  "wordCount" : "2998",
  "inLanguage": "en",
  "datePublished": "2022-04-18T10:52:07Z",
  "dateModified": "2022-04-18T10:52:07Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://x423xu.github.io/posts/2022-04-18-visual+text-review/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Xiaoyu's blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://x423xu.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://x423xu.github.io/" accesskey="h" title="Xiaoyu&#39;s blog (Alt + H)">Xiaoyu&#39;s blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://x423xu.github.io/" title="home">
                    <span>home</span>
                </a>
            </li>
            <li>
                <a href="https://x423xu.github.io/archives/" title="archives">
                    <span>archives</span>
                </a>
            </li>
            <li>
                <a href="https://x423xu.github.io/categories/" title="categories">
                    <span>categories</span>
                </a>
            </li>
            <li>
                <a href="https://x423xu.github.io/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
            <li>
                <a href="https://x423xu.github.io/search/" title="search (Alt &#43; /)" accesskey=/>
                    <span>search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://x423xu.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://x423xu.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      visual text review
    </h1>
    <div class="post-meta"><span title='2022-04-18 10:52:07 +0000 UTC'>April 18, 2022</span>&nbsp;·&nbsp;15 min

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#trends-in-integration-of-vision-and-language-research-a-survey-of-tasks-datasets-and-methods" aria-label="Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods">Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods</a><ul>
                        
                <li>
                    <a href="#abstract" aria-label="Abstract">Abstract</a></li>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#background" aria-label="Background">Background</a><ul>
                        
                <li>
                    <a href="#computer-vision-tasks" aria-label="Computer vision tasks">Computer vision tasks</a><ul>
                        
                <li>
                    <a href="#image-as-visual-information" aria-label="Image as visual information">Image as visual information</a></li>
                <li>
                    <a href="#video-as-visual-information" aria-label="Video as visual information">Video as visual information</a></li></ul>
                </li>
                <li>
                    <a href="#nlp-tasks" aria-label="NLP tasks">NLP tasks</a></li>
                <li>
                    <a href="#cv-and-nlp-integration-tasks" aria-label="CV and NLP integration tasks">CV and NLP integration tasks</a><ul>
                        
                <li>
                    <a href="#extension-of-nlp-tasks" aria-label="Extension of NLP tasks">Extension of NLP tasks</a></li>
                <li>
                    <a href="#extension-of-cv-tasks" aria-label="Extension of CV tasks">Extension of CV tasks</a></li>
                <li>
                    <a href="#extension-of-both-nlp-and-cv-tasks" aria-label="Extension of both NLP and CV tasks">Extension of both NLP and CV tasks</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#visual-description-generation-and-storytelling" aria-label="Visual Description Generation and Storytelling">Visual Description Generation and Storytelling</a><ul>
                        
                <li>
                    <a href="#visual-description-generation" aria-label="Visual description generation">Visual description generation</a><ul>
                        
                <li>
                    <a href="#image-description-generation" aria-label="Image description generation">Image description generation</a></li>
                <li>
                    <a href="#video-description-generation" aria-label="Video description generation">Video description generation</a></li></ul>
                </li>
                <li>
                    <a href="#visual-storytelling" aria-label="Visual storytelling">Visual storytelling</a><ul>
                        
                <li>
                    <a href="#image-storytelling" aria-label="Image storytelling">Image storytelling</a></li>
                <li>
                    <a href="#video-storytelling-less-explored" aria-label="Video storytelling (less explored)">Video storytelling (less explored)</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#visual-referring-expression-comprehension-and-generation" aria-label="Visual Referring Expression Comprehension and Generation">Visual Referring Expression Comprehension and Generation</a><ul>
                        
                <li>
                    <a href="#image-referring-expression-comprehension-and-generation" aria-label="Image referring expression comprehension and generation">Image referring expression comprehension and generation</a></li>
                <li>
                    <a href="#image-referring-expression-comprehension-and-generation-combination" aria-label="Image referring expression comprehension and generation combination">Image referring expression comprehension and generation combination</a></li>
                <li>
                    <a href="#video-referring-expression-comprehension-and-generation" aria-label="Video referring expression comprehension and generation">Video referring expression comprehension and generation</a></li></ul>
                </li>
                <li>
                    <a href="#visual-question-answering-reasoning-and-entailment" aria-label="Visual question answering, reasoning, and entailment">Visual question answering, reasoning, and entailment</a><ul>
                        
                <li>
                    <a href="#visual-question-answering" aria-label="Visual question answering">Visual question answering</a></li>
                <li>
                    <a href="#video-question-answering" aria-label="Video question answering">Video question answering</a></li>
                <li>
                    <a href="#visual-reasoning" aria-label="Visual reasoning">Visual reasoning</a><ul>
                        
                <li>
                    <a href="#image-reasoning" aria-label="Image reasoning">Image reasoning</a></li>
                <li>
                    <a href="#video-reasoning" aria-label="Video reasoning">Video reasoning</a></li></ul>
                </li>
                <li>
                    <a href="#visual-entailment" aria-label="Visual entailment">Visual entailment</a><ul>
                        
                <li>
                    <a href="#image-entailment" aria-label="Image entailment">Image entailment</a></li>
                <li>
                    <a href="#video-entailment" aria-label="Video entailment">Video entailment</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#visual-dialog" aria-label="Visual Dialog">Visual Dialog</a><ul>
                        
                <li>
                    <a href="#image-dialog" aria-label="Image Dialog">Image Dialog</a></li>
                <li>
                    <a href="#video-dialog" aria-label="Video dialog">Video dialog</a></li></ul>
                </li>
                <li>
                    <a href="#multimodal-machine-translation-mmt" aria-label="Multimodal Machine Translation (MMT)">Multimodal Machine Translation (MMT)</a><ul>
                        
                <li>
                    <a href="#machine-tanslation-with-image" aria-label="Machine tanslation with image">Machine tanslation with image</a></li>
                <li>
                    <a href="#machine-translation-with-video" aria-label="Machine translation with video">Machine translation with video</a></li></ul>
                </li>
                <li>
                    <a href="#language-to-vision-generation" aria-label="Language-to-Vision Generation">Language-to-Vision Generation</a><ul>
                        
                <li>
                    <a href="#language-to-image-generation" aria-label="Language-to-Image Generation">Language-to-Image Generation</a><ul>
                        
                <li>
                    <a href="#image-manipulation" aria-label="Image manipulation">Image manipulation</a></li>
                <li>
                    <a href="#fine-grained-image-generation" aria-label="Fine-grained image generation">Fine-grained image generation</a></li>
                <li>
                    <a href="#sequential-image-generation" aria-label="Sequential image generation">Sequential image generation</a></li></ul>
                </li>
                <li>
                    <a href="#language-to-video-generation" aria-label="Language-to-Video generation">Language-to-Video generation</a></li></ul>
                </li>
                <li>
                    <a href="#vision-and-language-navigation" aria-label="Vision-and-Language Navigation">Vision-and-Language Navigation</a><ul>
                        
                <li>
                    <a href="#image-and-language-navigation" aria-label="Image-and-Language Navigation">Image-and-Language Navigation</a></li></ul>
                </li>
                <li>
                    <a href="#vison-and-language-pretraining" aria-label="Vison-and-Language pretraining">Vison-and-Language pretraining</a><ul>
                        
                <li>
                    <a href="#single-stream-architectures" aria-label="Single-stream architectures">Single-stream architectures</a></li>
                <li>
                    <a href="#two-stream-architectures" aria-label="Two-stream architectures">Two-stream architectures</a>
                </li>
            </ul>
            </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="trends-in-integration-of-vision-and-language-research-a-survey-of-tasks-datasets-and-methods">Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods<a hidden class="anchor" aria-hidden="true" href="#trends-in-integration-of-vision-and-language-research-a-survey-of-tasks-datasets-and-methods">#</a></h1>
<h2 id="abstract">Abstract<a hidden class="anchor" aria-hidden="true" href="#abstract">#</a></h2>
<blockquote>
<p>Focus on ten prominent tasks that integrate language and vision by discussing their problem formulation, methods, existing datasets, evaluation measures, and compare results with the SOTA methods.</p>
</blockquote>
<h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<ul>
<li>Multimodal learning models:
<ol>
<li>generate comprehensible but concise and grammatically well-formed descriptions of the visual content, or vice versa by generating the visual content for a given textual description in a natural language of choice,</li>
<li>identify objets in the visual content and infer their relationships to reson about, or answer arbitrary questions about them,</li>
<li>navigate through and environment by leveraging input from both vision and natural language instructions,</li>
<li>translate textual content from one language to another while leveraging the visual content for sense disambiguation,</li>
<li>generate stories about the visual content, and so on.</li>
</ol>
</li>
<li>Applications:
<ol>
<li>visually impaired individuals can be asisted thereby.</li>
<li>automatic survelliance</li>
<li>autonomous driving</li>
<li>human-computer interaction</li>
<li>city navigation</li>
</ol>
</li>
<li>Task-specific multimodal models
<ol>
<li>image description (Bernardi et al.,2016; Bai &amp; An, 2018; Hossain et al., 2019)</li>
<li>video description generation (Aafaq et al.,2020)</li>
<li>visual question answering (Kafle &amp; Kanan, 2017; Wu et al., 2017)</li>
<li>action recognition (Gella &amp; Keller, 2017)</li>
<li>visual semantics (Liu et al., 2019)</li>
<li>NLP natual language generation (Gatt &amp; Krahmer, 2018; Garbacea &amp; Mei, 2020)</li>
<li>NLP commonsense reasoning (Storks et al., 2019)</li>
<li>understanding the limitations of the integration of vision and language research (Kafle et al., 2019)</li>
</ol>
</li>
</ul>
<hr>
<h2 id="background">Background<a hidden class="anchor" aria-hidden="true" href="#background">#</a></h2>
<h3 id="computer-vision-tasks">Computer vision tasks<a hidden class="anchor" aria-hidden="true" href="#computer-vision-tasks">#</a></h3>
<h4 id="image-as-visual-information">Image as visual information<a hidden class="anchor" aria-hidden="true" href="#image-as-visual-information">#</a></h4>
<ul>
<li>Utilization
<ol>
<li>the tasks where images are used as input</li>
<li>the representation of images</li>
</ol>
</li>
<li>Tasks
<ol>
<li>Image classification</li>
<li>Object localization</li>
<li>Object detection</li>
<li>Object segmentation</li>
<li>Object Identification</li>
<li>Instance segmentation</li>
<li>Panoptic segmentation</li>
</ol>
</li>
<li>Reoresentation
<blockquote>
<p>self supervised learning (Jing &amp; Tian, 2019)</p>
</blockquote>
</li>
</ul>
<h4 id="video-as-visual-information">Video as visual information<a hidden class="anchor" aria-hidden="true" href="#video-as-visual-information">#</a></h4>
<ul>
<li>Utilization
<ol>
<li>knowing the tasks where videos are used as inputs</li>
<li>the representation of a video</li>
</ol>
</li>
<li>Takss
<ol>
<li>Object tracking</li>
<li>Action classification</li>
<li>Emotion detection</li>
<li>Scene detection</li>
<li>Automated editing</li>
</ol>
</li>
</ul>
<h3 id="nlp-tasks">NLP tasks<a hidden class="anchor" aria-hidden="true" href="#nlp-tasks">#</a></h3>
<ul>
<li>Tasks
<ol>
<li>understanding language</li>
<li>generating language</li>
</ol>
<blockquote>
<p>Some of the classical NLP tasks, that are used to comprehend language, are shallow parsing, syntax parsing, semantic role labeling, named entity recognition, entity linking, co-reference resolution, etc.</p>
</blockquote>
</li>
<li>Representation</li>
</ul>
<h3 id="cv-and-nlp-integration-tasks">CV and NLP integration tasks<a hidden class="anchor" aria-hidden="true" href="#cv-and-nlp-integration-tasks">#</a></h3>
<!-- raw HTML omitted -->
<h4 id="extension-of-nlp-tasks">Extension of NLP tasks<a hidden class="anchor" aria-hidden="true" href="#extension-of-nlp-tasks">#</a></h4>
<ul>
<li>Visual Description Generation: The goal is to generate a human-readable text snippt that describes the input</li>
<li>Visual Storytelling: A sequence of visual inputs is used to generate a narrative summary based on text aligned with them</li>
<li>Visual Question Answering: Answer questions about a visual input.</li>
<li>Visual Dialog: Aim at creating a meaningful dislog in a natural and conversational language about a visual content.</li>
<li>Visual Reeferring Expression:</li>
<li>Visual Entailment: An inference task for predicting whether the image semantically entails the text.</li>
<li>Multimodal Machine Traslation: Translate from source language to target language by leveraging the visual information as auxiliary modality along with the natural language text in source language.</li>
</ul>
<h4 id="extension-of-cv-tasks">Extension of CV tasks<a hidden class="anchor" aria-hidden="true" href="#extension-of-cv-tasks">#</a></h4>
<ul>
<li>Visual generation: generate visual content by conditioning on input text from a chosen natural language.</li>
<li>Visual resoning: It is expected to output a relationship between detected objects by generating an entire visual scene graph. The scene graph is leverage to reason and answer questions about visual content. It can also be used to reason about whether a natural language statement is true or not regarding a visual input.</li>
</ul>
<h4 id="extension-of-both-nlp-and-cv-tasks">Extension of both NLP and CV tasks<a hidden class="anchor" aria-hidden="true" href="#extension-of-both-nlp-and-cv-tasks">#</a></h4>
<ul>
<li>Vision-and-Language Navigation: natural language navigation should be interpreted based on visual input. Combine both vision and language.</li>
</ul>
<hr>
<h2 id="visual-description-generation-and-storytelling">Visual Description Generation and Storytelling<a hidden class="anchor" aria-hidden="true" href="#visual-description-generation-and-storytelling">#</a></h2>
<blockquote>
<p>generate a textual description when conditioned on visual input</p>
</blockquote>
<h3 id="visual-description-generation">Visual description generation<a hidden class="anchor" aria-hidden="true" href="#visual-description-generation">#</a></h3>
<blockquote>
<p>Aim to generate either a global descriptionor dense captions for a given visual input.</p>
</blockquote>
<h4 id="image-description-generation">Image description generation<a hidden class="anchor" aria-hidden="true" href="#image-description-generation">#</a></h4>
<ul>
<li>Standard image description generation: generate a sentence-level description of the scene in a given image</li>
<li>Dense image description generation: create descriptions at the local object-level in a given image.</li>
<li>Image paragraph generation: create paragraphs instead of generating a single simple description. Generated paragraphs are expected to be coherent and contain fine-grained natural language descriptions.</li>
<li>Spoken language image description generation: expand the description generation task to work with spoken language, instead o flimiting to only the written forms of language.</li>
<li>Stylistic image description generation: add style to the standard image description generation, where the generated descriptions adhere to a specific style.</li>
<li>Unseen objects image description generation: leverage images which lack paired descriptions. Generate descriptions for visual object categories previously unseen in image-description corpora.</li>
<li>Diverse image description generation: incorporate variety and diversity in the generated captions.</li>
<li>Controllable image description generation: select specific objects in an image, defined by a control signal, to generate descriptions.</li>
<li>Image caption emendation as generation: build a model to emend both syntactic and semantic errors in the captions.</li>
</ul>
<h4 id="video-description-generation">Video description generation<a hidden class="anchor" aria-hidden="true" href="#video-description-generation">#</a></h4>
<ul>
<li>
<p>Global description generation:</p>
<ol>
<li>ground sentences that describe actions in the visual information extracted from videos (Motwani &amp; Mooney, 2012; Regneri et al., 2013)</li>
<li>Generate global natural language descriptions for videos with various approaches: <em>leveraging latent topics</em> (Das et al., 2013), <em>corpora knowledge</em> (Krishnamoorthy et al., 2013), <em>graphical models</em> (Rohrbach et al., 2013), and <em>sequence-to-sequence</em> learning (Venugopalan et al., 2015b, 2015a; Donahue et al., 2015; Srivastava et al., 2015; Xu et al., 2016; Ramanishka et al., 2016; Jin et al., 2016), <em>factor graph</em>(Thomason et al., 2014) combines visual detection with language statistics.</li>
<li>Seq2seeq based approaches: extra corpora (Venugopalan et al., 2016), soft-attention (Yao et al., 2015), multimodal fusion (Hori et al., 2017), temporal attention (Song et al., 2017), semantic consistency (Gao et al., 2017), residual connections(Li et al., 2019). Incorporation of semantic attributes learned from videos, ensembled-based description generator networks (Shetty et al., 2018), encoder-decoder reconstructors (Wang et al., 2018).</li>
<li>Other approaches: combined with entailment generation task (Pasunuru &amp; Bansal, 2017a), multiple fine-grained actions (Wang et al., 2018b), reinforcement learning (Pasunuru &amp; Bansal, 2017b), Visual Text correction system (Mazaheri &amp; Shah, 2018), object relational graph baed encoder, language model decoder (Zhang et al., 2020).</li>
</ol>
</li>
<li>
<p>Dense video description generation:</p>
<blockquote>
<p>achieve fine-grained video understanding by addressing two sub-problems: (1) localizing events in a video, and (2) generating captions for these localized events. the core challenge, namely the automatic evaluation of video captioning, is still unsolved.</p>
</blockquote>
</li>
<li>
<p>Movie description generation: input movie clips, align books to movies (Tapaswi et al., 2015; Zhu et al., 2015), movie descriptions (Rohrbach et al., 2015).</p>
</li>
</ul>
<h3 id="visual-storytelling">Visual storytelling<a hidden class="anchor" aria-hidden="true" href="#visual-storytelling">#</a></h3>
<blockquote>
<p>The task of visual storytelling aims to encode a sequence of images or frames (in the video) to generate a paragraph which is story-like.</p>
</blockquote>
<h4 id="image-storytelling">Image storytelling<a hidden class="anchor" aria-hidden="true" href="#image-storytelling">#</a></h4>
<blockquote>
<p>The aim of image storytelling is to generate stories from a sequence of images.</p>
</blockquote>
<ul>
<li>semantic coherence is captured in a photo stream.</li>
<li>discover semantic embeddings correlations (Yu et al., 2017), incorporated with reinforcement learning (Wang et al., 2018), hierarchically structured reinforced training (Huang et al., 2019), adversarial reward learning Wang et al. (2018a).</li>
<li>suffer from <em>repetitiveness</em>, the same objects/events undermine a good story structure. -&gt; inter-sentence diversity was explored with diverse beam search (Hsu et al., 2018).</li>
</ul>
<h4 id="video-storytelling-less-explored">Video storytelling (less explored)<a hidden class="anchor" aria-hidden="true" href="#video-storytelling-less-explored">#</a></h4>
<blockquote>
<p>In comparison to image storytelling, which only deals with a small sequence of images, the aim of video storytelling is to generate coherent and succinct stories for long videos.</p>
</blockquote>
<ul>
<li>Pinoeer Li et al. (2020) address challenges like diversity in the story and the inherent complexity of video.</li>
<li>goal: offer support to people with visual disabilities or technical issues like internet bandwidth limitations.</li>
</ul>
<hr>
<h2 id="visual-referring-expression-comprehension-and-generation">Visual Referring Expression Comprehension and Generation<a hidden class="anchor" aria-hidden="true" href="#visual-referring-expression-comprehension-and-generation">#</a></h2>
<blockquote>
<p>The objective of the task is to ground a natural language expression (e.g. a noun phrase or a longer piece of text) to objects in a visual input.</p>
</blockquote>
<h3 id="image-referring-expression-comprehension-and-generation">Image referring expression comprehension and generation<a hidden class="anchor" aria-hidden="true" href="#image-referring-expression-comprehension-and-generation">#</a></h3>
<blockquote>
<p>In a natural environment, people use referring expressions to unambiguously identify, indicate, or point to particular objects. This is usually done with a simple phrase or within a larger context (e.g. a sentence). Having  a larger context provides better scope for avoiding ambiguity and allows the referential expression to easily map to the target object. However, there can also be other possibilities in which people are asked to describe a target object based on its surrounding objects.</p>
</blockquote>
<ul>
<li>used to generate, an algorithm generates a referring expression for a given target object which is present in a visual scene
<ul>
<li>approaches: (FitzGerald et al., 2013) tackled the problem from the perspective of density estimation, learn distributions over logical exprssions identifying sets of objects in the world.</li>
</ul>
</li>
<li>used to perform comprehension, an algorithm locates in an image the object described by a given referring expression.
<ul>
<li>approaches: Nagaraja et al. (2016) integrate contexts between objects. Multiple Instance Learning (MIL). Hu et al. (2016) leverage a NLP query of the object to localize a target object, integrating spatial configurations and global scene-level contextual information. (Yu et al., 2018) subject appearence, location, and relationship to other objects. (Cirik et al., 2018a) syntactic analysis, build a dynamic computation graph. variational model (Zhang et al., 2018)</li>
<li>cross-modal: (Yang et al., 2019) cross-modal relationship inference， 1. highlight objects and relationships connected with a referring, 2. multi-modal semantic contexts. (可以将空间的referring用到depth estimation里面吗)。Recursive Grounding Tree (Hong et al., 2019) binary tree to parse referring expression-&gt; visual reasoning. (Liu et al., 2019), combining visual reasoning with referential expressions. object segmentation based referring expression (Liu et al., 2017).</li>
</ul>
</li>
</ul>
<h3 id="image-referring-expression-comprehension-and-generation-combination">Image referring expression comprehension and generation combination<a hidden class="anchor" aria-hidden="true" href="#image-referring-expression-comprehension-and-generation-combination">#</a></h3>
<ul>
<li>(Mao et al., 2016; Yu et al., 2016) find visual comparison to other objects within an image. <strong>(Yu et al., 2017a)</strong> a speaker, a listener, and a reinforcer. The speaker generate referring expressions, the listner comprehend referring expressions, the reinforce use a reward function to guide sampling of more discriminative expressions.</li>
</ul>
<h3 id="video-referring-expression-comprehension-and-generation">Video referring expression comprehension and generation<a hidden class="anchor" aria-hidden="true" href="#video-referring-expression-comprehension-and-generation">#</a></h3>
<ul>
<li>Vasudevan et al. (2018) stereo videos to explore temporal-spatial contextual information. Khoreva et al. (2018) language referring expressions to achieve object segmentation. Wang et al. (2020) video grounding with contextual information.</li>
</ul>
<hr>
<h2 id="visual-question-answering-reasoning-and-entailment">Visual question answering, reasoning, and entailment<a hidden class="anchor" aria-hidden="true" href="#visual-question-answering-reasoning-and-entailment">#</a></h2>
<blockquote>
<p>they share the common intention of answering questions when conditioned on a visual input</p>
</blockquote>
<h3 id="visual-question-answering">Visual question answering<a hidden class="anchor" aria-hidden="true" href="#visual-question-answering">#</a></h3>
<blockquote>
<p>The goal of Visual Question Answering (VQA) is to learn a model that comprehends visual content at both the global and local level for finding an association with pairs of questions and answers in the natural language form.</p>
</blockquote>
<ul>
<li>Image Q&amp;A as Visual Turing Test (Malinowski &amp; Fritz, 2014; Malinowski et al., 2015; Geman et al., 2015). fill-in-the-blank tasks (Yu et al., 2015), multiple-choice question-answering for images. Address open-ended Image Q&amp;A (Antol et al., 2015; Agrawal et al., 2017), ask free-form natural language question. Binary image Q&amp;A (Zhang et al., 2016). Relate local regions in the images (Zhu et al., 2016) by addressing object-level grounding</li>
<li>Interpretability or explainability by overcoming priors (Agrawal et al., 2018). Generate human-interpretable rules that provide better insights <strong>(Manjunatha et al., 2019)</strong>. cycle-consistency (Shah et al., 2019a). outside knowledge (Marino et al., 2019)(Shah et al., 2019b).</li>
<li>Multi-task learning, federated learning.</li>
</ul>
<h3 id="video-question-answering">Video question answering<a hidden class="anchor" aria-hidden="true" href="#video-question-answering">#</a></h3>
<blockquote>
<p>is to answer natural language questions about videos.</p>
</blockquote>
<ul>
<li>(Tu et al., 2014) jointly parsing videos with corresponding text to answer queries. open-ended movie Q&amp;A (Tapaswi et al., 2016). fill-in-the-blank questions (Zhu et al., 2017; Mazaheri et al., 2017). (Zeng et al., 2017) free-form Q&amp;A. High-level concept words (Yu et al., 2017b). Attention (Jang et al., 2017). <strong>spatio-temporal grounding (Lei et al., 2020)</strong></li>
<li>STAGE (Lei et al., 2020), aligned fusion is essential for improving Video Q&amp;A.</li>
</ul>
<h3 id="visual-reasoning">Visual reasoning<a hidden class="anchor" aria-hidden="true" href="#visual-reasoning">#</a></h3>
<blockquote>
<p>is to learn a model that comprehends the visual content by reasoning about it.</p>
</blockquote>
<h4 id="image-reasoning">Image reasoning<a hidden class="anchor" aria-hidden="true" href="#image-reasoning">#</a></h4>
<blockquote>
<p>s to answer sophisticated queries by reasoning about the visual world.</p>
</blockquote>
<ul>
<li>(Johnson et al., 2017a) design diagnostic tests going beyond benchmarks. VQA struggle with comparing the attributes of objects or novel attribute combinations. (Johnson et al., 2017b) program generator. (Hu et al., 2017) predict instance-specific network layouts. Santoro et al. (2017) relation-aware visual features. (Cao et al., 2018) global context reasoning.</li>
<li><strong>Mascharka et al. (2018) proposed a set of visual-reasoning primitives.</strong> Learning-By-Asking (LBA) (Misra et al., 2018b), mimic natural learning with the goal to make it more data efficient. compositional attention networks (Hudson &amp; Manning, 2018) explicit and expressive reasoning.</li>
<li><strong>neural-symbolic visual question answering (Yi et al., 2018)</strong>, recover structural scene representation from the image and a program trace from the question. <strong>Neuro-Symbolic Concept Learner (NS-CL) (Mao et al., 2019)</strong> learns visual concepts, word, and semantic parsing of sentences without explicit supervision. It learns by simply looking at images and reading paired questions and answers. multimodal relation network (Cadène et al., 2019) learn end-to-end reasoning over real images. Aditya et al. (2019) used spatial knowledge to aid visual reasoning, knowledge distillation, relational resoning, probabilistic logical languages. Explainable and explicit neurla modules (Shi et al., 2019) scene graph.</li>
<li><strong>Andreas et al. (2016a, 2016b)</strong> exploit the compositional linguistic structure of complex questions by forming  neural module networks which query about the abstract shapes observed in an image. Compositional question answering (Hudson &amp; Manning, 2019). Zellers et al. (2019) commonsense knowledge. NLVR (Suhr et al., 2017) and NLVR2 (Suhr et al., 2019) true or false.</li>
</ul>
<h4 id="video-reasoning">Video reasoning<a hidden class="anchor" aria-hidden="true" href="#video-reasoning">#</a></h4>
<blockquote>
<p>The goal of COG, Yang et al. (2018), is to address problems related to visual and logical reasoning and memory</p>
</blockquote>
<h3 id="visual-entailment">Visual entailment<a hidden class="anchor" aria-hidden="true" href="#visual-entailment">#</a></h3>
<blockquote>
<p>is to learn a model that predicts whether the visual content entails the augmented text along with hypothesis.</p>
</blockquote>
<h4 id="image-entailment">Image entailment<a hidden class="anchor" aria-hidden="true" href="#image-entailment">#</a></h4>
<ul>
<li>Vu et al. (2018), a visually grounded version of the textual entailment where an image is augmented with textual premise and hypothesis. Xie et al. (2019) predicts whether the image semantically entails the text, given image-sentence pairs, the premise is defined by an image.</li>
</ul>
<h4 id="video-entailment">Video entailment<a hidden class="anchor" aria-hidden="true" href="#video-entailment">#</a></h4>
<blockquote>
<p>(Liu et al., 2020) to infer whether the natural language hypothesis is entailed or contradicted when given a video clip aligned with the subtitles information</p>
</blockquote>
<hr>
<h2 id="visual-dialog">Visual Dialog<a hidden class="anchor" aria-hidden="true" href="#visual-dialog">#</a></h2>
<blockquote>
<p>involves a complex interaction between a human and an artificial agent.</p>
</blockquote>
<h3 id="image-dialog">Image Dialog<a hidden class="anchor" aria-hidden="true" href="#image-dialog">#</a></h3>
<blockquote>
<p>is to create AI agents that can hold dialog with humans in a natural language of choice about a visual content (Das et al., 2017a), represented by an image. To be more specific, given an image, a history of dialogs, and a question about the image, the goal of an AI agent is to ground the question in the image, infer the context from the history, and then answer the question accurately</p>
</blockquote>
<ul>
<li>(de Vries et al., 2017) locate an unkown object in the image by asking a swquence of questions. (Mostafazadeh et al., 2017). hold natural-sounding conversations about a shared image.</li>
<li>(Lu et al., 2017a) transfer from dialog generation. Seo et al. (2017) attentive memory. (Wu et al., 2018) reinforcement learning and GAN. (Kottur et al., 2018) form explicit and grounded co-reference between nouns and pronouns. (Niu et al., 2019) recursive visual attention. (Zheng et al., 2019) graphical model inference. Guo et al. (2019) builds an image-question-answer synergistic network. (Shekhar et al., 2019) a visually grounded encoder guessing and asking questions.</li>
</ul>
<h3 id="video-dialog">Video dialog<a hidden class="anchor" aria-hidden="true" href="#video-dialog">#</a></h3>
<blockquote>
<p>is to leverage scene information containing both audio (which can be transcribed as subtitles) and visual frames to hold a dialog (i.e., an exchange) with humans in a natural language of choice about the multimedia content (Alamri et al., 2019b 2019a).</p>
</blockquote>
<ul>
<li>multimodal-based video description (Hori et al., 2019).</li>
</ul>
<hr>
<h2 id="multimodal-machine-translation-mmt">Multimodal Machine Translation (MMT)<a hidden class="anchor" aria-hidden="true" href="#multimodal-machine-translation-mmt">#</a></h2>
<blockquote>
<p>is to translate natural language sentences that describe visual content (e.g. image) in a source language into a target language by taking the visual content as an additional input to the source language sentences.</p>
</blockquote>
<h3 id="machine-tanslation-with-image">Machine tanslation with image<a hidden class="anchor" aria-hidden="true" href="#machine-tanslation-with-image">#</a></h3>
<blockquote>
<p>is to translate sentences, that describe an image, in a source language into equivalent sentences in a target language.</p>
</blockquote>
<ul>
<li>single source translation, multisource MMT.</li>
<li>multimodal attention (Huang et al., 2016). doubly-attentive decoder incorporated visual features (Calixto et al., 2017). learning to translate, and learning visually grounded representations (Elliott &amp; Kádár, 2017). noisy image captions for MMT (Schamoni et al., 2018).</li>
</ul>
<h3 id="machine-translation-with-video">Machine translation with video<a hidden class="anchor" aria-hidden="true" href="#machine-translation-with-video">#</a></h3>
<blockquote>
<p>(Wang et al., 2019b) is to translate a source language description into the target language equivalent using the video information as additional spatio-temporal context.</p>
</blockquote>
<hr>
<h2 id="language-to-vision-generation">Language-to-Vision Generation<a hidden class="anchor" aria-hidden="true" href="#language-to-vision-generation">#</a></h2>
<blockquote>
<p>is to generate visual content given their natural language descriptions.</p>
</blockquote>
<h3 id="language-to-image-generation">Language-to-Image Generation<a hidden class="anchor" aria-hidden="true" href="#language-to-image-generation">#</a></h3>
<ul>
<li>sentence-level language-to-image generation: generate images cnditioned on the natural language descriptions. (Mansimov et al., 2016) iteratively draw patches on a canvas, attending to the relavant words in the description. (Reed et al., 2016b) visual concepts could be translated from characters to pixels with conditional gan. (Reed et al., 2016a) what content should be drawn in which location for high quality image generation. (Nguyen et al., 2017) conditioned on image classes. (Dash et al., 2017) condition on both sentence and class information. stackGAN (Zhang et al., 2017, 2019). attention-based GAN (Xu et al., 2018)</li>
<li>(Hong et al., 2018) infer the semantic layout of the image. Johnson et al. (2018) used image-specific scene graphs enabling explicitly reasoning about objects and their relationships.</li>
</ul>
<h4 id="image-manipulation">Image manipulation<a hidden class="anchor" aria-hidden="true" href="#image-manipulation">#</a></h4>
<ul>
<li>TAGAN (Nam et al., 2018) generate semantically manipulated images while preserving text-irrelevant contents. Only regions correspond to the given text are modified. (Zhu et al., 2019) attention generator, discriminator. (Li et al., 2020) designed error correction modules to rectify mismatched attributes and complete the missing contents.</li>
</ul>
<h4 id="fine-grained-image-generation">Fine-grained image generation<a hidden class="anchor" aria-hidden="true" href="#fine-grained-image-generation">#</a></h4>
<ul>
<li>(El-Nouby et al., 2018) recurrent image generation, output up to the current step &amp; past instructions. (Hinz et al., 2019) control location of objects by adding a pathway in an iterative manner.</li>
</ul>
<h4 id="sequential-image-generation">Sequential image generation<a hidden class="anchor" aria-hidden="true" href="#sequential-image-generation">#</a></h4>
<ul>
<li>StoryGAN (Li et al., 2019b) opposite to storytelling.</li>
</ul>
<h3 id="language-to-video-generation">Language-to-Video generation<a hidden class="anchor" aria-hidden="true" href="#language-to-video-generation">#</a></h3>
<ul>
<li>(Li et al., 2018) conditional generative model</li>
</ul>
<hr>
<h2 id="vision-and-language-navigation">Vision-and-Language Navigation<a hidden class="anchor" aria-hidden="true" href="#vision-and-language-navigation">#</a></h2>
<blockquote>
<p>is to carry out navigation in an environment by interpreting natural language instructions</p>
</blockquote>
<h3 id="image-and-language-navigation">Image-and-Language Navigation<a hidden class="anchor" aria-hidden="true" href="#image-and-language-navigation">#</a></h3>
<ul>
<li>(Anderson et al., 2018b) an autonomous agent navigate in an environment by interpreting natural language instructions. (Wang et al., 2019a), reinforced cross-modal matching. (Fried et al., 2018) train an action space with an embedded speaker model.</li>
<li>Embodied Question Answering (Das et al., 2018a, 2018b). interactive question answering (Gordon et al., 2018). grounded dialog (de Vries et al., 2018)</li>
</ul>
<hr>
<h2 id="vison-and-language-pretraining">Vison-and-Language pretraining<a hidden class="anchor" aria-hidden="true" href="#vison-and-language-pretraining">#</a></h2>
<blockquote>
<p>To jointly learn representations using both visual and textual content</p>
</blockquote>
<h3 id="single-stream-architectures">Single-stream architectures<a hidden class="anchor" aria-hidden="true" href="#single-stream-architectures">#</a></h3>
<ul>
<li>BERT-like (Devlin et al., 2019). VideoBERT (Sun et al., 2019). Bounding Boxes in Text Transformer (B2T2) (Alberti et al., 2019). Unicoder-VL (Li et al., 2020). VL-BERT (Su et al., 2020), VLP (Zhou et al., 2020), OSCAR (Li et al., 202), VinVL (Zhang et al., 2021) can jointly understand and generate from cross-modal data. (Cao et al., 2020) probe.</li>
</ul>
<h3 id="two-stream-architectures">Two-stream architectures<a hidden class="anchor" aria-hidden="true" href="#two-stream-architectures">#</a></h3>
<ul>
<li>two independent encoders for learning visual and text representations. ViLBERT (Lu et al., 2019) and LXMERT (Tan &amp; Bansal, 2019).</li>
</ul>
<p>-neuro-symbolic reasoning systems (Yi et al., 2018; Vedantam et al., 2019).</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://x423xu.github.io/tags/paper-reading/">paper-reading</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://x423xu.github.io/posts/2022-05-24-singularity-deployment/">
    <span class="title">« Prev</span>
    <br>
    <span>How to deploy singularity for data processing</span>
  </a>
  <a class="next" href="https://x423xu.github.io/posts/2022-04-09-logical-syntax/">
    <span class="title">Next »</span>
    <br>
    <span>Logical Syntax</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share visual text review on twitter"
        href="https://twitter.com/intent/tweet/?text=visual%20text%20review&amp;url=https%3a%2f%2fx423xu.github.io%2fposts%2f2022-04-18-visual%2btext-review%2f&amp;hashtags=paper-reading">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="https://x423xu.github.io/">Xiaoyu&#39;s blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
