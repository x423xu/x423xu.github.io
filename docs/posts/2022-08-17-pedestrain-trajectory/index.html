<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>pedestrain-trajectory | Xiaoyu&#39;s blog</title>
<meta name="keywords" content="paper-reading">
<meta name="description" content="Pedestrain trajectory Questions: 1. only pedestrain relation considered? hoe about environment 2. what is the general framework for pedestrain trajectory prediction.
SGCN: Sparse Graph Convolution Network for Pedestrain Trajectory Prediction (CVPR2021) SGCN framework superfulous interactions: dense interaction -&gt; one pedestrain is related to all other pedestrains while in fact it is not sparse undirected -&gt; equal interactions for a pair of pedestrains spatial GCN -&gt; sparse directed -&gt; not all pedestrains &#43; not equal interaction temporal GCN -&gt; motion tendency Disentangled Multi-Relational Graph Convolutional Network for Pedestrian Trajectory Prediction (AAAI2021) Use CNN to generalize complex interpersonal relations a graph representation: node as pedestrian, edges correspond to distance challeges only simple social relationship like collision avoidance is aggregated modelling social norms is not suitable for determining the end-points of pedestrians in the last frame (over-avoidance) contributions disentangled multi-scale aggregation to clearly distinguish between relevant pedestrians multi-relational GCN to extract sophisticated social interaction in a scene.">
<meta name="author" content="">
<link rel="canonical" href="https://x423xu.github.io/posts/2022-08-17-pedestrain-trajectory/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css" integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://x423xu.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://x423xu.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://x423xu.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://x423xu.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://x423xu.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<script>
    MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true
        },
        options: {
            skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        }
    };

    window.addEventListener('load', (event) => {
        document.querySelectorAll("mjx-container").forEach(function (x) {
            x.parentElement.classList += 'has-jax'
        })
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta property="og:title" content="pedestrain-trajectory" />
<meta property="og:description" content="Pedestrain trajectory Questions: 1. only pedestrain relation considered? hoe about environment 2. what is the general framework for pedestrain trajectory prediction.
SGCN: Sparse Graph Convolution Network for Pedestrain Trajectory Prediction (CVPR2021) SGCN framework superfulous interactions: dense interaction -&gt; one pedestrain is related to all other pedestrains while in fact it is not sparse undirected -&gt; equal interactions for a pair of pedestrains spatial GCN -&gt; sparse directed -&gt; not all pedestrains &#43; not equal interaction temporal GCN -&gt; motion tendency Disentangled Multi-Relational Graph Convolutional Network for Pedestrian Trajectory Prediction (AAAI2021) Use CNN to generalize complex interpersonal relations a graph representation: node as pedestrian, edges correspond to distance challeges only simple social relationship like collision avoidance is aggregated modelling social norms is not suitable for determining the end-points of pedestrians in the last frame (over-avoidance) contributions disentangled multi-scale aggregation to clearly distinguish between relevant pedestrians multi-relational GCN to extract sophisticated social interaction in a scene." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://x423xu.github.io/posts/2022-08-17-pedestrain-trajectory/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-08-17T13:30:03+00:00" />
<meta property="article:modified_time" content="2022-08-17T13:30:03+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="pedestrain-trajectory"/>
<meta name="twitter:description" content="Pedestrain trajectory Questions: 1. only pedestrain relation considered? hoe about environment 2. what is the general framework for pedestrain trajectory prediction.
SGCN: Sparse Graph Convolution Network for Pedestrain Trajectory Prediction (CVPR2021) SGCN framework superfulous interactions: dense interaction -&gt; one pedestrain is related to all other pedestrains while in fact it is not sparse undirected -&gt; equal interactions for a pair of pedestrains spatial GCN -&gt; sparse directed -&gt; not all pedestrains &#43; not equal interaction temporal GCN -&gt; motion tendency Disentangled Multi-Relational Graph Convolutional Network for Pedestrian Trajectory Prediction (AAAI2021) Use CNN to generalize complex interpersonal relations a graph representation: node as pedestrian, edges correspond to distance challeges only simple social relationship like collision avoidance is aggregated modelling social norms is not suitable for determining the end-points of pedestrians in the last frame (over-avoidance) contributions disentangled multi-scale aggregation to clearly distinguish between relevant pedestrians multi-relational GCN to extract sophisticated social interaction in a scene."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://x423xu.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "pedestrain-trajectory",
      "item": "https://x423xu.github.io/posts/2022-08-17-pedestrain-trajectory/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "pedestrain-trajectory",
  "name": "pedestrain-trajectory",
  "description": "Pedestrain trajectory Questions: 1. only pedestrain relation considered? hoe about environment 2. what is the general framework for pedestrain trajectory prediction.\nSGCN: Sparse Graph Convolution Network for Pedestrain Trajectory Prediction (CVPR2021) SGCN framework superfulous interactions: dense interaction -\u0026gt; one pedestrain is related to all other pedestrains while in fact it is not sparse undirected -\u0026gt; equal interactions for a pair of pedestrains spatial GCN -\u0026gt; sparse directed -\u0026gt; not all pedestrains + not equal interaction temporal GCN -\u0026gt; motion tendency Disentangled Multi-Relational Graph Convolutional Network for Pedestrian Trajectory Prediction (AAAI2021) Use CNN to generalize complex interpersonal relations a graph representation: node as pedestrian, edges correspond to distance challeges only simple social relationship like collision avoidance is aggregated modelling social norms is not suitable for determining the end-points of pedestrians in the last frame (over-avoidance) contributions disentangled multi-scale aggregation to clearly distinguish between relevant pedestrians multi-relational GCN to extract sophisticated social interaction in a scene.",
  "keywords": [
    "paper-reading"
  ],
  "articleBody": "Pedestrain trajectory Questions: 1. only pedestrain relation considered? hoe about environment 2. what is the general framework for pedestrain trajectory prediction.\nSGCN: Sparse Graph Convolution Network for Pedestrain Trajectory Prediction (CVPR2021) SGCN framework superfulous interactions: dense interaction -\u003e one pedestrain is related to all other pedestrains while in fact it is not sparse undirected -\u003e equal interactions for a pair of pedestrains spatial GCN -\u003e sparse directed -\u003e not all pedestrains + not equal interaction temporal GCN -\u003e motion tendency Disentangled Multi-Relational Graph Convolutional Network for Pedestrian Trajectory Prediction (AAAI2021) Use CNN to generalize complex interpersonal relations a graph representation: node as pedestrian, edges correspond to distance challeges only simple social relationship like collision avoidance is aggregated modelling social norms is not suitable for determining the end-points of pedestrians in the last frame (over-avoidance) contributions disentangled multi-scale aggregation to clearly distinguish between relevant pedestrians multi-relational GCN to extract sophisticated social interaction in a scene. a global temporal aggregation to compensate accumulated errors from over-avoidance. Gaze target estimation Dual Attention Guided Gaze Target Detection in the Wild challenges: prior works: gaze direction in 2D representations + barely depth-channel gaze. forward, backward, sideward. (refer to one person) salient objects from 2d cues. two candidates different depth along same gaze direction, hard to tell apart. (refer to one person with the scene understanding) fixation inconsistency: facing forward looking downward. solutions: estimate 3D gaze direction from head image. dual attention module -\u003e depth-aware perspective in the scene ?hasn’t found effective solution in the paper? -\u003e solved in the first stage? so is it still an independent challenge? or should be merged to the first challenge? ESCNet: Gaze Target Detection with the Understanding of 3D Scenes hard to handle: multiple salient objects lie in the same depth layer and field of view. occlusion plays an important role in gaze estimation , one cannot see through occluders. 3D geometry could br reconstructed by absolute depth and camera parameters FOV-based initial heatmap.\nMultimodal Across Domains Gaze Target Detection gaze target detection also referred as gaze-following false detections occur when there are multiple object-of-interests at different depths but along with the subject’s gaze direction ‘dual’ potentially being error-prone in real-life processing, e.g., when the eyes are not visible or not detectable ‘ESCNET’ auxiliary estimating depth, depend on depth and pseudo labels. not require gaze angles as supervision only spatial processing, but still detect gaze target use depth image how different modalities should be jointly learned for performing effective gaze target detection\nsolve domain shift problem Beyon 3D Siamese Tracking: A motion-Centric Paradigm for 3D single Object Tracking in Point Clouds point clouds are usually textureless and incomplete, which hinders effective appearance matching. Overlook motion clues among targets. propose a motion-centric paradigm $M^2-Track$ localizes the target within successive frames via motion transformation. refines the target box through motion-assisted shape completion Motivation the upper one obtain a canonical target template using the previous target box and search for the target in thecurrent frame according to the matching similarity. It is sensitive to distractors The bottom one learns relative target motion from two consecutive frames. Robustly localize the target. framework input: last point cloud, last 3D BBox, current point cloud\noutput current 3D BBox.\n? how many frame used and how many frame predicted.\n1 frame used, 1 frame predicted. quick comment: this task is not suitable for our goal. But it gives a good benchmark for kitti dataset.\nMotion CNN plan a safe and efficient route. a baseline form multimodal motion prediction. explanation framework input: image output trajectories. use 1 second of images to predict 8 seconds’. 3D Multi-Object Tracking in Point Clouds Based on Prediction confidence-Guided Data Association 3D multi-object tracker to more robustly track objects that are temporarily missed by detectors. a predictor employs constant acceleration motion model to estimate future positions and prediction confidence. a new pairwise cost. Multimodal Trajectory Prediction Conditioned on Lane-Graph Traversals Predicting the future motion of surrounding vehicles requires reasoning about the inherent uncertainty in driving behavior. Early works encodes HD maps using a rasterized bird’s eye view image. Recent wotks represent lane polylines as nodes of a graph. Represent HD map as a graph and encode the input context into a single context vector. challenges: The prediction header go off the road, violate traffic rules because of the complex mapping. lateral or route variability (e.g. will the driver change lane, will they turn right etc.). longitudinal variability (will the driver accelerate, brake, maintain spped) insights graph structure of the scene explicitly model the lateral or route variability in trajectories predictions conditioned on traversals: selectively aggregate part of scene context by sampling path traversals from a learned policy it lessens representational demands on the output decoder. the probabilistic policy leads to adiverse set of sampled paths. latent variable for longitudinal variability: condition the predictions with a sampled latent variable. enable to predict distinct trajectories even for identical path traversals. method predict the future trajectories of vehicles of interest, conditioned on their past trajectories the past trajectories of nearby vehicles and pedestrians the HD map of the scene represent the scene, predict in the bird’s eye view and use an agent-centric frame of reference aligned along the agent’s instantaneous direction of motion. Trajectory representation $$s_{-t_h:0}^i=[s_{-t_h}^i,\\cdots,s_{-1}^i,s_{0}^i]$$ $$s_t^i = [x_t^i, y_t^i, v_t^i, a_t^i, w_t^i,I_t^i]$$\nHD maps as lane graphs Nodes: lane centerlines $f_{1:N}^v = [f_{1}^v, cdots,f_{N}^v ]$, $f_n^v = [x_n^v, y_n^v, \\theta_n^v, I_n^v]$ Edges: successor edges-\u003elegal route; proximal edges-\u003e lane changes Output representation K trajectories.\nModel graph encoder: forms the backbone of our model, outputs representations for each node of the lane graph, incorporate HD map and surrounding agent context. policy header: output a discrete probability distribution over outgoing edeges at each node, allowing us to sample paths in the graph. trajectory decoder: output trajectories conditioned on paths traversed by the policy and a sampled latent variable. Encoding scene and agent context GRU encoders: target vehicle trajectory $s_{-t_h:0}^0$ -\u003e $h_{motion}$; surrounding vehicle trajectories $s_{-t_h:0}^i$ -\u003e $h_{agent}^i$; node features $f_{1:N}^v$-\u003e$h_{node}^v$. agent node attention: update node encodings with nearby agent encodings. keys and values from $h_{agent}^i$, query by $h_{node}^v$ output of agent-node-attention as the attention mask for GAT. Only attended nodes are employed for updating An explanation of Q, K, V\nThe Seq2Seq task always has encoder+decoder. The encoders job is to take in an input sequence and output a context vector/thought vector. The context vector is then input to decoder.\nHowever, the performance drops drastically for longer sentences.\nOne solution is to use skip-connection, by simple concatenation or summing up. -\u003e assume all hidden states are equally important.\nSo attention (dynamic weighting) is brought in.\nAn attention mechanism calculates the dynamic weights representing the relative importance of the inputs in the sequence (keys) for the particular output (query). Multiplying the dynamic weights with the input sequence (values) will then weight the sequence.\nThe exact values for Q,K, and V depend on exactly which attention mechnism is being referred to. For the trasformer, 3 types: 1. Encoder Attention, 2, Decoder Attention, 3. Encoder-decoder Attention. (Here for reference)\nDiscrete policy for graph traversal A policy $\\pi_{route}$ for graph traversal -\u003e sampled roll-outs of the policy correspond to likely routes the target might take in the future. policy: a discrete probability distribution over outgoing edges at each node. additionally add an edge to an end state, terminate at a goal location. $$score(u,v)=MLP(concat(h_{motion}, h_{node}^u, h_{node}^v, 1_{(u,v)\\in E}))$$ $$\\pi_{route}(v|u) = softmax({score(u,v)|(u,v)\\in E})$$ $$L_{BC} = \\sum_{(u,v)\\in E_{g,t}}-\\log(\\pi_{route}(v|u))$$ ",
  "wordCount" : "1234",
  "inLanguage": "en",
  "datePublished": "2022-08-17T13:30:03Z",
  "dateModified": "2022-08-17T13:30:03Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://x423xu.github.io/posts/2022-08-17-pedestrain-trajectory/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Xiaoyu's blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://x423xu.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://x423xu.github.io/" accesskey="h" title="Xiaoyu&#39;s blog (Alt + H)">Xiaoyu&#39;s blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://x423xu.github.io/" title="home">
                    <span>home</span>
                </a>
            </li>
            <li>
                <a href="https://x423xu.github.io/archives/" title="archives">
                    <span>archives</span>
                </a>
            </li>
            <li>
                <a href="https://x423xu.github.io/categories/" title="categories">
                    <span>categories</span>
                </a>
            </li>
            <li>
                <a href="https://x423xu.github.io/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
            <li>
                <a href="https://x423xu.github.io/search/" title="search (Alt &#43; /)" accesskey=/>
                    <span>search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://x423xu.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://x423xu.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      pedestrain-trajectory
    </h1>
    <div class="post-meta"><span title='2022-08-17 13:30:03 +0000 UTC'>August 17, 2022</span>&nbsp;·&nbsp;6 min

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#pedestrain-trajectory" aria-label="Pedestrain trajectory">Pedestrain trajectory</a></li>
                <li>
                    <a href="#sgcn-sparse-graph-convolution-network-for-pedestrain-trajectory-prediction-cvpr2021" aria-label="SGCN: Sparse Graph Convolution Network for Pedestrain Trajectory Prediction (CVPR2021)">SGCN: Sparse Graph Convolution Network for Pedestrain Trajectory Prediction (CVPR2021)</a><ul>
                        
                <li>
                    <a href="#sgcn-framework" aria-label="SGCN framework">SGCN framework</a></li></ul>
                </li>
                <li>
                    <a href="#disentangled-multi-relational-graph-convolutional-network-for-pedestrian-trajectory-prediction-aaai2021" aria-label="Disentangled Multi-Relational Graph Convolutional Network for Pedestrian Trajectory Prediction (AAAI2021)">Disentangled Multi-Relational Graph Convolutional Network for Pedestrian Trajectory Prediction (AAAI2021)</a><ul>
                        
                <li>
                    <a href="#challeges" aria-label="challeges">challeges</a></li>
                <li>
                    <a href="#contributions" aria-label="contributions">contributions</a></li></ul>
                </li>
                <li>
                    <a href="#gaze-target-estimation" aria-label="Gaze target estimation">Gaze target estimation</a></li>
                <li>
                    <a href="#dual-attention-guided-gaze-target-detection-in-the-wild" aria-label="Dual Attention Guided Gaze Target Detection in the Wild">Dual Attention Guided Gaze Target Detection in the Wild</a></li>
                <li>
                    <a href="#escnet-gaze-target-detection-with-the-understanding-of-3d-scenes" aria-label="ESCNet: Gaze Target Detection with the Understanding of 3D Scenes">ESCNet: Gaze Target Detection with the Understanding of 3D Scenes</a></li>
                <li>
                    <a href="#multimodal-across-domains-gaze-target-detection" aria-label="Multimodal Across Domains Gaze Target Detection">Multimodal Across Domains Gaze Target Detection</a></li>
                <li>
                    <a href="#beyon-3d-siamese-tracking-a-motion-centric-paradigm-for-3d-single-object-tracking-in-point-clouds" aria-label="Beyon 3D Siamese Tracking: A motion-Centric Paradigm for 3D single Object Tracking in Point Clouds">Beyon 3D Siamese Tracking: A motion-Centric Paradigm for 3D single Object Tracking in Point Clouds</a><ul>
                        
                <li>
                    <a href="#motivation" aria-label="Motivation">Motivation</a></li>
                <li>
                    <a href="#framework" aria-label="framework">framework</a></li></ul>
                </li>
                <li>
                    <a href="#motion-cnn" aria-label="Motion CNN">Motion CNN</a><ul>
                        
                <li>
                    <a href="#explanation" aria-label="explanation">explanation</a></li>
                <li>
                    <a href="#framework-1" aria-label="framework">framework</a></li></ul>
                </li>
                <li>
                    <a href="#3d-multi-object-tracking-in-point-clouds-based-on-prediction-confidence-guided-data-association" aria-label="3D Multi-Object Tracking in Point Clouds Based on Prediction confidence-Guided Data Association">3D Multi-Object Tracking in Point Clouds Based on Prediction confidence-Guided Data Association</a></li>
                <li>
                    <a href="#multimodal-trajectory-prediction-conditioned-on-lane-graph-traversals" aria-label="Multimodal Trajectory Prediction Conditioned on Lane-Graph Traversals">Multimodal Trajectory Prediction Conditioned on Lane-Graph Traversals</a><ul>
                        
                <li>
                    <a href="#challenges" aria-label="challenges:">challenges:</a></li>
                <li>
                    <a href="#insights" aria-label="insights">insights</a></li>
                <li>
                    <a href="#method" aria-label="method">method</a><ul>
                        
                <li>
                    <a href="#trajectory-representation" aria-label="Trajectory representation">Trajectory representation</a></li>
                <li>
                    <a href="#hd-maps-as-lane-graphs" aria-label="HD maps as lane graphs">HD maps as lane graphs</a></li>
                <li>
                    <a href="#output-representation" aria-label="Output representation">Output representation</a></li>
                <li>
                    <a href="#model" aria-label="Model">Model</a><ul>
                        
                <li>
                    <a href="#encoding-scene-and-agent-context" aria-label="Encoding scene and agent context">Encoding scene and agent context</a></li>
                <li>
                    <a href="#discrete-policy-for-graph-traversal" aria-label="Discrete policy for graph traversal">Discrete policy for graph traversal</a>
                </li>
            </ul>
            </li>
            </ul>
            </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="pedestrain-trajectory">Pedestrain trajectory<a hidden class="anchor" aria-hidden="true" href="#pedestrain-trajectory">#</a></h1>
<p>Questions:
1. only pedestrain relation considered? hoe about environment
2. what is the general framework for pedestrain trajectory prediction.</p>
<h1 id="sgcn-sparse-graph-convolution-network-for-pedestrain-trajectory-prediction-cvpr2021">SGCN: Sparse Graph Convolution Network for Pedestrain Trajectory Prediction (CVPR2021)<a hidden class="anchor" aria-hidden="true" href="#sgcn-sparse-graph-convolution-network-for-pedestrain-trajectory-prediction-cvpr2021">#</a></h1>
<h2 id="sgcn-framework">SGCN framework<a hidden class="anchor" aria-hidden="true" href="#sgcn-framework">#</a></h2>
<p><img loading="lazy" src="/assets/images/SGCN-framework.png" alt="SGCN framework"  />
</p>
<ul>
<li>superfulous interactions:
<ol>
<li>dense interaction -&gt; one pedestrain is related to all other pedestrains while in fact it is not</li>
<li>sparse undirected -&gt; equal interactions for a pair of pedestrains</li>
</ol>
</li>
<li>spatial GCN -&gt; sparse directed -&gt; not all pedestrains + not equal interaction</li>
<li>temporal GCN -&gt; motion tendency</li>
</ul>
<h1 id="disentangled-multi-relational-graph-convolutional-network-for-pedestrian-trajectory-prediction-aaai2021">Disentangled Multi-Relational Graph Convolutional Network for Pedestrian Trajectory Prediction (AAAI2021)<a hidden class="anchor" aria-hidden="true" href="#disentangled-multi-relational-graph-convolutional-network-for-pedestrian-trajectory-prediction-aaai2021">#</a></h1>
<ul>
<li>Use CNN to generalize complex interpersonal relations</li>
<li>a graph representation: node as pedestrian, edges correspond to distance</li>
</ul>
<h2 id="challeges">challeges<a hidden class="anchor" aria-hidden="true" href="#challeges">#</a></h2>
<ul>
<li>only simple social relationship like collision avoidance is aggregated</li>
<li>modelling social norms is not suitable for determining the end-points of pedestrians in the last frame (over-avoidance)</li>
</ul>
<h2 id="contributions">contributions<a hidden class="anchor" aria-hidden="true" href="#contributions">#</a></h2>
<ul>
<li>disentangled multi-scale aggregation to clearly distinguish between relevant pedestrians</li>
<li>multi-relational GCN to extract sophisticated social interaction in a scene.</li>
<li>a global temporal aggregation to compensate accumulated errors from over-avoidance.</li>
</ul>
<h1 id="gaze-target-estimation">Gaze target estimation<a hidden class="anchor" aria-hidden="true" href="#gaze-target-estimation">#</a></h1>
<h1 id="dual-attention-guided-gaze-target-detection-in-the-wild">Dual Attention Guided Gaze Target Detection in the Wild<a hidden class="anchor" aria-hidden="true" href="#dual-attention-guided-gaze-target-detection-in-the-wild">#</a></h1>
<ul>
<li>challenges:
<ol>
<li>prior works: gaze direction in 2D representations + barely depth-channel gaze. forward, backward, sideward. (refer to one person)</li>
<li>salient objects from 2d cues. two candidates different depth along same gaze direction, hard to tell apart. (refer to one person with the scene understanding)</li>
<li>fixation inconsistency: facing forward looking downward.</li>
</ol>
</li>
<li>solutions:
<ol>
<li>estimate 3D gaze direction from head image.</li>
<li>dual attention module -&gt; depth-aware perspective in the scene</li>
<li>?hasn&rsquo;t found effective solution in the paper? -&gt; solved in the first stage? so is it still an independent challenge? or should be merged to the first challenge?</li>
</ol>
</li>
</ul>
<h1 id="escnet-gaze-target-detection-with-the-understanding-of-3d-scenes">ESCNet: Gaze Target Detection with the Understanding of 3D Scenes<a hidden class="anchor" aria-hidden="true" href="#escnet-gaze-target-detection-with-the-understanding-of-3d-scenes">#</a></h1>
<ul>
<li>hard to handle: multiple salient objects lie in the same depth layer and field of view. occlusion plays an important role in gaze estimation , one cannot see through occluders.</li>
<li>3D geometry could br reconstructed by absolute depth and camera parameters</li>
</ul>
<p>FOV-based initial heatmap.</p>
<h1 id="multimodal-across-domains-gaze-target-detection">Multimodal Across Domains Gaze Target Detection<a hidden class="anchor" aria-hidden="true" href="#multimodal-across-domains-gaze-target-detection">#</a></h1>
<ul>
<li>gaze target detection also referred as gaze-following</li>
<li>false detections occur when there are multiple object-of-interests at different depths but along with the subject&rsquo;s gaze direction</li>
<li>&lsquo;dual&rsquo; potentially being error-prone in real-life processing, e.g., when the eyes are not visible or not detectable</li>
<li>&lsquo;ESCNET&rsquo; auxiliary estimating depth, depend on depth and pseudo labels.</li>
<li>not require gaze angles as supervision</li>
<li>only spatial processing, but still detect gaze target</li>
<li>use depth image</li>
</ul>
<p><strong>how different modalities should be jointly learned for performing effective gaze target detection</strong></p>
<ul>
<li>solve domain shift problem</li>
</ul>
<h1 id="beyon-3d-siamese-tracking-a-motion-centric-paradigm-for-3d-single-object-tracking-in-point-clouds">Beyon 3D Siamese Tracking: A motion-Centric Paradigm for 3D single Object Tracking in Point Clouds<a hidden class="anchor" aria-hidden="true" href="#beyon-3d-siamese-tracking-a-motion-centric-paradigm-for-3d-single-object-tracking-in-point-clouds">#</a></h1>
<ul>
<li>point clouds are usually textureless and incomplete, which hinders effective appearance matching.</li>
<li>Overlook motion clues among targets.</li>
<li>propose a motion-centric paradigm
<ol>
<li>$M^2-Track$ localizes the target within successive frames via motion transformation.</li>
<li>refines the target box through motion-assisted shape completion</li>
</ol>
</li>
</ul>
<h2 id="motivation">Motivation<a hidden class="anchor" aria-hidden="true" href="#motivation">#</a></h2>
<p><img loading="lazy" src="/assets/images/m2track.png" alt="motivaation"  />
</p>
<ul>
<li>the upper one obtain a canonical target template using the previous target box and search for the target in thecurrent frame according to the matching similarity. It is sensitive to distractors</li>
<li>The bottom one learns relative target motion from two consecutive frames. Robustly localize the target.</li>
</ul>
<h2 id="framework">framework<a hidden class="anchor" aria-hidden="true" href="#framework">#</a></h2>
<p><img loading="lazy" src="/assets/images/m2track-framework.png" alt="framework"  />
</p>
<ul>
<li>
<p>input: last point cloud, last 3D BBox, current point cloud</p>
</li>
<li>
<p>output current 3D BBox.</p>
</li>
<li>
<p>? how many frame used and how many frame predicted.</p>
<ul>
<li>1 frame used, 1 frame predicted.</li>
</ul>
</li>
<li>
<p>quick comment: this task is not suitable for our goal. But it gives a good benchmark for kitti dataset.</p>
</li>
</ul>
<h1 id="motion-cnn">Motion CNN<a hidden class="anchor" aria-hidden="true" href="#motion-cnn">#</a></h1>
<ul>
<li>plan a safe and efficient route.</li>
<li>a baseline form multimodal motion prediction.</li>
</ul>
<h2 id="explanation">explanation<a hidden class="anchor" aria-hidden="true" href="#explanation">#</a></h2>
<p><img loading="lazy" src="/assets/images/motioncnn.png" alt="motion-cnn"  />
</p>
<h2 id="framework-1">framework<a hidden class="anchor" aria-hidden="true" href="#framework-1">#</a></h2>
<p><img loading="lazy" src="/assets/images/motioncnn-framework.png" alt="motioncnn-framework"  />
</p>
<ul>
<li>input: image</li>
<li>output trajectories.</li>
<li>use 1 second of images to predict 8 seconds&rsquo;.</li>
</ul>
<h1 id="3d-multi-object-tracking-in-point-clouds-based-on-prediction-confidence-guided-data-association">3D Multi-Object Tracking in Point Clouds Based on Prediction confidence-Guided Data Association<a hidden class="anchor" aria-hidden="true" href="#3d-multi-object-tracking-in-point-clouds-based-on-prediction-confidence-guided-data-association">#</a></h1>
<ul>
<li>3D multi-object tracker to more robustly track objects that are temporarily missed by detectors.</li>
<li>a predictor employs constant acceleration motion model to estimate future positions and prediction confidence.</li>
<li>a new pairwise cost.</li>
</ul>
<h1 id="multimodal-trajectory-prediction-conditioned-on-lane-graph-traversals">Multimodal Trajectory Prediction Conditioned on Lane-Graph Traversals<a hidden class="anchor" aria-hidden="true" href="#multimodal-trajectory-prediction-conditioned-on-lane-graph-traversals">#</a></h1>
<ul>
<li>Predicting the future motion of surrounding vehicles requires reasoning about the inherent uncertainty in driving behavior.</li>
<li>Early works encodes HD maps using a rasterized bird&rsquo;s eye view image.</li>
<li>Recent wotks represent lane polylines as nodes of a graph.</li>
<li>Represent HD map as a graph and encode the input context into a single context vector.</li>
</ul>
<h2 id="challenges">challenges:<a hidden class="anchor" aria-hidden="true" href="#challenges">#</a></h2>
<ul>
<li>The prediction header go off the road, violate traffic rules because of the complex mapping.</li>
<li>lateral or route variability (e.g. will the driver change lane, will they turn right etc.). longitudinal variability (will the driver accelerate, brake, maintain spped)</li>
</ul>
<h2 id="insights">insights<a hidden class="anchor" aria-hidden="true" href="#insights">#</a></h2>
<ul>
<li>graph structure of the scene explicitly model the lateral or route variability in trajectories</li>
<li>predictions conditioned on traversals:
<ol>
<li>selectively aggregate part of scene context by <strong>sampling path traversals from a learned policy</strong></li>
<li>it lessens representational demands  on the output decoder.</li>
</ol>
<ul>
<li>the probabilistic policy leads to adiverse set of sampled paths.</li>
</ul>
</li>
<li>latent variable for longitudinal variability:
<ol>
<li>condition the predictions with a sampled latent variable.</li>
<li>enable to predict distinct trajectories even for identical path traversals.</li>
</ol>
</li>
</ul>
<h2 id="method">method<a hidden class="anchor" aria-hidden="true" href="#method">#</a></h2>
<ul>
<li>predict the future trajectories of vehicles of interest,</li>
<li>conditioned on
<ol>
<li>their past trajectories</li>
<li>the past trajectories of nearby vehicles and pedestrians</li>
<li>the HD map of the scene</li>
</ol>
</li>
<li>represent the scene, predict in the bird&rsquo;s eye view and use an agent-centric frame of reference aligned along the agent&rsquo;s instantaneous direction of motion.</li>
</ul>
<h3 id="trajectory-representation">Trajectory representation<a hidden class="anchor" aria-hidden="true" href="#trajectory-representation">#</a></h3>
<p>$$s_{-t_h:0}^i=[s_{-t_h}^i,\cdots,s_{-1}^i,s_{0}^i]$$
$$s_t^i = [x_t^i, y_t^i, v_t^i, a_t^i, w_t^i,I_t^i]$$</p>
<h3 id="hd-maps-as-lane-graphs">HD maps as lane graphs<a hidden class="anchor" aria-hidden="true" href="#hd-maps-as-lane-graphs">#</a></h3>
<ul>
<li>Nodes: lane centerlines $f_{1:N}^v = [f_{1}^v, cdots,f_{N}^v ]$, $f_n^v = [x_n^v, y_n^v, \theta_n^v, I_n^v]$</li>
<li>Edges: successor edges-&gt;legal route; proximal edges-&gt; lane changes</li>
</ul>
<h3 id="output-representation">Output representation<a hidden class="anchor" aria-hidden="true" href="#output-representation">#</a></h3>
<p>K trajectories.</p>
<h3 id="model">Model<a hidden class="anchor" aria-hidden="true" href="#model">#</a></h3>
<ul>
<li>graph encoder: forms the backbone of our model, outputs representations for each node of the lane graph, incorporate HD map and surrounding agent context.</li>
<li>policy header: output a discrete probability distribution over outgoing edeges at each node, allowing us to sample paths in the graph.</li>
<li>trajectory decoder: output trajectories conditioned on paths traversed by the policy and a sampled latent variable.</li>
</ul>
<h4 id="encoding-scene-and-agent-context">Encoding scene and agent context<a hidden class="anchor" aria-hidden="true" href="#encoding-scene-and-agent-context">#</a></h4>
<ul>
<li>GRU encoders: target vehicle trajectory $s_{-t_h:0}^0$ -&gt; $h_{motion}$; surrounding vehicle trajectories $s_{-t_h:0}^i$ -&gt; $h_{agent}^i$; node features $f_{1:N}^v$-&gt;$h_{node}^v$.</li>
<li>agent node attention: update node encodings with nearby agent encodings. keys and values from $h_{agent}^i$, query by $h_{node}^v$</li>
<li>output of agent-node-attention as the attention mask for GAT. Only attended nodes are employed for updating</li>
</ul>
<p><strong>An explanation of Q, K, V</strong></p>
<p>The Seq2Seq task always has encoder+decoder. The encoders job is to take in an input sequence and output a context vector/thought vector. The context vector is then input to decoder.</p>
<p>However, the performance drops drastically for longer sentences.</p>
<p>One solution is to use skip-connection, by simple concatenation or summing up. -&gt; assume all hidden states are equally important.</p>
<p>So attention (dynamic weighting) is brought in.</p>
<hr>
<blockquote>
<p>An attention mechanism calculates the dynamic weights representing the relative importance of the inputs in the sequence (<strong>keys</strong>) for the particular output (<strong>query</strong>). Multiplying the dynamic weights with the input sequence (<strong>values</strong>) will then weight the sequence.</p>
</blockquote>
<p><img loading="lazy" src="/assets/images/qkv.png" alt="qkv"  />

<img loading="lazy" src="/assets/images/calc_qkv.png" alt="calc_qkv"  />
</p>
<blockquote>
<p>The exact values for Q,K, and V depend on exactly which attention mechnism is being referred to. For the trasformer, 3 types: 1. Encoder Attention, 2, Decoder Attention, 3. Encoder-decoder Attention. <a href="https://medium.com/@b.terryjack/deep-learning-the-transformer-9ae5e9c5a190">(Here for reference)</a></p>
</blockquote>
<hr>
<h4 id="discrete-policy-for-graph-traversal">Discrete policy for graph traversal<a hidden class="anchor" aria-hidden="true" href="#discrete-policy-for-graph-traversal">#</a></h4>
<ul>
<li>A policy $\pi_{route}$ for graph traversal -&gt; sampled roll-outs of the policy correspond to likely routes the target might take in the future.</li>
<li>policy: a discrete probability distribution over outgoing edges at each node.</li>
<li>additionally add an edge to an <em>end</em> state, terminate at a goal location.
$$score(u,v)=MLP(concat(h_{motion}, h_{node}^u, h_{node}^v, 1_{(u,v)\in E}))$$
$$\pi_{route}(v|u) = softmax({score(u,v)|(u,v)\in E})$$
$$L_{BC} = \sum_{(u,v)\in E_{g,t}}-\log(\pi_{route}(v|u))$$</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://x423xu.github.io/tags/paper-reading/">paper-reading</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://x423xu.github.io/posts/2022-10-27-spectral-gnn/">
    <span class="title">« Prev</span>
    <br>
    <span>Spectral Gnn</span>
  </a>
  <a class="next" href="https://x423xu.github.io/posts/2022-07-24-point-cloud-cvpr2022/">
    <span class="title">Next »</span>
    <br>
    <span>point-cloud-CVPR2022</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share pedestrain-trajectory on twitter"
        href="https://twitter.com/intent/tweet/?text=pedestrain-trajectory&amp;url=https%3a%2f%2fx423xu.github.io%2fposts%2f2022-08-17-pedestrain-trajectory%2f&amp;hashtags=paper-reading">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://x423xu.github.io/">Xiaoyu&#39;s blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
