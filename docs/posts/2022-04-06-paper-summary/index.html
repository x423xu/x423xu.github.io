<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Paper summary | Xiaoyu&#39;s blog</title>
<meta name="keywords" content="paper-reading">
<meta name="description" content="2022/3/31 ~ 2022/4/6 Order-Embeddings of images and Language Core idea Explicitly modeling the partial order structure of the hierarchy over language and image -&gt; Visual sematic hierarchy How to do it Penalize order violations $$ E(x,y) = ||max(0,y-x)||^2 $$ where $E(x,y)=0 \Leftrightarrow x \preceq y$ Modeling heterogeneous hierarchies with relation-specific hyperbolic cones Core idea Embeds entities into hyperbolic cones &amp; models relations as transformations between the cones How to do it Poincare entailment cone at apex $x$ $$\zeta_x = {y\in \Beta^d | \angle_xy\leq\sin^{-1}(K\frac{1-||x||^2}{||x||})}$$ embed entity: $h=(h_1,h_2,\cdots,h_{d})$, where $h_i\in\Beta^2$ is the apex of the $i-$th 2D hyperbolic cone.">
<meta name="author" content="">
<link rel="canonical" href="https://x423xu.github.io/posts/2022-04-06-paper-summary/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css" integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://x423xu.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://x423xu.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://x423xu.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://x423xu.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://x423xu.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<script>
    MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true
        },
        options: {
            skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        }
    };

    window.addEventListener('load', (event) => {
        document.querySelectorAll("mjx-container").forEach(function (x) {
            x.parentElement.classList += 'has-jax'
        })
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta property="og:title" content="Paper summary" />
<meta property="og:description" content="2022/3/31 ~ 2022/4/6 Order-Embeddings of images and Language Core idea Explicitly modeling the partial order structure of the hierarchy over language and image -&gt; Visual sematic hierarchy How to do it Penalize order violations $$ E(x,y) = ||max(0,y-x)||^2 $$ where $E(x,y)=0 \Leftrightarrow x \preceq y$ Modeling heterogeneous hierarchies with relation-specific hyperbolic cones Core idea Embeds entities into hyperbolic cones &amp; models relations as transformations between the cones How to do it Poincare entailment cone at apex $x$ $$\zeta_x = {y\in \Beta^d | \angle_xy\leq\sin^{-1}(K\frac{1-||x||^2}{||x||})}$$ embed entity: $h=(h_1,h_2,\cdots,h_{d})$, where $h_i\in\Beta^2$ is the apex of the $i-$th 2D hyperbolic cone." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://x423xu.github.io/posts/2022-04-06-paper-summary/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-04-06T11:58:35-04:00" />
<meta property="article:modified_time" content="2022-04-06T11:58:35-04:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Paper summary"/>
<meta name="twitter:description" content="2022/3/31 ~ 2022/4/6 Order-Embeddings of images and Language Core idea Explicitly modeling the partial order structure of the hierarchy over language and image -&gt; Visual sematic hierarchy How to do it Penalize order violations $$ E(x,y) = ||max(0,y-x)||^2 $$ where $E(x,y)=0 \Leftrightarrow x \preceq y$ Modeling heterogeneous hierarchies with relation-specific hyperbolic cones Core idea Embeds entities into hyperbolic cones &amp; models relations as transformations between the cones How to do it Poincare entailment cone at apex $x$ $$\zeta_x = {y\in \Beta^d | \angle_xy\leq\sin^{-1}(K\frac{1-||x||^2}{||x||})}$$ embed entity: $h=(h_1,h_2,\cdots,h_{d})$, where $h_i\in\Beta^2$ is the apex of the $i-$th 2D hyperbolic cone."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://x423xu.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Paper summary",
      "item": "https://x423xu.github.io/posts/2022-04-06-paper-summary/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Paper summary",
  "name": "Paper summary",
  "description": "2022/3/31 ~ 2022/4/6 Order-Embeddings of images and Language Core idea Explicitly modeling the partial order structure of the hierarchy over language and image -\u0026gt; Visual sematic hierarchy How to do it Penalize order violations $$ E(x,y) = ||max(0,y-x)||^2 $$ where $E(x,y)=0 \\Leftrightarrow x \\preceq y$ Modeling heterogeneous hierarchies with relation-specific hyperbolic cones Core idea Embeds entities into hyperbolic cones \u0026amp; models relations as transformations between the cones How to do it Poincare entailment cone at apex $x$ $$\\zeta_x = {y\\in \\Beta^d | \\angle_xy\\leq\\sin^{-1}(K\\frac{1-||x||^2}{||x||})}$$ embed entity: $h=(h_1,h_2,\\cdots,h_{d})$, where $h_i\\in\\Beta^2$ is the apex of the $i-$th 2D hyperbolic cone.",
  "keywords": [
    "paper-reading"
  ],
  "articleBody": "2022/3/31 ~ 2022/4/6 Order-Embeddings of images and Language Core idea Explicitly modeling the partial order structure of the hierarchy over language and image -\u003e Visual sematic hierarchy How to do it Penalize order violations $$ E(x,y) = ||max(0,y-x)||^2 $$ where $E(x,y)=0 \\Leftrightarrow x \\preceq y$ Modeling heterogeneous hierarchies with relation-specific hyperbolic cones Core idea Embeds entities into hyperbolic cones \u0026 models relations as transformations between the cones How to do it Poincare entailment cone at apex $x$ $$\\zeta_x = {y\\in \\Beta^d | \\angle_xy\\leq\\sin^{-1}(K\\frac{1-||x||^2}{||x||})}$$ embed entity: $h=(h_1,h_2,\\cdots,h_{d})$, where $h_i\\in\\Beta^2$ is the apex of the $i-$th 2D hyperbolic cone. Transformation: Rotation transformation: $f_1(h,r) = \\exp_o(\\mathbf{G}(\\theta)\\log_o(h))$, where $\\mathbf{G(\\theta)=\\left[\\begin{matrix}cos(\\theta)\u0026-sin(\\theta)\\sin(\\theta)\u0026cos(\\theta)\\end{matrix}\\right]}$ Restricted rotation transformation:$f_2(h,r)=\\exp_h(s\\cdot \\mathbf{G}(\\theta\\frac{\\phi}{\\pi})\\bar h), r=(s,\\theta)$ Loss function: $\\ell_d=-\\log\\sigma(\\psi(h,r,t))-\\sum_{t^{’}}\\frac{1}{|T|}\\log\\sigma(-\\psi(h,r,t^{’}))$ $\\ell_a(h,r,t)=m\\cdot(max(0,\\angle_{h_i}t_i-\\phi(h_i)))$ Learning to detect unseen object classes by between-class attribute transfer Core idea Decompose an image/object into high-level semantic attributes such as shape, color, geographic information. Then detect new classes based on their attribute representations. How to do it Direct attribute prediction: decouple training images into attribute variables $\\alpha_m$ per-attribute parameters $\\beta_m$ at test: image $x\\rightarrow\\beta_m\\rightarrow \\alpha_m\\rightarrow class$ $$p(z|x)=\\sum_{a\\in{0,1}^M}p(z|a)p(a|x) = \\frac{p(z)}{p(a^z)}\\prod_{m=1}^{M}p(a_m^z|x)$$ Indirect attribute prediction add an attribute layer between two label layers for the indirect one, $p(a_m|x)=\\sum_{k=1}^K p(a_m|y_k)p(y_k|x)$ Zero-shot learning via joint similarity embedding Core idea To test if source-target pair matches each other Multi-grained vision language pre-training: aligning texts with visual concepts Core idea Locate visual concepts in the image given the associated texts \u0026 align the texts with the visual concepts. How to do it Bounding box loss: $$\\ell_{bbox} = \\mathbb{E}{(V^j,T^j)\\sim I;I\\sim D}\\left[\\ell{iou}(b_j,\\hat{b}_j)+||b_j-\\hat{b}_j||_1\\right]$$ Contrastive learning: $$p^{v2t}(V) = \\frac{\\exp(s(V,t)/\\tau)}{\\sum_{i=1}^{N}\\exp(s(V,T^i)/\\tau)}$$ $$p^{t2v}(T) = \\frac{\\exp(s(V,t)/\\tau)}{\\sum_{i=1}^{N}\\exp(s(V^i,T)/\\tau)}$$ $$\\ell_{cl}=\\frac{1}{2}\\mathbb{E}_{V,T\\sim D}\\left[H(y^{v2t}(V),p^{v2t}(V))+H(y^{t2v}(T),p^{t2v}(T))\\right]$$ Match prediction: whether a pair of visual concept and text is matched. $$\\ell_{match} = \\mathbb{E}_{V,T\\sim D}H(y^{match},p^{match}(V,T))$$ Masked Language Modeling: predict the masked words in the text based on the visual concept $$\\ell_{mlm} = \\mathbb{E}_{t_j\\sim \\hat{T};(V,\\hat{T}\\sim D)}H(y^j, p^j(V,\\hat{T}))$$ Visual Turing test for computer vision system Abstract An operator-assisted device that produces a stochastic sequence of binary questions from a given test image. The query engine proposes a question, the operator either provides correct answer or rejects the question as ambiguous. The system is designed to produce streams of questions that follw natural storylines, from the instantiation of a unique object, through an exploration of its properties, and on to its relationships with other uniquely instantiated objects. Introduction Alan Turing proposed that the ultimate test of a machine could think ot think at least as well as a person was for a human judge to be unable to tell which was which based on natural language conversations in an appropriately cloaked scenario. Or how well a computer can imitate a human. Semantic image interpretation-image to words Research shows that human brain can only think about one idea at a time\nVisual grounding overview TransVG: End-to-End Visual Grounding with Transformers Introduction Two-stage methods measure the similarity between region and expression embedding with an MLP. One-stage methods encode the language vector to visual feature by direct concatenation. Challenges: lead to sub-optimal results on long and complex language expressions. Built on pre-defined structures of language queries or image scenes. Ground object in an indirect way: candidates are sparse region proposals, or dense anchors. So the performance would be influenced by the proposals. Zero-Shot Robot Manipulation from Passive Human Videos Learn robot manipulation only by watching videos of humans doing arbitrary tasks in unstructured settings ",
  "wordCount" : "548",
  "inLanguage": "en",
  "datePublished": "2022-04-06T11:58:35-04:00",
  "dateModified": "2022-04-06T11:58:35-04:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://x423xu.github.io/posts/2022-04-06-paper-summary/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Xiaoyu's blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://x423xu.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://x423xu.github.io/" accesskey="h" title="Xiaoyu&#39;s blog (Alt + H)">Xiaoyu&#39;s blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://x423xu.github.io/" title="home">
                    <span>home</span>
                </a>
            </li>
            <li>
                <a href="https://x423xu.github.io/archives/" title="archives">
                    <span>archives</span>
                </a>
            </li>
            <li>
                <a href="https://x423xu.github.io/categories/" title="categories">
                    <span>categories</span>
                </a>
            </li>
            <li>
                <a href="https://x423xu.github.io/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
            <li>
                <a href="https://x423xu.github.io/search/" title="search (Alt &#43; /)" accesskey=/>
                    <span>search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://x423xu.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://x423xu.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      Paper summary
    </h1>
    <div class="post-meta"><span title='2022-04-06 11:58:35 -0400 EDT'>April 6, 2022</span>&nbsp;·&nbsp;3 min

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#2022331--202246" aria-label="2022/3/31 ~ 2022/4/6">2022/3/31 ~ 2022/4/6</a><ul>
                        
                <li>
                    <a href="#order-embeddings-of-images-and-language" aria-label="Order-Embeddings of images and Language">Order-Embeddings of images and Language</a><ul>
                        
                <li>
                    <a href="#core-idea" aria-label="Core idea">Core idea</a></li>
                <li>
                    <a href="#how-to-do-it" aria-label="How to do it">How to do it</a></li></ul>
                </li>
                <li>
                    <a href="#modeling-heterogeneous-hierarchies-with-relation-specific-hyperbolic-cones" aria-label="Modeling heterogeneous hierarchies with relation-specific hyperbolic cones">Modeling heterogeneous hierarchies with relation-specific hyperbolic cones</a><ul>
                        
                <li>
                    <a href="#core-idea-1" aria-label="Core idea">Core idea</a></li>
                <li>
                    <a href="#how-to-do-it-1" aria-label="How to do it">How to do it</a></li></ul>
                </li>
                <li>
                    <a href="#learning-to-detect-unseen-object-classes-by-between-class-attribute-transfer" aria-label="Learning to detect unseen object classes by between-class attribute transfer">Learning to detect unseen object classes by between-class attribute transfer</a><ul>
                        
                <li>
                    <a href="#core-idea-2" aria-label="Core idea">Core idea</a></li>
                <li>
                    <a href="#how-to-do-it-2" aria-label="How to do it">How to do it</a></li></ul>
                </li>
                <li>
                    <a href="#zero-shot-learning-via-joint-similarity-embedding" aria-label="Zero-shot learning via joint similarity embedding">Zero-shot learning via joint similarity embedding</a><ul>
                        
                <li>
                    <a href="#core-idea-3" aria-label="Core idea">Core idea</a></li></ul>
                </li>
                <li>
                    <a href="#multi-grained-vision-language-pre-training-aligning-texts-with-visual-concepts" aria-label="Multi-grained vision language pre-training: aligning texts with visual concepts">Multi-grained vision language pre-training: aligning texts with visual concepts</a><ul>
                        
                <li>
                    <a href="#core-idea-4" aria-label="Core idea">Core idea</a></li>
                <li>
                    <a href="#how-to-do-it-3" aria-label="How to do it">How to do it</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#visual-turing-test-for-computer-vision-system" aria-label="Visual Turing test for computer vision system">Visual Turing test for computer vision system</a><ul>
                        
                <li>
                    <a href="#abstract" aria-label="Abstract">Abstract</a></li>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li></ul>
                </li>
                <li>
                    <a href="#visual-grounding-overview" aria-label="Visual grounding overview">Visual grounding overview</a><ul>
                        
                <li>
                    <a href="#transvg-end-to-end-visual-grounding-with-transformers" aria-label="TransVG: End-to-End Visual Grounding with Transformers">TransVG: End-to-End Visual Grounding with Transformers</a><ul>
                        
                <li>
                    <a href="#introduction-1" aria-label="Introduction">Introduction</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#zero-shot-robot-manipulation-from-passive-human-videos" aria-label="Zero-Shot Robot Manipulation from Passive Human Videos">Zero-Shot Robot Manipulation from Passive Human Videos</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="2022331--202246">2022/3/31 ~ 2022/4/6<a hidden class="anchor" aria-hidden="true" href="#2022331--202246">#</a></h1>
<h2 id="order-embeddings-of-images-and-language">Order-Embeddings of images and Language<a hidden class="anchor" aria-hidden="true" href="#order-embeddings-of-images-and-language">#</a></h2>
<h3 id="core-idea">Core idea<a hidden class="anchor" aria-hidden="true" href="#core-idea">#</a></h3>
<ul>
<li>Explicitly modeling the partial order structure of the hierarchy over language and image -&gt; <strong>Visual sematic hierarchy</strong></li>
</ul>
<h3 id="how-to-do-it">How to do it<a hidden class="anchor" aria-hidden="true" href="#how-to-do-it">#</a></h3>
<ul>
<li>Penalize order violations
$$ E(x,y) = ||max(0,y-x)||^2 $$
where $E(x,y)=0 \Leftrightarrow x \preceq y$</li>
</ul>
<h2 id="modeling-heterogeneous-hierarchies-with-relation-specific-hyperbolic-cones">Modeling heterogeneous hierarchies with relation-specific hyperbolic cones<a hidden class="anchor" aria-hidden="true" href="#modeling-heterogeneous-hierarchies-with-relation-specific-hyperbolic-cones">#</a></h2>
<h3 id="core-idea-1">Core idea<a hidden class="anchor" aria-hidden="true" href="#core-idea-1">#</a></h3>
<ul>
<li>Embeds entities into hyperbolic cones &amp; models relations as transformations between the cones</li>
</ul>
<h3 id="how-to-do-it-1">How to do it<a hidden class="anchor" aria-hidden="true" href="#how-to-do-it-1">#</a></h3>
<ul>
<li>Poincare entailment cone at apex $x$ $$\zeta_x = {y\in \Beta^d | \angle_xy\leq\sin^{-1}(K\frac{1-||x||^2}{||x||})}$$</li>
<li>embed entity: $h=(h_1,h_2,\cdots,h_{d})$, where $h_i\in\Beta^2$ is the apex of the $i-$th 2D hyperbolic cone.</li>
<li>Transformation:
<ol>
<li>Rotation transformation: $f_1(h,r) = \exp_o(\mathbf{G}(\theta)\log_o(h))$, where $\mathbf{G(\theta)=\left[\begin{matrix}cos(\theta)&amp;-sin(\theta)\sin(\theta)&amp;cos(\theta)\end{matrix}\right]}$</li>
<li>Restricted rotation transformation:$f_2(h,r)=\exp_h(s\cdot \mathbf{G}(\theta\frac{\phi}{\pi})\bar h), r=(s,\theta)$</li>
</ol>
</li>
<li>Loss function:
<ol>
<li>$\ell_d=-\log\sigma(\psi(h,r,t))-\sum_{t^{&rsquo;}}\frac{1}{|T|}\log\sigma(-\psi(h,r,t^{&rsquo;}))$</li>
<li>$\ell_a(h,r,t)=m\cdot(max(0,\angle_{h_i}t_i-\phi(h_i)))$</li>
</ol>
</li>
</ul>
<h2 id="learning-to-detect-unseen-object-classes-by-between-class-attribute-transfer">Learning to detect unseen object classes by between-class attribute transfer<a hidden class="anchor" aria-hidden="true" href="#learning-to-detect-unseen-object-classes-by-between-class-attribute-transfer">#</a></h2>
<h3 id="core-idea-2">Core idea<a hidden class="anchor" aria-hidden="true" href="#core-idea-2">#</a></h3>
<ul>
<li>Decompose an image/object into high-level semantic attributes such as shape, color, geographic information. Then detect new classes based on their attribute representations.</li>
</ul>
<h3 id="how-to-do-it-2">How to do it<a hidden class="anchor" aria-hidden="true" href="#how-to-do-it-2">#</a></h3>
<ul>
<li>Direct attribute prediction:
<ol>
<li>decouple training images into attribute variables $\alpha_m$</li>
<li>per-attribute parameters $\beta_m$</li>
<li>at test: image $x\rightarrow\beta_m\rightarrow \alpha_m\rightarrow class$
$$p(z|x)=\sum_{a\in{0,1}^M}p(z|a)p(a|x) = \frac{p(z)}{p(a^z)}\prod_{m=1}^{M}p(a_m^z|x)$$</li>
</ol>
</li>
<li>Indirect attribute prediction
<ol>
<li>add an attribute layer between two label layers</li>
<li>for the indirect one, $p(a_m|x)=\sum_{k=1}^K p(a_m|y_k)p(y_k|x)$</li>
</ol>
</li>
</ul>
<h2 id="zero-shot-learning-via-joint-similarity-embedding">Zero-shot learning via joint similarity embedding<a hidden class="anchor" aria-hidden="true" href="#zero-shot-learning-via-joint-similarity-embedding">#</a></h2>
<h3 id="core-idea-3">Core idea<a hidden class="anchor" aria-hidden="true" href="#core-idea-3">#</a></h3>
<ul>
<li>To test if source-target pair matches each other</li>
</ul>
<h2 id="multi-grained-vision-language-pre-training-aligning-texts-with-visual-concepts">Multi-grained vision language pre-training: aligning texts with visual concepts<a hidden class="anchor" aria-hidden="true" href="#multi-grained-vision-language-pre-training-aligning-texts-with-visual-concepts">#</a></h2>
<h3 id="core-idea-4">Core idea<a hidden class="anchor" aria-hidden="true" href="#core-idea-4">#</a></h3>
<ul>
<li>Locate visual concepts in the image given the associated texts &amp; align the texts with the visual concepts.</li>
</ul>
<h3 id="how-to-do-it-3">How to do it<a hidden class="anchor" aria-hidden="true" href="#how-to-do-it-3">#</a></h3>
<ul>
<li>Bounding box loss:
$$\ell_{bbox} = \mathbb{E}<em>{(V^j,T^j)\sim I;I\sim D}\left[\ell</em>{iou}(b_j,\hat{b}_j)+||b_j-\hat{b}_j||_1\right]$$</li>
<li>Contrastive learning:
$$p^{v2t}(V) = \frac{\exp(s(V,t)/\tau)}{\sum_{i=1}^{N}\exp(s(V,T^i)/\tau)}$$
$$p^{t2v}(T) = \frac{\exp(s(V,t)/\tau)}{\sum_{i=1}^{N}\exp(s(V^i,T)/\tau)}$$
$$\ell_{cl}=\frac{1}{2}\mathbb{E}_{V,T\sim D}\left[H(y^{v2t}(V),p^{v2t}(V))+H(y^{t2v}(T),p^{t2v}(T))\right]$$</li>
<li>Match prediction: whether a pair of visual concept and text is matched.
$$\ell_{match} = \mathbb{E}_{V,T\sim D}H(y^{match},p^{match}(V,T))$$</li>
<li>Masked Language Modeling: predict the masked words in the text based on the visual concept
$$\ell_{mlm} = \mathbb{E}_{t_j\sim \hat{T};(V,\hat{T}\sim D)}H(y^j, p^j(V,\hat{T}))$$</li>
</ul>
<hr>
<h1 id="visual-turing-test-for-computer-vision-system">Visual Turing test for computer vision system<a hidden class="anchor" aria-hidden="true" href="#visual-turing-test-for-computer-vision-system">#</a></h1>
<h2 id="abstract">Abstract<a hidden class="anchor" aria-hidden="true" href="#abstract">#</a></h2>
<ul>
<li>An operator-assisted device that produces a stochastic sequence of binary questions from a given test image. The query engine proposes a question, the operator either provides correct answer or rejects the question as ambiguous.</li>
<li>The system is designed to produce streams of questions that follw natural storylines, from the instantiation of a unique object, through an exploration of its properties, and on to its relationships with other uniquely instantiated objects.</li>
</ul>
<h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<ul>
<li>Alan Turing proposed that the ultimate test of a machine could think ot think at least as well as a person was for a human judge to be unable to tell which was which based on natural language conversations in an appropriately cloaked scenario. Or how well a computer can imitate a human.</li>
<li>Semantic image interpretation-image to words</li>
</ul>
<p><strong>Research shows that human brain can only think about one idea at a time</strong></p>
<hr>
<h1 id="visual-grounding-overview">Visual grounding overview<a hidden class="anchor" aria-hidden="true" href="#visual-grounding-overview">#</a></h1>
<h2 id="transvg-end-to-end-visual-grounding-with-transformers">TransVG: End-to-End Visual Grounding with Transformers<a hidden class="anchor" aria-hidden="true" href="#transvg-end-to-end-visual-grounding-with-transformers">#</a></h2>
<!-- raw HTML omitted -->
<h3 id="introduction-1">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction-1">#</a></h3>
<ul>
<li>Two-stage methods measure the similarity between region and expression embedding with an MLP.</li>
<li>One-stage methods encode the language vector to visual feature by direct concatenation.</li>
<li>Challenges:
<ol>
<li>lead to sub-optimal results on long and complex language expressions.</li>
<li>Built on pre-defined structures of language queries or image scenes.</li>
<li>Ground object in an indirect way: candidates are sparse region proposals, or dense anchors. So the performance would be influenced by the proposals.</li>
</ol>
</li>
</ul>
<h1 id="zero-shot-robot-manipulation-from-passive-human-videos">Zero-Shot Robot Manipulation from Passive Human Videos<a hidden class="anchor" aria-hidden="true" href="#zero-shot-robot-manipulation-from-passive-human-videos">#</a></h1>
<ul>
<li>Learn robot manipulation only by watching videos of humans doing arbitrary tasks in unstructured settings</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://x423xu.github.io/tags/paper-reading/">paper-reading</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://x423xu.github.io/posts/2022-04-09-logical-syntax/">
    <span class="title">« Prev</span>
    <br>
    <span>Logical Syntax</span>
  </a>
  <a class="next" href="https://x423xu.github.io/posts/2022-03-20-x11-for-vscode/">
    <span class="title">Next »</span>
    <br>
    <span>RemoteX11 configuration on vscode</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Paper summary on twitter"
        href="https://twitter.com/intent/tweet/?text=Paper%20summary&amp;url=https%3a%2f%2fx423xu.github.io%2fposts%2f2022-04-06-paper-summary%2f&amp;hashtags=paper-reading">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://x423xu.github.io/">Xiaoyu&#39;s blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
