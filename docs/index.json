[{"content":"Introduction Transformers have been tremendously successful in natural language processing tasks and other domains. Many variants have been suggested. Descriptions of transformers are usually graphical, verbal, partial, or incremental. No pseudocode has ever been published for any variant. ","permalink":"https://x423xu.github.io/posts/formal-algorithm-for-transformers/","summary":"Introduction Transformers have been tremendously successful in natural language processing tasks and other domains. Many variants have been suggested. Descriptions of transformers are usually graphical, verbal, partial, or incremental. No pseudocode has ever been published for any variant. ","title":"Formal Algorithm for Transformers"},{"content":"Mout a disk using gparted to format the disk to ext4 file system: sudo apt-get install gparted -\u0026gt; sudo gparted -\u0026gt; format the disk mount the disk to mountpoint, eg: sudo mount /dev/sda2 ~/HDD Permanently mounting: cat /etc/fstab to get UUID -\u0026gt; here change the ownership of the folder ~/HDD: sudo chown xxy ~/ vim cheatsheet vscode cheetsheat 1 2 3 4 5 skip to the front of the line: `home` skip to the end of the line: `end` select to the end: `shift+end` close editor:`ctrl+w` open recent: `ctrl+R` singularity cheatsheet 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 build sif file: sudo singularity build xiaoyu.sif install.def using singularity: singularity shell --nv -B /scratch/xiaoyu -B /project/6002585/xiaoyu -B /project/6002585/xiaoyu/depth-estimation/source /project/6002585/xiaoyu/xiaoyu.sif singularity shell --nv -B /scratch/xiaoyu -B /project/def-zhouwang/xiaoyu /project/def-zhouwang/xiaoyu/xiaoyu.sif singularity shell --nv -B /scratch/xiaoyu -B /project/def-zhouwang/xiaoyu /scratch/xiaoyu/xiaoyu.sif CC交互模式: salloc --time=3:0:0 --nodes=1 --gres=gpu:4 --cpus-per-task=8 --mem-per-cpu=3000M --account=rrg-zhouwang --x11 CC singularity使用: 1.打开交互模式；2. module load singularity; 3. singularity加载sif文件。或者: 1. module load singularity; 2. sbatch挂载任务 (apt install -y python3-opencv) singularity sandbox build: 1. sudo singularity build --sandbox ./sif/ install.def 2.sudo singularity shell --writable sif/ 3.sudo singularity build xiaoyu.sif ./sif/ github cheetsheat 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 set git using *ssh* rather than *https*: git remote set-url origin git@github.com:x423xu/x423xu.github.io create branch: git checkout --orphan branch-name git push: git init git add README.md git commit -m \u0026#34;first commit\u0026#34; git branch -M main git remote add origin https://github.com/x423xu/x423xu.github.io.git git push -u origin main git set remote url: git remote set-url origin xxx !ERROR: Updates were rejected because the remote contains work that you do not have locally. This is usually caused by another repository pushing to the same ref. You may want to first integrate the remote changes (e.g., \u0026#39;git pull ...\u0026#39;) before pushing again. git push -f origin main #comment: this error is caused by the difference of local branch and remote branch. with \u0026#39;-f\u0026#39; arg, the local branch will be forced to update to the remote. configure github login with access token: gh auth login delete remote branch or local branch: git push origin --delete main #delete git branch -D main\t#local create local branch: git branch main switch to a different branch: git checkout xxx add all files to branch: git add -f . uncommit 1 committing: git reset HEAD~1 linux cheatsheet 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 remove all after: `ctrl+k` 后台运行：nohup python f.py \u0026gt;log 2\u0026gt;\u0026amp;1 \u0026amp; 解压： tar -xvf compress: tar -czvf x.tar.gz path 查看当前文件夹大小： du -sh 查看当前文件夹下每个文件的大小：ls ./ -lht 查看cuda版本：nvcc --version linux查看进程详细信息：ll /proc/PID 查看cuda路径: python -c \u0026#34;import torch;from torch.utils.cpp_extension import CUDA_HOME;print(CUDA_HOME);print(torch.cuda.is_available())\u0026#34; 统计当前目录下文件的个数（不包括目录）： ls -l | grep \u0026#34;^-\u0026#34; | wc -l 统计当前目录下文件的个数（包括子目录）： ls -lR| grep \u0026#34;^-\u0026#34; | wc -l 查看某目录下文件夹(目录)的个数（包括子目录）： ls -lR | grep \u0026#34;^d\u0026#34; | wc -l 复制ssh key: ssh-copy-id -i id_rsa.pub zduanmu@129.97.68.248 rsync: 传输命令: rsync -avz --no-p --no-g xiaoyu.sif xiaoyu@graham.computecanada.ca:/project/def-zhouwang/xiaoyu/ --progress 按规则同步: rsync -av --include=\u0026#34;*.jpg\u0026#34; --exclude=* ./ xiaoyu@graham.computecanada.ca:/scratch/xiaoyu/depth-estimation/source/RaMDE/test_imgs/ --progress ignore existing: rsync -av --include=\u0026#34;*.jpg\u0026#34; --ignore-existing --exclude=* ./ xiaoyu@graham.computecanada.ca:/scratch/xiaoyu/depth-estimation/source/RaMDE/test_imgs/ --progress open html in remote server: 1. remote server: python -m http.server 8000 2. local machine: ssh -L 8000:localhost:8000 server-name 3. open firefox: http://localhost:8000/file.html open a new tab in terminal: ctrl shift t set warning error: python -W error to the left side: `ctrl+a` to the right side `ctrl+e` delete all before `ctrl+u` delete all behind `ctrl+k` switch between tab `ctrl+page up/page down` chrome cheatsheet 1 2 3 4 5 6 open a new tab `ctrl+t` open a new window `ctrl+n` close the tab `ctrl+w` close the window `ctrl+shift+w` move among tabs `ctrl+tab` search in tab `ctrl+k/ctrl+e/ctrl+l/alt+d` tmux cheatsheet 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 make a vertical split: ctrl+b+% make a horizontal split: ctrl+b+\u0026#34; move to the left pane: ctrl+b+left arrow 1.创建session: tmux new -s xxy 2.列出现有session: tmux ls 3.删除session: tmux kill-session -t xxy 4.进入session: tmux a -t xxy 5.退出session: tmux detach //创建window：Ctrl+b +c //删除window：Ctrl+b \u0026amp; //下一个window：Ctrl+b n //上一个window：Ctrl+b p //重命名window：Ctrl+b , //在多个window里搜索关键字：Ctrl+b f //在相邻的两个window里切换：Ctrl+b l //横切split pane horizontal：Ctrl+b ” (问号的上面，shift+’) //竖切split pane vertical：Ctrl+b % （shift+5） //按顺序在pane之间移动：Ctrl+b o //上下左右选择pane：Ctrl+b 方向键上下左右 //调整pane的大小：Ctrl+b :resize-pane -U #向上 Ctrl+b :resize-pane -D #向下 Ctrl+b :resize-pane -L #向左 Ctrl+b :resize-pane -R #向右 在上下左右的调整里，最后的参数可以加数字 用以控制移动的大小，例如： Ctrl+b :resize-pane -D 50 //在同一个window里左右移动pane：Ctrl+b { （往左边，往上面）：Ctrl+b } （往右边，往下面） //删除pane：Ctrl+b x //更换pane排版：Ctrl+b “空格” //移动pane至window：Ctrl+b ! //移动pane合并至某个window：Ctrl+b :join-pane -t $window_name //显示pane编号：Ctrl+b q //按顺序移动pane位置：Ctrl+b Ctrl+o vnc cheatsheet 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 For login node: server: ssh graham vncserver -MaxConnectionTime 3600 -AlwaysShared\tgrep port /home/xiaoyu/.vnc/gra-login1:3.log vncserver -list vncserver -kill :44 local: ssh graham -L 5901:gra-login1:5903 vncviewer localhost:5901 For compute node: server: 1. export XDG_RUNTIME_DIR=${SLURM_TMPDIR} 2. vncserver 3. grep port /home/xiaoyu/.vnc/cdr348.int.cedar.computecanada.ca:1.log local: 1. ssh username@cedar.computecanada.ca -L 5902:cdr348:5901 2. vncviewer localhost:5902 Slurm cheatsheet 1 2 3 4 5 6 7 8 9 10 11 12 Filename Pattern: %% The character \u0026#34;%\u0026#34;. %A Job array\u0026#39;s master job allocation number. %a Job array ID (index) number. %J jobid.stepid of the running job. (e.g. \u0026#34;128.0\u0026#34;) %j jobid of the running job. %N short hostname. This will create a separate IO file per node. %n Node identifier relative to current job (e.g. \u0026#34;0\u0026#34; is the first node of the running job) This will create a separate IO file per node. %s stepid of the running job. %t task identifier (rank) relative to current job. This will create a separate IO file per task. %u User name. %x Job name. other collections 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 CC setup.py: 1. python -m venv ./venv --system-site-packages 2. source /project/6002585/xiaoyu/venv/bin/activate 3. cd ~rcnn 4. python setup.py build develop singularity脚本: #!/bin/bash #SBATCH --account=rrg-zhouwang #SBATCH --time=00:40:00 #SBATCH --job-name=bls2017_49 #SBATCH --gres=gpu:1 #SBATCH --cpus-per-task=8 #SBATCH --mem-per-cpu=3000M #SBATCH --output=/home/z777li/scratch/output/%x-%j.out module load singularity singularity exec --nv -B /home -B /scratch ~/Documents/tfcompression.sif \\ python ~/Documents/compression/models/bls2017.py --model_path ~/scratch/compression_models/bls2017_49 -V \\ rdcurve --npy_path ~/scratch/bls2017_stats/rdstats_49.npy --split train make a new jekyll static site: step 1: jekyll new xxx #make a new directory step 2: cd xxx step 3: bundle exec jekyll serve #make it available on a local server step 4: push to github ","permalink":"https://x423xu.github.io/posts/2022-05-26-linux-operations/","summary":"Mout a disk using gparted to format the disk to ext4 file system: sudo apt-get install gparted -\u0026gt; sudo gparted -\u0026gt; format the disk mount the disk to mountpoint, eg: sudo mount /dev/sda2 ~/HDD Permanently mounting: cat /etc/fstab to get UUID -\u0026gt; here change the ownership of the folder ~/HDD: sudo chown xxy ~/ vim cheatsheet vscode cheetsheat 1 2 3 4 5 skip to the front of the line: `home` skip to the end of the line: `end` select to the end: `shift+end` close editor:`ctrl+w` open recent: `ctrl+R` singularity cheatsheet 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 build sif file: sudo singularity build xiaoyu.","title":"linux-operations"},{"content":"Here I would introduce how to use jupyter hub in HPC\nIntroduction JupyterHub is the best way to serve Jupyter notebook for multiple users. Because jupyterhub manages a separate Jupyter encironment for each user, it can be used in a class of students, a corporate data scientific research group. It is a multi-user Hub that spawns, manages, and proxies multiple instances of the sinfle0user Jupyter notebook server.\nIt offers distributions for different use cases.\nThe little JupyterHub for numbre of users (1-100) Zero to JupyterHub with Kubernets deploy dynamic servers on the cloud for more users. Four subsystems:\na Hub is the heart aconfigurable HTTP Procy receives the requests from the client\u0026rsquo;s browser multiple Single-User Jupyter Notebook Servers monitores by Spawners. an Authentication Class manages how users can access the system. Functions:\nThe Hub launches a proxy The proxy forwards all requests to the Hub by default The Hub handles user login and spawns single-user servers on demand The Hub configures the procy to forward URL prefixes to the single-user notebook servers. By default, a notebook server runs locally at 127.0.0.1:8888 and is accessible only from localhost. You may access the notebook server from the browser using http://127.0.0.1:8888.\nHowTo 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Steps: first login: 0. cd scratch 1. module load python 2. virtualenv --no-download jupyter_py3 3. source jupyter_py3/bin/activate 4. pip install --no-index --upgrade pip 5. pip install --no-index jupyterlab 6. echo -e \u0026#39;#!/bin/bash\\nunset XDG_RUNTIME_DIR\\njupyter lab --ip $(hostname -f) --no-browser\u0026#39; \u0026gt; $VIRTUAL_ENV/bin/jupyterlab.sh 7. chmod u+x $VIRTUAL_ENV/bin/jupyterlab.sh next login: source jupyter_py3/bin/activate salloc --time=3:0:0 --nodes=1 --gres=gpu:1 --ntasks=1 --cpus-per-task=8 --mem-per-cpu=3000M --account=def-zhouwang --x11 srun $VIRTUAL_ENV/bin/jupyterlab.sh local: sshuttle --dns -Nr xiaoyu@beluga.computecanada.ca ","permalink":"https://x423xu.github.io/posts/jupyterhub/","summary":"Here I would introduce how to use jupyter hub in HPC\nIntroduction JupyterHub is the best way to serve Jupyter notebook for multiple users. Because jupyterhub manages a separate Jupyter encironment for each user, it can be used in a class of students, a corporate data scientific research group. It is a multi-user Hub that spawns, manages, and proxies multiple instances of the sinfle0user Jupyter notebook server.\nIt offers distributions for different use cases.","title":"Jupyterhub"},{"content":" 1 2 3 4 5 X Error of failed request: GLXBadContext Major opcode of failed request: 152 (GLX) Minor opcode of failed request: 6 (X_GLXIsDirect) Serial number of failed request: 39 Current serial number in output stream: 38 Reproduce: on compute canada, with singularity, installed mesa-utils, libgl1-mesa-glx. 1 2 3 4 python from vedo import Plotter v = Plotter() v.show() Solution: use --nv args in singularity shell command.\nExperience: the mesa requires nvidia graphical card to run correctly. It is necessary to have one GPU.\n1 2 libGL error: No matching fbConfigs or visuals found libGL error: failed to load driver: swrast Solution: export LIBGL_ALWAYS_INDIRECT=1, apt-get install -y mesa-utils libgl1-mesa-glx ","permalink":"https://x423xu.github.io/posts/debugs/","summary":"1 2 3 4 5 X Error of failed request: GLXBadContext Major opcode of failed request: 152 (GLX) Minor opcode of failed request: 6 (X_GLXIsDirect) Serial number of failed request: 39 Current serial number in output stream: 38 Reproduce: on compute canada, with singularity, installed mesa-utils, libgl1-mesa-glx. 1 2 3 4 python from vedo import Plotter v = Plotter() v.show() Solution: use --nv args in singularity shell command.\nExperience: the mesa requires nvidia graphical card to run correctly.","title":"Debugs"},{"content":"what is pytorch lightning PyTorch Lightning is the deep learning framework with “batteries included” for professional AI researchers and machine learning engineers who need maximal flexibility while super-charging performance at scale.\nquick start\nYour browser does not support the video tag. summary steps:\nlightning module forward func configure optimizers def training_step def validation_step remove .cuda() backward and step as hook init lightning module init trainer add other functions as call back explanation about dataloader and sampler LightningDataModule was designed as a way of decoupling data-related hooks from the LightningDataModule, so you can develop dataset agonostic models. The LightningDataModule makes it easy to hot swap different Datasets with your model, so you can test it and benchmark it across domains. It also makes sharing and resuing the exact data splits and transforms across projects possible.\nLIGHTNINGDATAMODULE Your browser does not support the video tag. A datamodule encapsulates the five steps involved in data preprocessing in Pytorch:\nDownload/tokenize/process Clean and save to disk Load inside Dataset Apply transforms Wrap inside a dataloader The class can then be shared and used anywhere\n1 2 3 4 5 6 7 8 9 10 from pl_bolts.datamodules import CIFAR10DataModule, ImagenetDataModule model = LitClassifier() trainer = Trainer() imagenet = ImagenetDataModule() trainer.fit(model, datamodule=imagenet) cifar10 = CIFAR10DataModule() trainer.fit(model, datamodule=cifar10) why do i need a DataModule In normal pytorch code, the data cleaning or preparation is usually scattered across many files. This makes sharing and reusing the exact splits and transforms across projects impossible.\nThe DataModule solves following questions:\nwhat splists did you use? what transforms did you use? what normalization did you use? how did you prepare/tokenize the data? what is a DataModule A DataModule is simply a collection of a train_dataloader, val_dataloader, test_dataloader and predict_dataloader along with the matching transforms and data precessing/downloads steps required.\na simply pytorch example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # regular PyTorch test_data = MNIST(my_path, train=False, download=True) predict_data = MNIST(my_path, train=False, download=True) train_data = MNIST(my_path, train=True, download=True) train_data, val_data = random_split(train_data, [55000, 5000]) train_loader = DataLoader(train_data, batch_size=32) val_loader = DataLoader(val_data, batch_size=32) test_loader = DataLoader(test_data, batch_size=32) predict_loader = DataLoader(predict_data, batch_size=32) # Datamodule class MNISTDataModule(pl.LightningDataModule): def __init__(self, data_dir: str = \u0026#34;path/to/dir\u0026#34;, batch_size: int = 32): super().__init__() self.data_dir = data_dir self.batch_size = batch_size def setup(self, stage: Optional[str] = None): self.mnist_test = MNIST(self.data_dir, train=False) self.mnist_predict = MNIST(self.data_dir, train=False) mnist_full = MNIST(self.data_dir, train=True) self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000]) def train_dataloader(self): return DataLoader(self.mnist_train, batch_size=self.batch_size) def val_dataloader(self): return DataLoader(self.mnist_val, batch_size=self.batch_size) def test_dataloader(self): return DataLoader(self.mnist_test, batch_size=self.batch_size) def predict_dataloader(self): return DataLoader(self.mnist_predict, batch_size=self.batch_size) def teardown(self, stage: Optional[str] = None): # Used to clean-up when the run is finished ... As the complexity of the preprocessing grows (transforms, multiple-GPU training), you can let lightning handle those details for you while making this dataset reusable so you can share with collegues or use in different projects. 1 2 3 4 5 mnist = MNISTDataModule(my_path) model = LitClassifier() trainer = Trainer() trainer.fit(model, mnist) a more realistic DataModule with reusability 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 import pytorch_lightning as pl from torch.utils.data import random_split, DataLoader # Note - you must have torchvision installed for this example from torchvision.datasets import MNIST from torchvision import transforms class MNISTDataModule(pl.LightningDataModule): def __init__(self, data_dir: str = \u0026#34;./\u0026#34;): super().__init__() self.data_dir = data_dir self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]) def prepare_data(self): # download MNIST(self.data_dir, train=True, download=True) MNIST(self.data_dir, train=False, download=True) def setup(self, stage: Optional[str] = None): # Assign train/val datasets for use in dataloaders if stage == \u0026#34;fit\u0026#34; or stage is None: mnist_full = MNIST(self.data_dir, train=True, transform=self.transform) self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000]) # Assign test dataset for use in dataloader(s) if stage == \u0026#34;test\u0026#34; or stage is None: self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform) if stage == \u0026#34;predict\u0026#34; or stage is None: self.mnist_predict = MNIST(self.data_dir, train=False, transform=self.transform) def train_dataloader(self): return DataLoader(self.mnist_train, batch_size=32) def val_dataloader(self): return DataLoader(self.mnist_val, batch_size=32) def test_dataloader(self): return DataLoader(self.mnist_test, batch_size=32) def predict_dataloader(self): return DataLoader(self.mnist_predict, batch_size=32) prepare_data explanation Downloading and saving data with multiple processes will result in corrupt data. Lightning ensures the prepare_data() is called only within a single process on CPU, so you can safely add your downloading lgic within. In case of multi-node training, the execution of this hook depends upon prepare_data_per_node setup() is called after prepare_data and there is a barrier in between which ensures that all the processes proceed to setup once the data is prepared and available for use. It will only be executed once.\nsetup explanation There are also data operations you might want to perform on every GPU. Use setup() to do things like:\ncount numbers of classes build vocabulary perform train/val/test splits create datasets apply transforms etc\u0026hellip; ","permalink":"https://x423xu.github.io/posts/pytorch-lightning/","summary":"what is pytorch lightning PyTorch Lightning is the deep learning framework with “batteries included” for professional AI researchers and machine learning engineers who need maximal flexibility while super-charging performance at scale.\nquick start\nYour browser does not support the video tag. summary steps:\nlightning module forward func configure optimizers def training_step def validation_step remove .cuda() backward and step as hook init lightning module init trainer add other functions as call back explanation about dataloader and sampler LightningDataModule was designed as a way of decoupling data-related hooks from the LightningDataModule, so you can develop dataset agonostic models.","title":"Pytorch Lightning"},{"content":"Preface To understand the basics of the graph neural network, it can\u0026rsquo;t circumvent the topic of \u0026ldquo;Spectral-GNN\u0026rdquo;. Today, I\u0026rsquo;d like to deeply explore what the spectral GNN is and how does it works.\nTable of contents Theory part\nBasic theory 1 \u0026lt;The Emerging Field of Signal Processing on Graphs\u0026gt; challenges a \u0026ldquo;classical\u0026rdquo; signal $f(t)$ has a concept of \u0026ldquo;translate to the right by 3\u0026rdquo; to get $f(t-3)$. But for graph signal, it is not clear to say \u0026ldquo;translate by 3\u0026rdquo;. Because it doesn\u0026rsquo;t have a clear order, thus is not \u0026ldquo;shift-invariant\u0026rdquo;. Multiplying a signal with a complex exponential signal corresponds to translation in Fourier domain. But the signal on graph is irregulaly spaced, hard to define translation in spectral domain. Hard to down sample a signal on graph vertices. Need a method to create a coarser graph which somehow captures the structural properties from the original one. Need a method, localized operations, to compute information from a small neighboring vertices. Problem settings A weighted graph $G$ is always expressed as $G = {V, E, W}$, which contains vertices $V$ with $|V| = N$, a set of edges $E$ and adjacency matrix $W$. If the edge $e(i,j)$ which connect two vertices $i, j$ exists, the entry $w_{i,j}$ denotes the weight of the edge; otherwise $w_{i,j} = 0$. Sometimes the graph $G$ can be separated into several subgraphs, these subgraphs are insides connected while not connected mutually. So the graph would be represented as $M$ pieces connected components. A signal or function defined on the vertices are denoted as ${f:V\\rightarrow \\real, f\\in \\real^N}$. In a graph neural network, the $f$ generally represented embedded features with $f\\in\\real^{S\\times N}$, where $S$ denotes the shape of features.\nThe Graph Laplacian Here the graph laplacian refers to the unormalized one. Generally in a graph, the laplacian is defined as $L = D-W$. $D$ is a degree matrix, which is diagonal with $i$-th diagonal entry $D_i = \\sum_j w_{i,j}$. Given the defined signal $f$ on the graph, the laplacian operation of the $f$ is formulated as: $$(Lf)(i) = \\sum_{i\\in N_i}w_{i,j}[f(i)-f(j)]$$ The laplacian operator in actually a difference operator, which describe the average difference level between the vertice $i$ and its neighboring vertices $N_i$.\nThere are some definitions and properties for laplacian matrix to be explained in advance for better comprehension\nThe laplacian matrix is a real symmetric matrix, it has a complete set of orthonormal eigenvectors, denoted as ${u_l}$, and corresponding real nonpositive eigenvalues ${\\lambda_l}_{l= 0,1,\\cdots,N-1}$ The number of zero eigenvalues indicates the number of connected componnets, namely subgraphs. The eigenvalues of laplacian matrix is sorted as $0=\\lambda_0\u0026lt;\\lambda_1\\leq\\lambda_2\\cdots\\leq\\lambda_{N-1}=\\lambda_{max}$. The spectrum of the laplacian matrix is defined as ${\\lambda_0, \\lambda_1,\\cdots,\\lambda_{N-1}}$ A graph transform Recall the classical Fourier transform on one-dimensional continuous signal $$\\hat{f}(\\xi) = \u0026lt;f, e^{2\\pi j\\xi t}\u0026gt;=\\int_{\\real}f(t)e^{-2\\pi j\\xi t}dt$$ The Fourier transform can be understood as the expansion of the signal $f(t)$ with the complex exponentials $e^{2\\pi j\\xi t}$. What is interesting is the complex exponential $e^{2\\pi j \\xi t}$ is an eigenfunction of laplace operator $$-\\Delta(e^{2\\pi j \\xi t}) = -\\frac{\\partial^2}{\\partial t^2}e^{2\\pi j \\xi t} =K\\cdot e^{2\\pi j \\xi t}$$ One step further, the $e^{2\\pi j \\xi t}$ is the eigenfunction for any order derivation operators. The Fourier transform can be understood as to project a signal from temporal domain to spectral domain. According to the same way, the graph transform is defined as $$\\hat{f}(\\lambda_l) = \u0026lt;f, u_l\u0026gt;=\\sum_i f(i)u^*_i$$ The connections between the graph transform and Fourier transform can be explained in this way: for Fourier transform, it has a space which is constructed on bases $e^{2\\pi j \\xi t}$. Each signal $f(t)$ can be represented by these bases, the coefficient of which is exactly the $\\hat{f}(\\xi)$. The bases can be seen as the eigenvectors for Fourier transform, and the eigenvalue is ${f}(\\xi)$ (this is just a kind of analogy, since the Fourier trasform is operated on continuous signal). For graph transform, each graph signal $f$ is transformed into the spectral domain, which has bases ${u_1, u_2,\\cdots u_i }$. So the eigenvectors $u_l$ corresponding to low frequency $\\lambda_l$ vary slowly across the graph and vice versa.\nSpectral graph convolution Given the node features $X$ and convolutional kernel $H$, the process of the spectral-based GNN is formulated in the following steps:\nEigen-decomposition of Laplacian matrix $L$: $L = U \\Lambda U^{\\top}$. Each column of the $U$ is the eigenvector of the $L$. $\\Lambda$ is a diagonal matrix composed of eigenvalues. Now the Laplacian matrix of a graph $G$ is decomposed into a set of orthogonal bases. The input node features $X$ can be projected to the space formed with the bases.\nProject the $G$ and $X$ to the spectral domain using $U$: $$ \\begin{split} \\hat{H} \u0026amp;= U^{\\top}\\cdot H \\ \\hat{X} \u0026amp;= U^{\\top}\\cdot X \\end{split} $$\nSpectral multiplication and reprojection: $$ \\begin{split} X^{\\prime} \u0026amp;= U(\\hat{H}\\cdot \\hat{X}) \\ \u0026amp;= U(U^{\\top}\\cdot H \\cdot U^{\\top}\\cdot X) \\end{split} $$\nThe $X^{\\prime}$ denotes the output feature of one GNN layer. To get deep GNN, multiple layers need to be stacked and rerun the above steps. For the final output node features, there are two kinds of tasks that can be applied. One is the node-level task, such as social media accounts classification, in which each social media account is represented as one node. The second is the graph-level task, such as text classification, in which each word in the text is seen as one node and the whole graph can be classified as spam or not.\nMethods Demos to evaluate the properties ","permalink":"https://x423xu.github.io/posts/2022-10-27-spectral-gnn/","summary":"Preface To understand the basics of the graph neural network, it can\u0026rsquo;t circumvent the topic of \u0026ldquo;Spectral-GNN\u0026rdquo;. Today, I\u0026rsquo;d like to deeply explore what the spectral GNN is and how does it works.\nTable of contents Theory part\nBasic theory 1 \u0026lt;The Emerging Field of Signal Processing on Graphs\u0026gt; challenges a \u0026ldquo;classical\u0026rdquo; signal $f(t)$ has a concept of \u0026ldquo;translate to the right by 3\u0026rdquo; to get $f(t-3)$. But for graph signal, it is not clear to say \u0026ldquo;translate by 3\u0026rdquo;.","title":"Spectral Gnn"},{"content":"Pedestrain trajectory Questions: 1. only pedestrain relation considered? hoe about environment 2. what is the general framework for pedestrain trajectory prediction.\nSGCN: Sparse Graph Convolution Network for Pedestrain Trajectory Prediction (CVPR2021) SGCN framework superfulous interactions: dense interaction -\u0026gt; one pedestrain is related to all other pedestrains while in fact it is not sparse undirected -\u0026gt; equal interactions for a pair of pedestrains spatial GCN -\u0026gt; sparse directed -\u0026gt; not all pedestrains + not equal interaction temporal GCN -\u0026gt; motion tendency Disentangled Multi-Relational Graph Convolutional Network for Pedestrian Trajectory Prediction (AAAI2021) Use CNN to generalize complex interpersonal relations a graph representation: node as pedestrian, edges correspond to distance challeges only simple social relationship like collision avoidance is aggregated modelling social norms is not suitable for determining the end-points of pedestrians in the last frame (over-avoidance) contributions disentangled multi-scale aggregation to clearly distinguish between relevant pedestrians multi-relational GCN to extract sophisticated social interaction in a scene. a global temporal aggregation to compensate accumulated errors from over-avoidance. Gaze target estimation Dual Attention Guided Gaze Target Detection in the Wild challenges: prior works: gaze direction in 2D representations + barely depth-channel gaze. forward, backward, sideward. (refer to one person) salient objects from 2d cues. two candidates different depth along same gaze direction, hard to tell apart. (refer to one person with the scene understanding) fixation inconsistency: facing forward looking downward. solutions: estimate 3D gaze direction from head image. dual attention module -\u0026gt; depth-aware perspective in the scene ?hasn\u0026rsquo;t found effective solution in the paper? -\u0026gt; solved in the first stage? so is it still an independent challenge? or should be merged to the first challenge? ESCNet: Gaze Target Detection with the Understanding of 3D Scenes hard to handle: multiple salient objects lie in the same depth layer and field of view. occlusion plays an important role in gaze estimation , one cannot see through occluders. 3D geometry could br reconstructed by absolute depth and camera parameters FOV-based initial heatmap.\nMultimodal Across Domains Gaze Target Detection gaze target detection also referred as gaze-following false detections occur when there are multiple object-of-interests at different depths but along with the subject\u0026rsquo;s gaze direction \u0026lsquo;dual\u0026rsquo; potentially being error-prone in real-life processing, e.g., when the eyes are not visible or not detectable \u0026lsquo;ESCNET\u0026rsquo; auxiliary estimating depth, depend on depth and pseudo labels. not require gaze angles as supervision only spatial processing, but still detect gaze target use depth image how different modalities should be jointly learned for performing effective gaze target detection\nsolve domain shift problem Beyon 3D Siamese Tracking: A motion-Centric Paradigm for 3D single Object Tracking in Point Clouds point clouds are usually textureless and incomplete, which hinders effective appearance matching. Overlook motion clues among targets. propose a motion-centric paradigm $M^2-Track$ localizes the target within successive frames via motion transformation. refines the target box through motion-assisted shape completion Motivation the upper one obtain a canonical target template using the previous target box and search for the target in thecurrent frame according to the matching similarity. It is sensitive to distractors The bottom one learns relative target motion from two consecutive frames. Robustly localize the target. framework input: last point cloud, last 3D BBox, current point cloud\noutput current 3D BBox.\n? how many frame used and how many frame predicted.\n1 frame used, 1 frame predicted. quick comment: this task is not suitable for our goal. But it gives a good benchmark for kitti dataset.\nMotion CNN plan a safe and efficient route. a baseline form multimodal motion prediction. explanation framework input: image output trajectories. use 1 second of images to predict 8 seconds\u0026rsquo;. 3D Multi-Object Tracking in Point Clouds Based on Prediction confidence-Guided Data Association 3D multi-object tracker to more robustly track objects that are temporarily missed by detectors. a predictor employs constant acceleration motion model to estimate future positions and prediction confidence. a new pairwise cost. Multimodal Trajectory Prediction Conditioned on Lane-Graph Traversals Predicting the future motion of surrounding vehicles requires reasoning about the inherent uncertainty in driving behavior. Early works encodes HD maps using a rasterized bird\u0026rsquo;s eye view image. Recent wotks represent lane polylines as nodes of a graph. Represent HD map as a graph and encode the input context into a single context vector. challenges: The prediction header go off the road, violate traffic rules because of the complex mapping. lateral or route variability (e.g. will the driver change lane, will they turn right etc.). longitudinal variability (will the driver accelerate, brake, maintain spped) insights graph structure of the scene explicitly model the lateral or route variability in trajectories predictions conditioned on traversals: selectively aggregate part of scene context by sampling path traversals from a learned policy it lessens representational demands on the output decoder. the probabilistic policy leads to adiverse set of sampled paths. latent variable for longitudinal variability: condition the predictions with a sampled latent variable. enable to predict distinct trajectories even for identical path traversals. method predict the future trajectories of vehicles of interest, conditioned on their past trajectories the past trajectories of nearby vehicles and pedestrians the HD map of the scene represent the scene, predict in the bird\u0026rsquo;s eye view and use an agent-centric frame of reference aligned along the agent\u0026rsquo;s instantaneous direction of motion. Trajectory representation $$s_{-t_h:0}^i=[s_{-t_h}^i,\\cdots,s_{-1}^i,s_{0}^i]$$ $$s_t^i = [x_t^i, y_t^i, v_t^i, a_t^i, w_t^i,I_t^i]$$\nHD maps as lane graphs Nodes: lane centerlines $f_{1:N}^v = [f_{1}^v, cdots,f_{N}^v ]$, $f_n^v = [x_n^v, y_n^v, \\theta_n^v, I_n^v]$ Edges: successor edges-\u0026gt;legal route; proximal edges-\u0026gt; lane changes Output representation K trajectories.\nModel graph encoder: forms the backbone of our model, outputs representations for each node of the lane graph, incorporate HD map and surrounding agent context. policy header: output a discrete probability distribution over outgoing edeges at each node, allowing us to sample paths in the graph. trajectory decoder: output trajectories conditioned on paths traversed by the policy and a sampled latent variable. Encoding scene and agent context GRU encoders: target vehicle trajectory $s_{-t_h:0}^0$ -\u0026gt; $h_{motion}$; surrounding vehicle trajectories $s_{-t_h:0}^i$ -\u0026gt; $h_{agent}^i$; node features $f_{1:N}^v$-\u0026gt;$h_{node}^v$. agent node attention: update node encodings with nearby agent encodings. keys and values from $h_{agent}^i$, query by $h_{node}^v$ output of agent-node-attention as the attention mask for GAT. Only attended nodes are employed for updating An explanation of Q, K, V\nThe Seq2Seq task always has encoder+decoder. The encoders job is to take in an input sequence and output a context vector/thought vector. The context vector is then input to decoder.\nHowever, the performance drops drastically for longer sentences.\nOne solution is to use skip-connection, by simple concatenation or summing up. -\u0026gt; assume all hidden states are equally important.\nSo attention (dynamic weighting) is brought in.\nAn attention mechanism calculates the dynamic weights representing the relative importance of the inputs in the sequence (keys) for the particular output (query). Multiplying the dynamic weights with the input sequence (values) will then weight the sequence.\nThe exact values for Q,K, and V depend on exactly which attention mechnism is being referred to. For the trasformer, 3 types: 1. Encoder Attention, 2, Decoder Attention, 3. Encoder-decoder Attention. (Here for reference)\nDiscrete policy for graph traversal A policy $\\pi_{route}$ for graph traversal -\u0026gt; sampled roll-outs of the policy correspond to likely routes the target might take in the future. policy: a discrete probability distribution over outgoing edges at each node. additionally add an edge to an end state, terminate at a goal location. $$score(u,v)=MLP(concat(h_{motion}, h_{node}^u, h_{node}^v, 1_{(u,v)\\in E}))$$ $$\\pi_{route}(v|u) = softmax({score(u,v)|(u,v)\\in E})$$ $$L_{BC} = \\sum_{(u,v)\\in E_{g,t}}-\\log(\\pi_{route}(v|u))$$ ","permalink":"https://x423xu.github.io/posts/2022-08-17-pedestrain-trajectory/","summary":"Pedestrain trajectory Questions: 1. only pedestrain relation considered? hoe about environment 2. what is the general framework for pedestrain trajectory prediction.\nSGCN: Sparse Graph Convolution Network for Pedestrain Trajectory Prediction (CVPR2021) SGCN framework superfulous interactions: dense interaction -\u0026gt; one pedestrain is related to all other pedestrains while in fact it is not sparse undirected -\u0026gt; equal interactions for a pair of pedestrains spatial GCN -\u0026gt; sparse directed -\u0026gt; not all pedestrains + not equal interaction temporal GCN -\u0026gt; motion tendency Disentangled Multi-Relational Graph Convolutional Network for Pedestrian Trajectory Prediction (AAAI2021) Use CNN to generalize complex interpersonal relations a graph representation: node as pedestrian, edges correspond to distance challeges only simple social relationship like collision avoidance is aggregated modelling social norms is not suitable for determining the end-points of pedestrians in the last frame (over-avoidance) contributions disentangled multi-scale aggregation to clearly distinguish between relevant pedestrians multi-relational GCN to extract sophisticated social interaction in a scene.","title":"pedestrain-trajectory"},{"content":" Voxel Set Transformer: A Set-to-Set Approach to 3D Object Detection From Point Clouds 3DJCG: A Unified Framework for Joint Dense Captioning and Visual Grounding on 3D Point Clouds Multi-Instance Point Cloud Registration by Efficient Correspondence Clustering Contrastive Boundary Learning for Point Cloud Segmentation Lepard: Learning Partial Point Cloud Matching in Rigid and Deformable Scenes CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding Density-Preserving Deep Point Cloud Compression Robust Structured Declarative Classifiers for 3D Point Clouds: Defending Adversarial Attacks With Implicit Gradients Neural Points: Point Cloud Representation With Neural Fields for Arbitrary Upsampling Not All Points Are Equal: Learning Highly Efficient Point-Based Detectors for 3D LiDAR Point Clouds Equivariant Point Cloud Analysis via Learning Orientations for Message Passing Point Cloud Pre-Training With Natural 3D Structures A Unified Query-Based Paradigm for Point Cloud Understanding REGTR: End-to-End Point Cloud Correspondences With Transformers 3DeformRS: Certifying Spatial Deformations on Point Clouds IDEA-Net: Dynamic 3D Point Cloud Interpolation via Deep Embedding Alignment AziNorm: Exploiting the Radial Symmetry of Point Cloud for Azimuth-Normalized 3D Perception Surface Reconstruction From Point Clouds by Learning Predictive Context Priors Point2Cyl: Reverse Engineering 3D Objects From Point Clouds to Extrusion Cylinders RigidFlow: Self-Supervised Scene Flow Learning on Point Clouds by Local Rigidity Prior Deterministic Point Cloud Registration via Novel Transformation Decomposition Surface Representation for Point Clouds 3D-VField: Adversarial Augmentation of Point Clouds for Domain Generalization in 3D Object Detection An MIL-Derived Transformer for Weakly Supervised Point Cloud Segmentation Why Discard if You Can Recycle?: A Recycling Max Pooling Module for 3D Point Cloud Analysis ART-Point: Improving Rotation Robustness of Point Cloud Classifiers via Adversarial Rotation Pyramid Architecture for Multi-Scale Processing in Point Cloud Segmentation Finding Good Configurations of Planar Primitives in Unorganized Point Clouds No-Reference Point Cloud Quality Assessment via Domain Adaptation Learning Local Displacements for Point Cloud Completion Point-BERT: Pre-Training 3D Point Cloud Transformers With Masked Point Modeling Point Cloud Color Constancy Multimodal Colored Point Cloud to Image Alignment Self-Supervised Arbitrary-Scale Point Clouds Upsampling via Implicit Neural Representation Shape-Invariant 3D Adversarial Point Clouds Deformation and Correspondence Aware Unsupervised Synthetic-to-Real Scene Flow Estimation for Point Clouds LiDARCap: Long-Range Marker-Less 3D Human Motion Capture With LiDAR Point Clouds Domain Adaptation on Point Clouds via Geometry-Aware Implicits SC2-PCR: A Second Order Spatial Compatibility for Efficient and Robust Point Cloud Registration WarpingGAN: Warping Multiple Uniform Priors for Adversarial 3D Point Cloud Generation Weakly Supervised Segmentation on Outdoor 4D Point Clouds With Temporal Matching and Spatial Graph Propagation LAKe-Net: Topology-Aware Point Cloud Completion by Localizing Aligned Keypoints No Pain, Big Gain: Classify Dynamic Point Cloud Sequences With Static Models by Fitting Feature-Level Space-Time Surfaces PTTR: Relational 3D Point Cloud Object Tracking With Transformer ZZ-Net: A Universal Rotation Equivariant Architecture for 2D Point Clouds SS3D: Sparsely-Supervised 3D Object Detection From Point Cloud 3DAC: Learning Attribute Compression for Point Clouds Learning a Structured Latent Space for Unsupervised Point Cloud Completion RCP: Recurrent Closest Point for Point Cloud Upright-Net: Learning Upright Orientation for 3D Point Cloud Reconstructing Surfaces for Sparse Point Clouds With On-Surface Priors HybridCR: Weakly-Supervised 3D Point Cloud Semantic Segmentation via Hybrid Contrastive Regularization Self-Supervised Global-Local Structure Modeling for Point Cloud Domain Adaptation With Reliable Voted Pseudo Labels PointCLIP: Point Cloud Understanding by CLIP Geometric Transformer for Fast and Robust Point Cloud Registration Beyond 3D Siamese Tracking: A Motion-Centric Paradigm for 3D Single Object Tracking in Point Clouds Boosting 3D Object Detection by Simulating Multimodality on Point Clouds SemAffiNet: Semantic-Affine Transformation for Point Cloud Segmentation DiGS: Divergence Guided Shape Implicit Neural Representation for Unoriented Point Clouds SoftGroup for 3D Instance Segmentation on Point Clouds Bridged Transformer for Vision and Point Cloud 3D Object Detection Stratified Transformer for 3D Point Cloud Segmentation ","permalink":"https://x423xu.github.io/posts/2022-07-24-point-cloud-cvpr2022/","summary":"Voxel Set Transformer: A Set-to-Set Approach to 3D Object Detection From Point Clouds 3DJCG: A Unified Framework for Joint Dense Captioning and Visual Grounding on 3D Point Clouds Multi-Instance Point Cloud Registration by Efficient Correspondence Clustering Contrastive Boundary Learning for Point Cloud Segmentation Lepard: Learning Partial Point Cloud Matching in Rigid and Deformable Scenes CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding Density-Preserving Deep Point Cloud Compression Robust Structured Declarative Classifiers for 3D Point Clouds: Defending Adversarial Attacks With Implicit Gradients Neural Points: Point Cloud Representation With Neural Fields for Arbitrary Upsampling Not All Points Are Equal: Learning Highly Efficient Point-Based Detectors for 3D LiDAR Point Clouds Equivariant Point Cloud Analysis via Learning Orientations for Message Passing Point Cloud Pre-Training With Natural 3D Structures A Unified Query-Based Paradigm for Point Cloud Understanding REGTR: End-to-End Point Cloud Correspondences With Transformers 3DeformRS: Certifying Spatial Deformations on Point Clouds IDEA-Net: Dynamic 3D Point Cloud Interpolation via Deep Embedding Alignment AziNorm: Exploiting the Radial Symmetry of Point Cloud for Azimuth-Normalized 3D Perception Surface Reconstruction From Point Clouds by Learning Predictive Context Priors Point2Cyl: Reverse Engineering 3D Objects From Point Clouds to Extrusion Cylinders RigidFlow: Self-Supervised Scene Flow Learning on Point Clouds by Local Rigidity Prior Deterministic Point Cloud Registration via Novel Transformation Decomposition Surface Representation for Point Clouds 3D-VField: Adversarial Augmentation of Point Clouds for Domain Generalization in 3D Object Detection An MIL-Derived Transformer for Weakly Supervised Point Cloud Segmentation Why Discard if You Can Recycle?","title":"point-cloud-CVPR2022"},{"content":" 1 Jure Leskovec, Stanford CS224W: Machine Learning with Graphs, http://cs224w.stanford.edu Why graphs relations of entities Similar data points arbitrary sizes, no spatial index, no reference order representation learning Map nodes to d-dimensional embeddings-\u0026gt; similar nodes in the network are embedded close together Applications of Graph ML different tasks: Node classification Link prediction: knowledge graph completion Graph classification: molecule property prediction Clustering Graph generation Graph evolution: physical simulation Examples: node-level: Protein folding Recommender system: recommend related pins to users by edge level classification subgraph-level: traffic prediction: nodes: road segments, edges: connectivity between nodes-\u0026gt; predict time arrival etc graph-level: drug discovery: nodes: atoms, edges: chemical bonds. graph generation, generate new molecules in a targeted way. physicalsimulation: nodes: particles, edges interactions between particles-\u0026gt; predict how a graph will evolve over. Choice of Graph representation Node degree (ubdirected): the number of edges adjacent to node i in-degree, out-degree(directed) Bipartite graph: two disjoint sets U and V. e. author to papers. two different type of nodes Adjacency matrix. For a directed graph, adjacency matrix is not symmetric Adjacency sparse for most real-word networks. represent as a list of edges. represent as an adjacency list node and edge attributes: weight, ranking, type, sign, etc. weighted and unweighted adjacency matrix. self-loop node, multigraph-\u0026gt;adjacency matrix different. connected graph: any two nodes canbe joined by a graph. A graph with mutiple components: adjacency matrix can be written in a block way. Strongly connected directed graph: has a directed path to every other node. weakly connected directed: is connected if disregard the edge direction strongly connected directed components: in the component, it is strongly connected. Traditional methods: Node hand-designed features Node-level node degree node centrality: A node is important if surrounded by important neighboring nodes $u\\in N(v)$ $$c_v = \\frac{1}{\\lambda}\\sum_{u\\in N(v)}c_u \\rightarrow \\lambda \\mathbb{c}=\\mathbb{Ac}$$ Where \u0026amp;A\u0026amp; is adjacency matrix. Clossness centrality: clustering coefficient: count triangles that a node touches graphlets: graph degree vector: counts the occurence of the graphlets Link-level the key is design of the feature of pair nodes. Links missing prediction links overtime prediction compute score c(x,y)-\u0026gt; sort score-\u0026gt; keep top k representation: shortest path between two nodes. common neighbors between two nodes jaccard\u0026rsquo;s coefficient Adamic-adar index Global neighborhood overlap: computing number of paths between two nodes: can be obtained by computing power of adjacency matrix power: $P^{(K)} = A^K$-\u0026gt; means # paths of length K between $u$ and $v$. katz index:$S_{v_1 v_2} = \\sum_{l=1}^{\\inf}\\beta^lA_{v_1 v_2}^l$ katz index matrix: $S=\\sum_{i=1}^{\\inf}\\beta^i A^i= (I-\\beta A)^{-1}-I$ graph-level features to characterize the structure of an entire graph Kernel methods: $K(a,b) = \\phi(a)^T\\phi(b)$. kernel matrix K has to be semi-definite and can be orthogonally decomposed. goal: design graph feature vector $\\phi(G)$ Key idea: bag of words for a graph. bag of degrees nodes graphlet kernel: # different graphlets in a graph. graphlets list(g1,g2,g3,\u0026hellip;) (not need to be connected different from nodel-level graphlets which has to be connected) -limitations: counting graphlets is expensive weisfeiler-Lehman kernel goal: design an efficient graph feature desciptor idea: use neighborhood structure to iteratively enrich node vocabulary.-\u0026gt; color refinement $c^{k+1}(v) = HASH({c^{k}(v), {c^k(u)}}_{u\\in N(v)})$. HASH maps input to colors. K steps summarize the structure of k-hop neighborhood Node embedding eg. 2d embedding of nodes of the Zachary\u0026rsquo;s Karate Club network from \u0026ldquo;Deepwalk:Online learning of social representations\u0026rdquo;\nkey idea: define a node similarity function $P(v|z_u)$ the probability of visiting node v on random walk starting from the node u. Softmax function/Sigmoid Random walk; $z_u^Tz_v$ probability u and v cooccur at a random walk unsupervised feature learning: maximize log-likelihood: $\\max_f\\sum_{u\\in V}logP(N(u)|Z_u)$ Given a node U, we want to learn feature representations that are predictive of the nodes in its random walk neighborhood $N_R(u)$, wehere R is ramdom walk strategy Intuition: optimize embeddings to maximize the likelihood of random walk co-occurrences Negative sampling: $P(v|z_u) = softmax(z_u^Tz_v)\\rightarrow log(\\frac{exp(z_u^Tz_v)}{\\sum_{n\\in V}exp(z_u^Tz_n)})\\rightarrow log(\\sigma(z_u^Tz_v))-\\sum_{i=1}^k\\log(\\sigma(z_u^Tz_{n_i})), n_i~P_V, \\sigma Sigmoid$ NCE approximation How to select strategy Node2vec: local and global: BFS and DFS biased fixed-length random walk: 2-nd order random walks: return/ BFS/DFS two parameters: p and q parallelizable Embed entire graph simple idea: just sum up the node embeddings in the graph-\u0026gt; classification Virtual node to represent the (sub)graph Anonymous walk embeddings represent the graph as a probability distribution over these walks Learn walk embeddings PageRank, random walk and embeddings $r = Mr$ for directed graph. eigen vector centrality: $\\lambda c = Ac$ for undirected graph. Random walk with restarts and personalized pagerank Personalized pagerank: ranks proximity of nodes to the teleport nodes S. idea every node has importance; importance gets evenly split among all edges and pushed to the neighbors. Matrix factorization above all random walk based methods\nGraph neural network Message passing and node classification Correlation: nearby nodes same class Homophily: the tendency of individuals to associate and bond with similar others. Influence: social connections can influence the individual charactersictis of a person Relational classification and Iterative classification Relational: class probability of node v is a weighted average of class probability of its neighbors. Iterative: use attributes and labels of neighbor nodes. Collective classification A dynamic programming approach answering probability queries in a graph iterative way: neighbor nodes talk to each other. I believe you belong to class 1 with likelihood. Graph neural networks Encoder: $z(v) = ENC(v)$, decoder: similarity function. can also embed subgraphs, graphs modern depplearning tool box is designed for simple sequences and grids. networks: arbitrary size, complex topological structure, no fixed ordering or reference point, dynamic Deep learning for graphs A naive appproach join adjacency matrix and features Challenges: no ordering nodes, how to use convolutiuon Solution: $\\sum W_i h_i$. Node\u0026rsquo;s neighborhood defines a computation graph. step 1, define computation graph step 2, propagat einformation. Aggregate neighbors: nodes aggregate information from their neighbors using neural network. Every node define a computation graph based on its neighborhood. Model can be of arbitrary depth. Only do for limited steps. layer-k embeddings get k-hop information. Permutation invariant How to train the model: feed embedding into any loss function and run SGD to train the parameters. Unsupervised settingL use the graph structure as the supervision. $L = \\sum_{z_u,z_v}CE(y_{u,v},DEC(z_u, z_v))$ A general perspective on GNN GNN layer = Message + Aggregation A single layer of GNN GraphSAGE: $h_v^{(l)} = \\sigma(w^{(l)}\\cdot CONCAT(h_v^{(l-1)}, AGG({h_u^{(l-1)},\\forall u \\in N(v)})))$ Graph attention networks: $h_v^{(l)} = \\sigma(\\sum_{u\\in N(v)}\\alpha_{vu}W^{(l)}h_u^{(l-1)})$, where $a_{vu}$ is attention weight. in graphSAGE, it is $\\frac{1}{|N(v)|}$ multi-head attention: $a_{vu}^1, a_{vu}^2,a_{vu}^3$ Stacking layers of a GNN GNN suffers from over-smoothing problem: all the node embeddings coverge to the same value. Why it happens: the receptive of the GNN is too big, so all the node receive the same information, so the final output tend to be the same. Explanation: two nodes have highly overlapped receptive fields, their embeddings are highly similar. Overcome: be cautious when adding GNN layers. Increase the expressive power within each GNN layer: linear aggregation to non-linear/ deep neural network. Add layers do not pass messages. Add skip connection in GNNs Graph augmentation Idea: Raw input graph $\\neq$ computational graph graph feature augmentation: input graph lacks features-\u0026gt; feature computation graph structure augmentation: too sparse, too dense, too large-\u0026gt;add virtual nodes/edges it is unlikely the input graph to be the optimal computational graph.-\u0026gt;sample subgraphs Feature augmentation: input graph doesn\u0026rsquo;t have node fatures: assign constant values to nodes assign IDs to nodes and onr-hot encoding node cannot differentiate the circle and infinitely long series Use cycle count as node features:[0,0,0,1,0,0] and [0,0,0,0,1,0] Structure augmentation Add virtual edges: connect 2-hop neighbors via virtual edges intuition: adjacency matrix: $A+A^2$ Add virtual nodes: in a sparse graph the fdistance too large adding virtual nodes, distance will be two Node neighborhood sampling sample a node\u0026rsquo;s neighborhood for messgae passing Train a GNN prediction head: node-level tasks edge-level tasks graph-level tasks options for edge: concatenation of $h_u, h_v$ + Linear prediction dot product: $\\hat{y}_{uv} = h_u\\cdot h_v$ graph-level prediction make prediction using all node embeddings options global mean pooling global max pooling global sum pooling issues lose information hierarchical pooling: aggregate partially and then aggregate aggregations supervised vs unsupervised e.g. link prediction Unsupervised -\u0026gt; self-supervised Setting up graph training each data point is a node, test data influence prediction on training data\noptions:\ntransductive settings: the graph structure can be observed over all stages inductive setting: different graphs for different stages link prediction: self-supervised\ntwo kinds of edges: message passing and predictive edges transductive or inductive setting tools: DeepSNAP, GraphGym\nHow expressive are graph neural networks Key idea: generate node embedding based on local network neighborhoods Intuition: Nodes aggregate information from neighbors using NN how powerful are GNNs proposed: GCN, GAT, graphsage, design space GCN: Meanpooling GraphSAGE: max pooling How well can a GNN distinguish different graph structures can GNN node embeddings distinguish different node\u0026rsquo;s local neighborhood structures? if so, when? if not when will a GNN fail? How a GNN captures local neighborhood structure?-\u0026gt; computational graph. computational structure = rooted subtree structure Injective function: function: injective if it maps different elements into different outputs. it retains all information of the input Most expressuve GNN should map subtrees to the node embeddings injectively. If each step of GNN\u0026rsquo;s aggregation can fully retain the neighboring information the generated node embeddings can distinguish different rooted subtrees In another words, most expressive GNN would use an injective neighbor aggregation function at each step. Design the most expressive GNN key idea: expressive power of GNNs can be characterized by that of neighbor aggregation functions they use. ObeservationL neighbor aggregation can be abstracted as a function over a multi-set GCN: $Mean({x_u}{u\\in N(v)})$, GraphSAGE: $Max({x_u}{u\\in N(v)})$ GCN \u0026amp; GraphSAGE cannot distinguish different multi-sets with the same color proportion. Any injective multi-set function can be expressed as :$\\phi(\\sum_{x\\in S}f(x))$, where $\\phi, f$ refer to non-linear function Graph Isomorphism Network relation to WL kernel Heterogeneous graphs relational GCNs, Knowledge graph, embeddings for KG Completion $G = (V,E,R,T)$, nodes, edges, node type, relation type Relational GCn multiple edge types recap GCN: what if the GCN has multiple relation types Use different network weights for different relation types:$h_v^{(l+1)} = \\sigma(\\sum_{r\\in R}\\sum_{u \\in N_v^r}\\frac{1}{C_{v,r}}W_r^{(l)}h_u^{(l)}+W_0^{(l)}h_v^{(l)})$ Share weights across relations represent the matrix of each relation as a linear combination of basis transformations $W_r = \\sub_b a_{rb}V_b$, where $V_b$ is share across all relations, so each relation only needs to learn $a_{rb}$ link prediction: Knowledge graph completion KG in graph: entities, types, relationships. KG task: predict the missing tails. shallow encoding: the encoder is just an embedding lookup KG representation: (head relation, tail)=(h,r,t) given the (h,r,t), the embedding (h,r) should be close to t. scoring function: $f_r(h,t) = -||h+r-t||$ relations in a heterougeneous KG have different properties. relation patterns: symmetric, antisymmetric, inverse, composition, 1-to-N antisymmetric: TansE cannot model symmetric or antisymmetric, 1-to-N TransR: $h^\\prime = M_r h, t^\\prime = M_r t$ scoring function: $f_r(h,t) = -||h^\\prime+r-t^\\prime||$ DisMult: cannot model antisymmetric, inverse, compositional. scoring function (hrt) ComplEx Reasoning in KG Goal: How to perform multi-hop reasoning over KGs one-hop, path, conjunctive KG incompleteness is not able to identify all anser entities. Answering in KGs TransE: $a = v_a + r_1 + r_2 + \\cdots$, TransR, DistMult, ComplEx cannot handle the path queries KGs and Box embeddings: entity embeddings: entites are seen as a zero-volume boxes relation: each relation takes a box and produces a new box: Generative models for graphs motivation for graph generation: we want to generate realistic graph understand formulation of graphs predictions graph use same process to general novel graph instances Road map properties of real-world graphs traditional fraph generative models deep graph generative models propertites: degree distribution: probability a randomly chosen node has degree k clustering coefficient: how connected are i\u0026rsquo;s neighbors to each other connectivity: size of the largest connected component Path length: 90% within 8 hops-\u0026gt; small world model Erdos-Renyi Random Graphs $G_{np}$ undirected graph on n nodes where each edge iid with probability -. propertities: Degree distribution: binomial distribution clustering coefficient path length Deep grah generation generate graph are similar to a given set of graphs goal-direted graph generation given $p_{data}(G)$ learn the distribution $p_{model}(G)$ sample $p_{model}(G)$ the most common approach: sample $z_i$ and transform to graph $x$ with $f(z_i)$ auto-regressive models: $p_{model}(x;\\theta)$ is used for both density estimation and sampling. (VAE and GAN have 2 or more models, each playing one of the rules) idea: chain rule. $p_{model}(x;\\theta) = \\prod_{t=1}^n p_{model}(x_t|x_1,x_2, \\cdots, x_{t-1};\\theta)$ where $x_t$ will be the t-th action (add node, add edge) GraphRNN generating graphs via sequentially adding nodes and edges.\nnode-level: add nodes, one at a time edge-level: add edges each node step is an edge sequences. each time add a new node, decision on edge connection to every former nodes has to be made. Summary: a graph + a node ordering = a sequence of sequences. transform to a sequence generation problem-\u0026gt; RNN has a nodel-level RNN adn an edge-level RNN scaling up by Breadth First Search (BFS)\nCompare sets of training graph statistics and generated graph statistics\nhow to compare two graph statistics: Earth mover distance how to compare sets of graph statistics: Maximum Mean Discrepancy Earth mover distance: measure the minimum effort that move earth from one pile to the other.\nMaximum Mean Discrepancy:\nApplication of deep graph generation molecule generation GCPN: use reinforcement learning to decide whether take action to link two nodes. Position-aware GNN structure-aware task position-aware task: use anchor nodes to locate nodes in the graph more anchors can better characterize node position in different regions of the graph. Identity-aware GNN assign a color to the node we want to embed heterogenous message passing: another GNN applies different message to nodes with different colorings. scaling up GNNS when nodes too many, hard to train GraphSAGE neighbor sampling randomly sample M nodes, get k-hop neighborhood of each node, tehn construct computational graph to train. sampling at most H neighbors at each hop to reduce computaion complexity. cluster GCN sample a small subgraph of the large graph and perform layer-wise node embeddings simplifying GCN ","permalink":"https://x423xu.github.io/posts/2022-06-10-graph-cs224w/","summary":"1 Jure Leskovec, Stanford CS224W: Machine Learning with Graphs, http://cs224w.stanford.edu Why graphs relations of entities Similar data points arbitrary sizes, no spatial index, no reference order representation learning Map nodes to d-dimensional embeddings-\u0026gt; similar nodes in the network are embedded close together Applications of Graph ML different tasks: Node classification Link prediction: knowledge graph completion Graph classification: molecule property prediction Clustering Graph generation Graph evolution: physical simulation Examples: node-level: Protein folding Recommender system: recommend related pins to users by edge level classification subgraph-level: traffic prediction: nodes: road segments, edges: connectivity between nodes-\u0026gt; predict time arrival etc graph-level: drug discovery: nodes: atoms, edges: chemical bonds.","title":"graph and machine learning"},{"content":"Installation Install on local machine from singularity-installation Create an \u0026ldquo;install.def\u0026rdquo; file: An example file: (docker image downloaded from here) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 Bootstrap: docker From: pytorch/pytorch:1.9.0-cuda11.1-cudnn8-devel %post apt-get update apt-get install -y gcc apt-get install -y g++ apt-get install -y libglib2.0-0 apt-get install -y libsm6 apt-get install -y libx11-dev apt-get install -y git pip install yacs pip install torchvision==0.10.0 pip install numpy pip install matplotlib pip install h5py pip install pickle-mixin pip install Pillow pip install tqdm pip install pytorch3d pip install regex pip install requests pip install easydict pip install ninja pip install cython pip install dill pip install opencv-python pip install pycocotools pip install cffi pip install scipy pip install msgpack pip install pyaml pip install tensorboardX pip install timm Create sif file in sanding box way sudo singularity build \u0026ndash;sandbox ./sif/ install.def # create neccessary files for a sandbox in sif folder sudo singularity shell \u0026ndash;writable sif/ # open a shell environment for installing packages using apt-get xxx or pip install xxx to install your desired packages sudo singularity build xiaoyu.sif ./sif/ # build a sif file from sif folder Deploy in remote server upload sif file to the remote server. create python virtual env: python -m venv ./venv --system-site-packages execute command: singularity exec --nv -B /scratch/xiaoyu -B /project/def-zhouwang/xiaoyu /project/def-zhouwang/xiaoyu/xiaoyu.sif /scratch/xiaoyu/venv/bin/python train_mde.py ${arg_arr[$SLURM_ARRAY_TASK_ID]} An example shell file: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #!/bin/bash #SBATCH --account=rrg-zhouwang #SBATCH --time=20:00:00 #SBATCH --job-name=RaMDE #SBATCH --gres=gpu:p100:4 #SBATCH --cpus-per-task=8 #SBATCH --array=0-7 #SBATCH --mem-per-cpu=3000M #SBATCH --output=./%A-%a.out module load singularity source /scratch/xiaoyu/venv/bin/activate cd /scratch/xiaoyu/depth-estimation/source/RaMDE/ arg_arr[0]=\u0026#34;--bs 8 --workers 4 --data_path ../../data/kitti_data --filenames_file ./train_test_inputs/kitti_eigen_train_files_with_gt.txt --data_path_eval ../../data/kitti_data --filenames_file_eval ./train_test_inputs/kitti_eigen_test_files_with_gt.txt --gt_path ../../data/kitti_depth/ --gt_path_eval ../../data/kitti_depth/ --dataset kitti --do_kb_crop --max_depth 80 --max_eval_num 50 --algo tri_graph --model_name RaMDE --print_every 100 --validate_every 100\u0026#34; arg_arr[1]=\u0026#34;--bs 8 --workers 4 --data_path ../../data/kitti_data --filenames_file ./train_test_inputs/kitti_eigen_train_files_with_gt.txt --data_path_eval ../../data/kitti_data --filenames_file_eval ./train_test_inputs/kitti_eigen_test_files_with_gt.txt --gt_path ../../data/kitti_depth/ --gt_path_eval ../../data/kitti_depth/ --dataset kitti --do_kb_crop --max_depth 80 --max_eval_num 50 --algo tri_graph --orthogonal_disable --model_name RaMDE --print_every 100 --validate_every 100\u0026#34; arg_arr[2]=\u0026#34;--bs 8 --workers 4 --data_path ../../data/kitti_data --filenames_file ./train_test_inputs/kitti_eigen_train_files_with_gt.txt --data_path_eval ../../data/kitti_data --filenames_file_eval ./train_test_inputs/kitti_eigen_test_files_with_gt.txt --gt_path ../../data/kitti_depth/ --gt_path_eval ../../data/kitti_depth/ --dataset kitti --do_kb_crop --max_depth 80 --max_eval_num 50 --algo tri_graph --attention_disable --model_name RaMDE --print_every 100 --validate_every 100\u0026#34; arg_arr[3]=\u0026#34;--bs 8 --workers 4 --data_path ../../data/kitti_data --filenames_file ./train_test_inputs/kitti_eigen_train_files_with_gt.txt --data_path_eval ../../data/kitti_data --filenames_file_eval ./train_test_inputs/kitti_eigen_test_files_with_gt.txt --gt_path ../../data/kitti_depth/ --gt_path_eval ../../data/kitti_depth/ --dataset kitti --do_kb_crop --max_depth 80 --max_eval_num 50 --algo tri_graph --attention_disable --orthogonal_disable --model_name RaMDE --print_every 100 --validate_every 100\u0026#34; arg_arr[4]=\u0026#34;--bs 12 --workers 4 --data_path ../../data/NYUv2Whole/nyudepthv2/train --data_path_eval ../../data/NYUv2Whole/nyudepthv2/val --dataset nyu --max_depth 10 --max_eval_num 50 --algo tri_graph --model_name RaMDE --print_every 100 --validate_every 100\u0026#34; arg_arr[5]=\u0026#34;--bs 12 --workers 4 --data_path ../../data/NYUv2Whole/nyudepthv2/train --data_path_eval ../../data/NYUv2Whole/nyudepthv2/val --dataset nyu --max_depth 10 --max_eval_num 50 --algo tri_graph --orthogonal_disable --model_name RaMDE --print_every 100 --validate_every 100\u0026#34; arg_arr[6]=\u0026#34;--bs 12 --workers 4 --data_path ../../data/NYUv2Whole/nyudepthv2/train --data_path_eval ../../data/NYUv2Whole/nyudepthv2/val --dataset nyu --max_depth 10 --max_eval_num 50 --algo tri_graph --attention_disable --model_name RaMDE --print_every 100 --validate_every 100\u0026#34; arg_arr[7]=\u0026#34;--bs 12 --workers 4 --data_path ../../data/NYUv2Whole/nyudepthv2/train --data_path_eval ../../data/NYUv2Whole/nyudepthv2/val --dataset nyu --max_depth 10 --max_eval_num 50 --algo tri_graph --attention_disable --orthogonal_disable --model_name RaMDE --print_every 100 --validate_every 100\u0026#34; singularity exec --nv -B /scratch/xiaoyu -B /project/def-zhouwang/xiaoyu /project/def-zhouwang/xiaoyu/xiaoyu.sif /scratch/xiaoyu/venv/bin/python train_mde.py ${arg_arr[$SLURM_ARRAY_TASK_ID]} ","permalink":"https://x423xu.github.io/posts/2022-05-24-singularity-deployment/","summary":"Installation Install on local machine from singularity-installation Create an \u0026ldquo;install.def\u0026rdquo; file: An example file: (docker image downloaded from here) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 Bootstrap: docker From: pytorch/pytorch:1.9.0-cuda11.1-cudnn8-devel %post apt-get update apt-get install -y gcc apt-get install -y g++ apt-get install -y libglib2.","title":"How to deploy singularity for data processing"},{"content":"Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods Abstract Focus on ten prominent tasks that integrate language and vision by discussing their problem formulation, methods, existing datasets, evaluation measures, and compare results with the SOTA methods.\nIntroduction Multimodal learning models: generate comprehensible but concise and grammatically well-formed descriptions of the visual content, or vice versa by generating the visual content for a given textual description in a natural language of choice, identify objets in the visual content and infer their relationships to reson about, or answer arbitrary questions about them, navigate through and environment by leveraging input from both vision and natural language instructions, translate textual content from one language to another while leveraging the visual content for sense disambiguation, generate stories about the visual content, and so on. Applications: visually impaired individuals can be asisted thereby. automatic survelliance autonomous driving human-computer interaction city navigation Task-specific multimodal models image description (Bernardi et al.,2016; Bai \u0026amp; An, 2018; Hossain et al., 2019) video description generation (Aafaq et al.,2020) visual question answering (Kafle \u0026amp; Kanan, 2017; Wu et al., 2017) action recognition (Gella \u0026amp; Keller, 2017) visual semantics (Liu et al., 2019) NLP natual language generation (Gatt \u0026amp; Krahmer, 2018; Garbacea \u0026amp; Mei, 2020) NLP commonsense reasoning (Storks et al., 2019) understanding the limitations of the integration of vision and language research (Kafle et al., 2019) Background Computer vision tasks Image as visual information Utilization the tasks where images are used as input the representation of images Tasks Image classification Object localization Object detection Object segmentation Object Identification Instance segmentation Panoptic segmentation Reoresentation self supervised learning (Jing \u0026amp; Tian, 2019)\nVideo as visual information Utilization knowing the tasks where videos are used as inputs the representation of a video Takss Object tracking Action classification Emotion detection Scene detection Automated editing NLP tasks Tasks understanding language generating language Some of the classical NLP tasks, that are used to comprehend language, are shallow parsing, syntax parsing, semantic role labeling, named entity recognition, entity linking, co-reference resolution, etc.\nRepresentation CV and NLP integration tasks Extension of NLP tasks Visual Description Generation: The goal is to generate a human-readable text snippt that describes the input Visual Storytelling: A sequence of visual inputs is used to generate a narrative summary based on text aligned with them Visual Question Answering: Answer questions about a visual input. Visual Dialog: Aim at creating a meaningful dislog in a natural and conversational language about a visual content. Visual Reeferring Expression: Visual Entailment: An inference task for predicting whether the image semantically entails the text. Multimodal Machine Traslation: Translate from source language to target language by leveraging the visual information as auxiliary modality along with the natural language text in source language. Extension of CV tasks Visual generation: generate visual content by conditioning on input text from a chosen natural language. Visual resoning: It is expected to output a relationship between detected objects by generating an entire visual scene graph. The scene graph is leverage to reason and answer questions about visual content. It can also be used to reason about whether a natural language statement is true or not regarding a visual input. Extension of both NLP and CV tasks Vision-and-Language Navigation: natural language navigation should be interpreted based on visual input. Combine both vision and language. Visual Description Generation and Storytelling generate a textual description when conditioned on visual input\nVisual description generation Aim to generate either a global descriptionor dense captions for a given visual input.\nImage description generation Standard image description generation: generate a sentence-level description of the scene in a given image Dense image description generation: create descriptions at the local object-level in a given image. Image paragraph generation: create paragraphs instead of generating a single simple description. Generated paragraphs are expected to be coherent and contain fine-grained natural language descriptions. Spoken language image description generation: expand the description generation task to work with spoken language, instead o flimiting to only the written forms of language. Stylistic image description generation: add style to the standard image description generation, where the generated descriptions adhere to a specific style. Unseen objects image description generation: leverage images which lack paired descriptions. Generate descriptions for visual object categories previously unseen in image-description corpora. Diverse image description generation: incorporate variety and diversity in the generated captions. Controllable image description generation: select specific objects in an image, defined by a control signal, to generate descriptions. Image caption emendation as generation: build a model to emend both syntactic and semantic errors in the captions. Video description generation Global description generation:\nground sentences that describe actions in the visual information extracted from videos (Motwani \u0026amp; Mooney, 2012; Regneri et al., 2013) Generate global natural language descriptions for videos with various approaches: leveraging latent topics (Das et al., 2013), corpora knowledge (Krishnamoorthy et al., 2013), graphical models (Rohrbach et al., 2013), and sequence-to-sequence learning (Venugopalan et al., 2015b, 2015a; Donahue et al., 2015; Srivastava et al., 2015; Xu et al., 2016; Ramanishka et al., 2016; Jin et al., 2016), factor graph(Thomason et al., 2014) combines visual detection with language statistics. Seq2seeq based approaches: extra corpora (Venugopalan et al., 2016), soft-attention (Yao et al., 2015), multimodal fusion (Hori et al., 2017), temporal attention (Song et al., 2017), semantic consistency (Gao et al., 2017), residual connections(Li et al., 2019). Incorporation of semantic attributes learned from videos, ensembled-based description generator networks (Shetty et al., 2018), encoder-decoder reconstructors (Wang et al., 2018). Other approaches: combined with entailment generation task (Pasunuru \u0026amp; Bansal, 2017a), multiple fine-grained actions (Wang et al., 2018b), reinforcement learning (Pasunuru \u0026amp; Bansal, 2017b), Visual Text correction system (Mazaheri \u0026amp; Shah, 2018), object relational graph baed encoder, language model decoder (Zhang et al., 2020). Dense video description generation:\nachieve fine-grained video understanding by addressing two sub-problems: (1) localizing events in a video, and (2) generating captions for these localized events. the core challenge, namely the automatic evaluation of video captioning, is still unsolved.\nMovie description generation: input movie clips, align books to movies (Tapaswi et al., 2015; Zhu et al., 2015), movie descriptions (Rohrbach et al., 2015).\nVisual storytelling The task of visual storytelling aims to encode a sequence of images or frames (in the video) to generate a paragraph which is story-like.\nImage storytelling The aim of image storytelling is to generate stories from a sequence of images.\nsemantic coherence is captured in a photo stream. discover semantic embeddings correlations (Yu et al., 2017), incorporated with reinforcement learning (Wang et al., 2018), hierarchically structured reinforced training (Huang et al., 2019), adversarial reward learning Wang et al. (2018a). suffer from repetitiveness, the same objects/events undermine a good story structure. -\u0026gt; inter-sentence diversity was explored with diverse beam search (Hsu et al., 2018). Video storytelling (less explored) In comparison to image storytelling, which only deals with a small sequence of images, the aim of video storytelling is to generate coherent and succinct stories for long videos.\nPinoeer Li et al. (2020) address challenges like diversity in the story and the inherent complexity of video. goal: offer support to people with visual disabilities or technical issues like internet bandwidth limitations. Visual Referring Expression Comprehension and Generation The objective of the task is to ground a natural language expression (e.g. a noun phrase or a longer piece of text) to objects in a visual input.\nImage referring expression comprehension and generation In a natural environment, people use referring expressions to unambiguously identify, indicate, or point to particular objects. This is usually done with a simple phrase or within a larger context (e.g. a sentence). Having a larger context provides better scope for avoiding ambiguity and allows the referential expression to easily map to the target object. However, there can also be other possibilities in which people are asked to describe a target object based on its surrounding objects.\nused to generate, an algorithm generates a referring expression for a given target object which is present in a visual scene approaches: (FitzGerald et al., 2013) tackled the problem from the perspective of density estimation, learn distributions over logical exprssions identifying sets of objects in the world. used to perform comprehension, an algorithm locates in an image the object described by a given referring expression. approaches: Nagaraja et al. (2016) integrate contexts between objects. Multiple Instance Learning (MIL). Hu et al. (2016) leverage a NLP query of the object to localize a target object, integrating spatial configurations and global scene-level contextual information. (Yu et al., 2018) subject appearence, location, and relationship to other objects. (Cirik et al., 2018a) syntactic analysis, build a dynamic computation graph. variational model (Zhang et al., 2018) cross-modal: (Yang et al., 2019) cross-modal relationship inference， 1. highlight objects and relationships connected with a referring, 2. multi-modal semantic contexts. (可以将空间的referring用到depth estimation里面吗)。Recursive Grounding Tree (Hong et al., 2019) binary tree to parse referring expression-\u0026gt; visual reasoning. (Liu et al., 2019), combining visual reasoning with referential expressions. object segmentation based referring expression (Liu et al., 2017). Image referring expression comprehension and generation combination (Mao et al., 2016; Yu et al., 2016) find visual comparison to other objects within an image. (Yu et al., 2017a) a speaker, a listener, and a reinforcer. The speaker generate referring expressions, the listner comprehend referring expressions, the reinforce use a reward function to guide sampling of more discriminative expressions. Video referring expression comprehension and generation Vasudevan et al. (2018) stereo videos to explore temporal-spatial contextual information. Khoreva et al. (2018) language referring expressions to achieve object segmentation. Wang et al. (2020) video grounding with contextual information. Visual question answering, reasoning, and entailment they share the common intention of answering questions when conditioned on a visual input\nVisual question answering The goal of Visual Question Answering (VQA) is to learn a model that comprehends visual content at both the global and local level for finding an association with pairs of questions and answers in the natural language form.\nImage Q\u0026amp;A as Visual Turing Test (Malinowski \u0026amp; Fritz, 2014; Malinowski et al., 2015; Geman et al., 2015). fill-in-the-blank tasks (Yu et al., 2015), multiple-choice question-answering for images. Address open-ended Image Q\u0026amp;A (Antol et al., 2015; Agrawal et al., 2017), ask free-form natural language question. Binary image Q\u0026amp;A (Zhang et al., 2016). Relate local regions in the images (Zhu et al., 2016) by addressing object-level grounding Interpretability or explainability by overcoming priors (Agrawal et al., 2018). Generate human-interpretable rules that provide better insights (Manjunatha et al., 2019). cycle-consistency (Shah et al., 2019a). outside knowledge (Marino et al., 2019)(Shah et al., 2019b). Multi-task learning, federated learning. Video question answering is to answer natural language questions about videos.\n(Tu et al., 2014) jointly parsing videos with corresponding text to answer queries. open-ended movie Q\u0026amp;A (Tapaswi et al., 2016). fill-in-the-blank questions (Zhu et al., 2017; Mazaheri et al., 2017). (Zeng et al., 2017) free-form Q\u0026amp;A. High-level concept words (Yu et al., 2017b). Attention (Jang et al., 2017). spatio-temporal grounding (Lei et al., 2020) STAGE (Lei et al., 2020), aligned fusion is essential for improving Video Q\u0026amp;A. Visual reasoning is to learn a model that comprehends the visual content by reasoning about it.\nImage reasoning s to answer sophisticated queries by reasoning about the visual world.\n(Johnson et al., 2017a) design diagnostic tests going beyond benchmarks. VQA struggle with comparing the attributes of objects or novel attribute combinations. (Johnson et al., 2017b) program generator. (Hu et al., 2017) predict instance-specific network layouts. Santoro et al. (2017) relation-aware visual features. (Cao et al., 2018) global context reasoning. Mascharka et al. (2018) proposed a set of visual-reasoning primitives. Learning-By-Asking (LBA) (Misra et al., 2018b), mimic natural learning with the goal to make it more data efficient. compositional attention networks (Hudson \u0026amp; Manning, 2018) explicit and expressive reasoning. neural-symbolic visual question answering (Yi et al., 2018), recover structural scene representation from the image and a program trace from the question. Neuro-Symbolic Concept Learner (NS-CL) (Mao et al., 2019) learns visual concepts, word, and semantic parsing of sentences without explicit supervision. It learns by simply looking at images and reading paired questions and answers. multimodal relation network (Cadène et al., 2019) learn end-to-end reasoning over real images. Aditya et al. (2019) used spatial knowledge to aid visual reasoning, knowledge distillation, relational resoning, probabilistic logical languages. Explainable and explicit neurla modules (Shi et al., 2019) scene graph. Andreas et al. (2016a, 2016b) exploit the compositional linguistic structure of complex questions by forming neural module networks which query about the abstract shapes observed in an image. Compositional question answering (Hudson \u0026amp; Manning, 2019). Zellers et al. (2019) commonsense knowledge. NLVR (Suhr et al., 2017) and NLVR2 (Suhr et al., 2019) true or false. Video reasoning The goal of COG, Yang et al. (2018), is to address problems related to visual and logical reasoning and memory\nVisual entailment is to learn a model that predicts whether the visual content entails the augmented text along with hypothesis.\nImage entailment Vu et al. (2018), a visually grounded version of the textual entailment where an image is augmented with textual premise and hypothesis. Xie et al. (2019) predicts whether the image semantically entails the text, given image-sentence pairs, the premise is defined by an image. Video entailment (Liu et al., 2020) to infer whether the natural language hypothesis is entailed or contradicted when given a video clip aligned with the subtitles information\nVisual Dialog involves a complex interaction between a human and an artificial agent.\nImage Dialog is to create AI agents that can hold dialog with humans in a natural language of choice about a visual content (Das et al., 2017a), represented by an image. To be more specific, given an image, a history of dialogs, and a question about the image, the goal of an AI agent is to ground the question in the image, infer the context from the history, and then answer the question accurately\n(de Vries et al., 2017) locate an unkown object in the image by asking a swquence of questions. (Mostafazadeh et al., 2017). hold natural-sounding conversations about a shared image. (Lu et al., 2017a) transfer from dialog generation. Seo et al. (2017) attentive memory. (Wu et al., 2018) reinforcement learning and GAN. (Kottur et al., 2018) form explicit and grounded co-reference between nouns and pronouns. (Niu et al., 2019) recursive visual attention. (Zheng et al., 2019) graphical model inference. Guo et al. (2019) builds an image-question-answer synergistic network. (Shekhar et al., 2019) a visually grounded encoder guessing and asking questions. Video dialog is to leverage scene information containing both audio (which can be transcribed as subtitles) and visual frames to hold a dialog (i.e., an exchange) with humans in a natural language of choice about the multimedia content (Alamri et al., 2019b 2019a).\nmultimodal-based video description (Hori et al., 2019). Multimodal Machine Translation (MMT) is to translate natural language sentences that describe visual content (e.g. image) in a source language into a target language by taking the visual content as an additional input to the source language sentences.\nMachine tanslation with image is to translate sentences, that describe an image, in a source language into equivalent sentences in a target language.\nsingle source translation, multisource MMT. multimodal attention (Huang et al., 2016). doubly-attentive decoder incorporated visual features (Calixto et al., 2017). learning to translate, and learning visually grounded representations (Elliott \u0026amp; Kádár, 2017). noisy image captions for MMT (Schamoni et al., 2018). Machine translation with video (Wang et al., 2019b) is to translate a source language description into the target language equivalent using the video information as additional spatio-temporal context.\nLanguage-to-Vision Generation is to generate visual content given their natural language descriptions.\nLanguage-to-Image Generation sentence-level language-to-image generation: generate images cnditioned on the natural language descriptions. (Mansimov et al., 2016) iteratively draw patches on a canvas, attending to the relavant words in the description. (Reed et al., 2016b) visual concepts could be translated from characters to pixels with conditional gan. (Reed et al., 2016a) what content should be drawn in which location for high quality image generation. (Nguyen et al., 2017) conditioned on image classes. (Dash et al., 2017) condition on both sentence and class information. stackGAN (Zhang et al., 2017, 2019). attention-based GAN (Xu et al., 2018) (Hong et al., 2018) infer the semantic layout of the image. Johnson et al. (2018) used image-specific scene graphs enabling explicitly reasoning about objects and their relationships. Image manipulation TAGAN (Nam et al., 2018) generate semantically manipulated images while preserving text-irrelevant contents. Only regions correspond to the given text are modified. (Zhu et al., 2019) attention generator, discriminator. (Li et al., 2020) designed error correction modules to rectify mismatched attributes and complete the missing contents. Fine-grained image generation (El-Nouby et al., 2018) recurrent image generation, output up to the current step \u0026amp; past instructions. (Hinz et al., 2019) control location of objects by adding a pathway in an iterative manner. Sequential image generation StoryGAN (Li et al., 2019b) opposite to storytelling. Language-to-Video generation (Li et al., 2018) conditional generative model Vision-and-Language Navigation is to carry out navigation in an environment by interpreting natural language instructions\nImage-and-Language Navigation (Anderson et al., 2018b) an autonomous agent navigate in an environment by interpreting natural language instructions. (Wang et al., 2019a), reinforced cross-modal matching. (Fried et al., 2018) train an action space with an embedded speaker model. Embodied Question Answering (Das et al., 2018a, 2018b). interactive question answering (Gordon et al., 2018). grounded dialog (de Vries et al., 2018) Vison-and-Language pretraining To jointly learn representations using both visual and textual content\nSingle-stream architectures BERT-like (Devlin et al., 2019). VideoBERT (Sun et al., 2019). Bounding Boxes in Text Transformer (B2T2) (Alberti et al., 2019). Unicoder-VL (Li et al., 2020). VL-BERT (Su et al., 2020), VLP (Zhou et al., 2020), OSCAR (Li et al., 202), VinVL (Zhang et al., 2021) can jointly understand and generate from cross-modal data. (Cao et al., 2020) probe. Two-stream architectures two independent encoders for learning visual and text representations. ViLBERT (Lu et al., 2019) and LXMERT (Tan \u0026amp; Bansal, 2019). -neuro-symbolic reasoning systems (Yi et al., 2018; Vedantam et al., 2019).\n","permalink":"https://x423xu.github.io/posts/2022-04-18-visual+text-review/","summary":"Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods Abstract Focus on ten prominent tasks that integrate language and vision by discussing their problem formulation, methods, existing datasets, evaluation measures, and compare results with the SOTA methods.\nIntroduction Multimodal learning models: generate comprehensible but concise and grammatically well-formed descriptions of the visual content, or vice versa by generating the visual content for a given textual description in a natural language of choice, identify objets in the visual content and infer their relationships to reson about, or answer arbitrary questions about them, navigate through and environment by leveraging input from both vision and natural language instructions, translate textual content from one language to another while leveraging the visual content for sense disambiguation, generate stories about the visual content, and so on.","title":"visual text review"},{"content":"Content\nBackground Preface Introduction What is logical syntax Language as calculi THE DEFINITE LANGUAGE 1\nRules of formation for language 1 Predicates and functors Syntactical gothic symbols The junction symbols Content Background What is syntax\nIn logic, syntax is anything having to do with formal languages or formal systems without regard to any interpretation or meaning given to them. Syntax is concerned with the rules used for constructing, or transforming the symbols and words of a language, as contrasted with the semantics of a language which is concerned with its meaning\nThe symbols, formulas, systems, theorems, proofs, and interpretations expressed in formal languages are syntactic entities whose properties may be studied without regard to any meaning they may be given, and, in fact, need not be given any.\nPreface The purpose of present work is to give a systematic exposition of such a mthod, namely, of the method of \u0026ldquo;logical syntax\u0026rdquo;. The aim of logical syntax is to provide a system of cocncepts, a language, by the help of which the results of logical analysis will be exactly formulable. phylosophy is to be replaced by he logical syntax of the language of science. Language I covers a narrow field of concepts, lanuage II is richer in modes of expression. Introduction What is logical syntax The logical syntax mean the formal theory of the linguistic forms of a language-the systematic statement of the formal rules which govern it together with the development of consequences which follow from these rules.\nThe syntax of a language is supposed to lay down rules according to which the linguistic structures (e.g. the sentences) are to be built up from the elements (e.g. words). The chief task of logic is supposed to be that of formulating rules according to which judgemets may be inferred from other judgements; in other words, according to which conclusions may be drawn from premisses.\nThe logic can be studied with any degree of accuracy when it is based not on judgements (thoughts, or the content of thoughts) but rather on linguistic expressions. Sentence is the most important. The rules of logic are formal, logical characteristics of sentences and the logical relations are dependent on syntactic structure of the sentences. -\u0026gt; logic will become a part of syntax. -\u0026gt; difference: formation rules \u0026amp; transformation rules. -\u0026gt; designate as logical syntax the system which comprises the rules of formation and transformation\nnatural word-language (such as German \u0026amp; Latin) is unsystematic and logically imperfect structured. The same arises in the artificial word-languages.\nRules of formation and transformation permit of being formally apprehended.\n-\u0026gt; Consider the syntax of two artificially constructed formal symbolic language (employ formal symbols instead of words).\nThe sentences, definitions, and rules of the syntax of a language are concerned with the forms of that language.\nhow are them correctly expressed? is a kind of super-language necessary? a third language to explain the super-language? is it possible to formulate the syntax of a language with in the language itself? -\u0026gt; it is possible to express the syntax of a language in the language itself. start by constructing the syntax, then procced to formalize its concepts and thereby determine its logical character. object-language, syntax-language\nLanguage as calculi A system of conventions or rules is understood by a calculus. The rules are concerned with symbols about the nature, and relations of the nature are distributed in various classes. Any finite series of these symbols is called an expression of the calculus in question. The rules of the calculus determine the condition under which an expression can be said to belong to a certain category of expressions. -\u0026gt; formation = syntactic rules under what conditions the transformation of one or more expressions into another or others are allowed. -\u0026gt; transformation = logical laws of deduction. a system of a language is a calculus every well-determined mathematical discipline is a calculus. logical syntax is the same thing as the construction and manipulation of a calculus. Languages are the most important examples of calculi. The syntax is only concerned with the formal properties of expressions, thw design of the individual symbols is indifferent. Any series of any things will equally well serve as terms or expressions in a calculus, or in a language. pure syntax and descriptive syntax ~ mathmetical geometry and physical geometry pure syntax concerned with the forms of sentences. THE DEFINITE LANGUAGE 1 Rules of formation for language 1 Predicates and functors previous knowledge: objects: proper names. e.g. house name, color names. name-language sysematic positional co-ordinates (symbols show the place of the objects in the system, and their positions in relation to one another). e.g. house number, color figures/letters. co-ordinate-language In language 1, natural numbers as co-ordinates. predicates: to express a property of an object, or of a position, or a relation between several objects or positions. predicates are proper names for the properties of positions. descripptive predicates: express empirical properties or relations logical predicates: functors: express properties or relations of position by means of numbers. descriptive functors, logical functors. numerical expression: an expression which in any way designates a number. e.g. \u0026ldquo;te(3), sum(3,4)\u0026rdquo; sentence: an expression which corresponds to a propositional sentence of a word-language. e.g.\u0026ldquo;te(3)=5, sum(3,4)=7, blue(3)\u0026rdquo; Syntactical gothic symbols two expressions are equal when their corresponding symbols are equal symbols. If two symbols or two expressions are equal, they have same syntactical design. An exprssion of I consists of an ordered series of symbols of I. By a syntactic form we understand any kind or category of expressions which is syntactically determined. Five kinds of symbols: eleven indivisual symbols: variables constant predicates fuctors The junction symbols The one-term or two-term junction symols are used to costruct a new sentences outof one or two sentences respectively. ","permalink":"https://x423xu.github.io/posts/2022-04-09-logical-syntax/","summary":"Content\nBackground Preface Introduction What is logical syntax Language as calculi THE DEFINITE LANGUAGE 1\nRules of formation for language 1 Predicates and functors Syntactical gothic symbols The junction symbols Content Background What is syntax\nIn logic, syntax is anything having to do with formal languages or formal systems without regard to any interpretation or meaning given to them. Syntax is concerned with the rules used for constructing, or transforming the symbols and words of a language, as contrasted with the semantics of a language which is concerned with its meaning","title":"Logical Syntax"},{"content":"2022/3/31 ~ 2022/4/6 Order-Embeddings of images and Language Core idea Explicitly modeling the partial order structure of the hierarchy over language and image -\u0026gt; Visual sematic hierarchy How to do it Penalize order violations $$ E(x,y) = ||max(0,y-x)||^2 $$ where $E(x,y)=0 \\Leftrightarrow x \\preceq y$ Modeling heterogeneous hierarchies with relation-specific hyperbolic cones Core idea Embeds entities into hyperbolic cones \u0026amp; models relations as transformations between the cones How to do it Poincare entailment cone at apex $x$ $$\\zeta_x = {y\\in \\Beta^d | \\angle_xy\\leq\\sin^{-1}(K\\frac{1-||x||^2}{||x||})}$$ embed entity: $h=(h_1,h_2,\\cdots,h_{d})$, where $h_i\\in\\Beta^2$ is the apex of the $i-$th 2D hyperbolic cone. Transformation: Rotation transformation: $f_1(h,r) = \\exp_o(\\mathbf{G}(\\theta)\\log_o(h))$, where $\\mathbf{G(\\theta)=\\left[\\begin{matrix}cos(\\theta)\u0026amp;-sin(\\theta)\\sin(\\theta)\u0026amp;cos(\\theta)\\end{matrix}\\right]}$ Restricted rotation transformation:$f_2(h,r)=\\exp_h(s\\cdot \\mathbf{G}(\\theta\\frac{\\phi}{\\pi})\\bar h), r=(s,\\theta)$ Loss function: $\\ell_d=-\\log\\sigma(\\psi(h,r,t))-\\sum_{t^{\u0026rsquo;}}\\frac{1}{|T|}\\log\\sigma(-\\psi(h,r,t^{\u0026rsquo;}))$ $\\ell_a(h,r,t)=m\\cdot(max(0,\\angle_{h_i}t_i-\\phi(h_i)))$ Learning to detect unseen object classes by between-class attribute transfer Core idea Decompose an image/object into high-level semantic attributes such as shape, color, geographic information. Then detect new classes based on their attribute representations. How to do it Direct attribute prediction: decouple training images into attribute variables $\\alpha_m$ per-attribute parameters $\\beta_m$ at test: image $x\\rightarrow\\beta_m\\rightarrow \\alpha_m\\rightarrow class$ $$p(z|x)=\\sum_{a\\in{0,1}^M}p(z|a)p(a|x) = \\frac{p(z)}{p(a^z)}\\prod_{m=1}^{M}p(a_m^z|x)$$ Indirect attribute prediction add an attribute layer between two label layers for the indirect one, $p(a_m|x)=\\sum_{k=1}^K p(a_m|y_k)p(y_k|x)$ Zero-shot learning via joint similarity embedding Core idea To test if source-target pair matches each other Multi-grained vision language pre-training: aligning texts with visual concepts Core idea Locate visual concepts in the image given the associated texts \u0026amp; align the texts with the visual concepts. How to do it Bounding box loss: $$\\ell_{bbox} = \\mathbb{E}{(V^j,T^j)\\sim I;I\\sim D}\\left[\\ell{iou}(b_j,\\hat{b}_j)+||b_j-\\hat{b}_j||_1\\right]$$ Contrastive learning: $$p^{v2t}(V) = \\frac{\\exp(s(V,t)/\\tau)}{\\sum_{i=1}^{N}\\exp(s(V,T^i)/\\tau)}$$ $$p^{t2v}(T) = \\frac{\\exp(s(V,t)/\\tau)}{\\sum_{i=1}^{N}\\exp(s(V^i,T)/\\tau)}$$ $$\\ell_{cl}=\\frac{1}{2}\\mathbb{E}_{V,T\\sim D}\\left[H(y^{v2t}(V),p^{v2t}(V))+H(y^{t2v}(T),p^{t2v}(T))\\right]$$ Match prediction: whether a pair of visual concept and text is matched. $$\\ell_{match} = \\mathbb{E}_{V,T\\sim D}H(y^{match},p^{match}(V,T))$$ Masked Language Modeling: predict the masked words in the text based on the visual concept $$\\ell_{mlm} = \\mathbb{E}_{t_j\\sim \\hat{T};(V,\\hat{T}\\sim D)}H(y^j, p^j(V,\\hat{T}))$$ Visual Turing test for computer vision system Abstract An operator-assisted device that produces a stochastic sequence of binary questions from a given test image. The query engine proposes a question, the operator either provides correct answer or rejects the question as ambiguous. The system is designed to produce streams of questions that follw natural storylines, from the instantiation of a unique object, through an exploration of its properties, and on to its relationships with other uniquely instantiated objects. Introduction Alan Turing proposed that the ultimate test of a machine could think ot think at least as well as a person was for a human judge to be unable to tell which was which based on natural language conversations in an appropriately cloaked scenario. Or how well a computer can imitate a human. Semantic image interpretation-image to words Research shows that human brain can only think about one idea at a time\nVisual grounding overview TransVG: End-to-End Visual Grounding with Transformers Introduction Two-stage methods measure the similarity between region and expression embedding with an MLP. One-stage methods encode the language vector to visual feature by direct concatenation. Challenges: lead to sub-optimal results on long and complex language expressions. Built on pre-defined structures of language queries or image scenes. Ground object in an indirect way: candidates are sparse region proposals, or dense anchors. So the performance would be influenced by the proposals. ","permalink":"https://x423xu.github.io/posts/2022-04-06-paper-summary-last-weekend/","summary":"2022/3/31 ~ 2022/4/6 Order-Embeddings of images and Language Core idea Explicitly modeling the partial order structure of the hierarchy over language and image -\u0026gt; Visual sematic hierarchy How to do it Penalize order violations $$ E(x,y) = ||max(0,y-x)||^2 $$ where $E(x,y)=0 \\Leftrightarrow x \\preceq y$ Modeling heterogeneous hierarchies with relation-specific hyperbolic cones Core idea Embeds entities into hyperbolic cones \u0026amp; models relations as transformations between the cones How to do it Poincare entailment cone at apex $x$ $$\\zeta_x = {y\\in \\Beta^d | \\angle_xy\\leq\\sin^{-1}(K\\frac{1-||x||^2}{||x||})}$$ embed entity: $h=(h_1,h_2,\\cdots,h_{d})$, where $h_i\\in\\Beta^2$ is the apex of the $i-$th 2D hyperbolic cone.","title":"Paper summary"},{"content":"Remote X11 understanding Suppose we have a local machine (windows/linux), wanna do some deep learning training or data analysis in a remote linux server to . To show images like plt.plot() \u0026amp; plt.show() in local machine we need X11 forwarding which directly renderes images in local machine.\nOk, first step we should connect to a remote linux server from our local machine. Supposing using SSH connection in MobaXterm, we need a private key in local machine and a public key in remote server. Then ssh user@ip -X. The -X here enables X11 forwarding. W.o. -X, any plot command like plt.plot() \u0026amp; plt.show() wouldn\u0026rsquo;t be shown in local machine. Next, what we need to care about is the $DISPLAY variable in remote server. Using echo $DISPLAY in local terminal to check the value. If the -X args is enabled, the it will output something like localhost:10.0, wehre localhost refers to hostname of your computer (I\u0026rsquo;m not sure whether it refers to remote server or local machine). 10 display name, 0 screen name. Generally speaking, a display is composed of a screen, a keyboard and a mouse. Now use xclock in your terminal to test GUI. configure on vscode Install remoet X11 and remote X11 (ssh) Use echo $DISPLAY in vscode terminal to check display value If display value is none, add export DISPLAY=localhost:10.0 to ~/.bashrc and source ~/.bashrc. Open vscode configuration ctrl+shit+p and find out remote X11 settings, set up the display 10, screen 0. Using xclock to test Notice The default X11 forwarding can\u0026rsquo;t render images or videos generated by python gym library. Because, the X11 forwarding only supports OPENGL1.5 and gym requires higher version. To render gym images, it involves a technique named \u0026ldquo;VirtualGL\u0026rdquo;. It is complicated, I need some time to figure it out. Maybe next time.\nConfigure GUI backend for singularity Sometimes although the RemoteX11 is setup for ssh login, the matplotlib in singularity cannot show as expected. A common warning is UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure..\nSolution for remote node:\nFor a remote node, we have to ssh with x11 forwarding, -X. For a interactive node or computation node on remote cluster, we have to apply with arg --x11. To show matplotlib.pyplot -\u0026gt; plt.switch_backend('tkAgg'). The solution only works for X11 forwarding. Some applications require openGL library. Now let\u0026rsquo;s try to sort it out.\nTrying to understand the openGL remote server with openGL is still not solved:frowning:\n众所周知，OpenGL作为图形界的工业标准，其仅仅定义了一组2D和3D图形接口API，而对于窗口管理、IO消息响应等并没有规定。也就是说，OpenGL依赖各平台提供用于渲染的context以及具体实现方式，而各平台提供的实现不尽相同。这些实现主要有：Windows平台下的WGL、Linux下的Mesa/GLX、Mac OS X下的Cocoa/NSGL，以及跨平台的GLUT、GLFW、SDL等等。\nMesa是Linux下的OpenGL实现。它提供了对AMD Radeon系列、Nvidia GPU、Intel i965, i945, i915以及VMWare虚拟GPU等多种硬件驱动的支持，同时也提供了对softpipe等多种软件驱动的支持。Mesa项目由Brian Paul于1993年8月创建，于1995年2月发布了第一个发行版，此后便受到越来越多的关注，如今Mesa已经是任何一个Linux版本首选的OpenGL实现。\nGLX则是在Linux上用于提供GL与窗口交互、窗口管理等等的一组API。它的作用与Windows的WGL、Mac OS X的AGL以及针对OpenGL ES的EGL相似。在Linux上，窗口创建、管理等API遵循X Window接口，而GLX提供了OpenGL与X Window交互的办法。因此GLX也可以运用于其他使用X Window的平台，例如FreeBSD等。\nHowever, OpenGL by itself is not an API, but merely a specification. It is just a description of what exactly the result/output of each function should be and how it should perform. It is already implemented inside your driver, by manufacturers, following the specification. Hence, there is no such thing as “installing” OpenGL. But, we do need to install libraries, that would help us interact with the operating system to access the implementation and set up the windowing system \u0026amp; OpenGL context.\nGLFW\nGLFW is an Open Source, multi-platform library for OpenGL, OpenGL ES and Vulkan development on the desktop OpenGL Setup: GLFW Library Before you start with creating stunning graphics, you need to initialize an OpenGL context and create an application window to draw in. We’ll do this using a popular C library: GLFW(Graphics Library Framework). This library also helps us handle the input from the joystick, keyboard, and mouse. GLEW\nThe OpenGL Extension Wrangler Library (GLEW) is a cross-platform open-source C/C++ extension loading library. GLEW provides efficient run-time mechanisms for determining which OpenGL extensions are supported on the target platform. Xserver\nIn this example you have a local X11-server with two \u0026ldquo;screens\u0026rdquo; on your hostA. Usually there would be only one server with one screen (:0.0), which spans across all your monitors (makes multi-monitor applications way easier). hostB has two X servers, where the second one has no physical display (e.g. virtual framebuffer for VNC). hostC is a headless server without any monitors.\nterminal 1a, 2a, 5a, 6a: If you open a local terminal, and set the display to :0.0 (default) or :0.1, the drawing calls for your graphical programs will be sent to the local X server directly via the memory.\nterminal 1b, 5b: If you ssh onto some server, usually the display will be set automatically to the local X server, if there is one available. Otherwise, it will not be set at all (reason see terminal 3).\nterminal 2b, 6b: If you ssh onto a server, and enable X11-forwarding via the \u0026ldquo;-X\u0026rdquo; parameter, a tunnel is automatically created through the ssh-connection. In this case, TCP Port 6010 (6000+display#) on hostB is forwarding the traffic to Port 6000 (X server #0) on hostA. Usually the first 10 displays are reserved for \u0026ldquo;real\u0026rdquo; servers, therefore ssh remaps display #10 (next user connecting with ssh -X while you\u0026rsquo;re logged in, would then get #11). There is no additional X server started, and permissions for X-server #0 on hostA are handled automatically by ssh.\nterminal 4: If you add a hostname (e.g. localhost) in front of the display/screen#, X11 will also communicate via TCP instead of the memory.\nterminal 3: You can also directly send X11 commands over the network, without setting up a ssh-tunnel first. The main problem here is, that your network/firewall/etc. needs to be configured to allow this (beware X11 is practically not encrypted), and permissions for the X server need to be granted manually (xhosts or Xauthority).\nXorg\nXorg (commonly referred to as simply X) is the most popular display server among Linux users. Its ubiquity has led to making it an ever-present requisite for GUI applications, resulting in massive adoption from most distributions mesa\nSolutions for openGL in remote server First and the most important is the remote server should support openGL. For our HPC server, openGL is not supported, so there is no way rendering any 3D plots in remote server in a terminal way.\nHow to work around it? For graham, it is better to login grad-vdi.computecanada.ca. For other clusters, using Paraview is a good idea.\nFor grad-vdi 1 2 3 4 local: 1. vncviewer 2. grad-vdi.computecanada.ca 3. login and module load stdenv For Paraview 1 2 3 4 5 6 7 8 Remote: 1. module load gcc/9.3.0 paraview-offscreen-gpu/5.10.0 2. unser DISPLAY 3. pvserver local: 1. ssh username@dns -L 11111:gra947:11111 2. paraview 3. `File-\u0026gt;connect-\u0026gt;add server` with port `11111` ","permalink":"https://x423xu.github.io/posts/2022-03-20-x11-for-vscode/","summary":"Remote X11 understanding Suppose we have a local machine (windows/linux), wanna do some deep learning training or data analysis in a remote linux server to . To show images like plt.plot() \u0026amp; plt.show() in local machine we need X11 forwarding which directly renderes images in local machine.\nOk, first step we should connect to a remote linux server from our local machine. Supposing using SSH connection in MobaXterm, we need a private key in local machine and a public key in remote server.","title":"RemoteX11 configuration on vscode"},{"content":"Using Jekyll to create a gitpage on windows Understanding Jekyll, Gem, Bundle, Ruby what is Ruby :hear_no_evil:\nRuby is a dynamic, open source programming language with a focus on simplicity and productivity. It has an elegant syntax that is natural to read and easy to write.\nRuby is most used for building web applications. However, it is a general-purpose language similar to Python, so it has many other applications like data analysis, prototyping, and proof of concepts.\nWhat is a Ruby gem? A gem is a package that you can download \u0026amp; install. When you require an installed gem you’re adding extra functionality to your Ruby program.\nWhat Is Bundler? While learning about Ruby gems you may also read about Bundler. But what is Bundler exactly? Bundler is a tool for dependency management. Didn’t RubyGems already handle this? Well, it does… but only for the gems themselves. Your regular Ruby application isn’t built as a gem, so it doesn’t get this feature. That’s why Bundler exists!\nBundler (and RubyGems since version 2.0) can read Gemfile \u0026amp; install the requested versions of the gems.\ngem 'rails', '~\u0026gt; 5.2.1' gem 'puma', '~\u0026gt; 3.11' TODO:\nwhat is gem what is Bundle What is Jekyll Using jekyll to create a \u0026ldquo;Hello world\u0026rdquo; post Install Ruby, Gem Using Gem to install Jekyll, Bundler, Wdm, Webrick: gem install jekyll bundler wdm webrick Create a directory to deposite your personal website: mkdir docs, cd docs Create Gemfile to list project\u0026rsquo;s dependencies: bundle init Modify Gemfile by adding: gem \u0026quot;jekyll\u0026quot; gem webrick gem 'wdm', '\u0026gt;= 0.1.0' if Gem.win_platform? Create your site by creating an index.html Write something: 1 2 3 4 5 6 7 8 9 \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Home\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Hello World!\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; bundle install to install all gem packages from \u0026ldquo;Gemfile\u0026rdquo;. Create local site: jekyll serve Some notes _drafts folder deposits posts that you are not ready to publish. Once they are ready, move them to _posts folder. To view the drafts, use the command jekyll serve --drafts Add page: change layout to page Add permalink: permalink: /:categories Add defaults front matter to _config.yml: 1 2 3 4 5 6 7 8 defaults: - scope: path: \u0026#34;\u0026#34; #which file to apply to, empty-\u0026gt;all files type: \u0026#34;post\u0026#34; #aplly to all posts values: layout: \u0026#34;post\u0026#34; title: \u0026#34;my title\u0026#34; Change a theme: in Gemfile, add theme: theme-name _layouts folder contains layout html files. The name of the layout is the file name. If wanna use them, just change the layout variable in your front matter. different level layouts embedding. Access variable by {{ variable }} Include: header, footer or navigation: { include header.html } change a theme ","permalink":"https://x423xu.github.io/posts/2022-03-20-jekyll/","summary":"Using Jekyll to create a gitpage on windows Understanding Jekyll, Gem, Bundle, Ruby what is Ruby :hear_no_evil:\nRuby is a dynamic, open source programming language with a focus on simplicity and productivity. It has an elegant syntax that is natural to read and easy to write.\nRuby is most used for building web applications. However, it is a general-purpose language similar to Python, so it has many other applications like data analysis, prototyping, and proof of concepts.","title":"Jekyll"}]