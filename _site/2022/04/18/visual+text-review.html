<h1 id="trends-in-integration-of-vision-and-language-research-a-survey-of-tasks-datasets-and-methods">Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods</h1>
<h2 id="abstract">Abstract</h2>
<blockquote>
  <p>Focus on ten prominent tasks that integrate language and vision by discussing their problem formulation, methods, existing datasets, evaluation measures, and compare results with the SOTA methods.</p>
</blockquote>

<h2 id="introduction">Introduction</h2>
<ul>
  <li>Multimodal learning models:
    <ol>
      <li>generate comprehensible but concise and grammatically well-formed descriptions of the visual content, or vice versa by generating the visual content for a given textual description in a natural language of choice,</li>
      <li>identify objets in the visual content and infer their relationships to reson about, or answer arbitrary questions about them,</li>
      <li>navigate through and environment by leveraging input from both vision and natural language instructions,</li>
      <li>translate textual content from one language to another while leveraging the visual content for sense disambiguation,</li>
      <li>generate stories about the visual content, and so on.</li>
    </ol>
  </li>
  <li>Applications:
    <ol>
      <li>visually impaired individuals can be asisted thereby.</li>
      <li>automatic survelliance</li>
      <li>autonomous driving</li>
      <li>human-computer interaction</li>
      <li>city navigation</li>
    </ol>
  </li>
  <li>Task-specific multimodal models
    <ol>
      <li>image description (Bernardi et al.,2016; Bai &amp; An, 2018; Hossain et al., 2019)</li>
      <li>video description generation (Aafaq et al.,2020)</li>
      <li>visual question answering (Kafle &amp; Kanan, 2017; Wu et al., 2017)</li>
      <li>action recognition (Gella &amp; Keller, 2017)</li>
      <li>visual semantics (Liu et al., 2019)</li>
      <li>NLP natual language generation (Gatt &amp; Krahmer, 2018; Garbacea &amp; Mei, 2020)</li>
      <li>NLP commonsense reasoning (Storks et al., 2019)</li>
      <li>understanding the limitations of the integration of vision and language research (Kafle et al., 2019)</li>
    </ol>
  </li>
</ul>

<hr />
<h2 id="background">Background</h2>
<h3 id="computer-vision-tasks">Computer vision tasks</h3>
<h4 id="image-as-visual-information">Image as visual information</h4>
<ul>
  <li>Utilization
    <ol>
      <li>the tasks where images are used as input</li>
      <li>the representation of images</li>
    </ol>
  </li>
  <li>Tasks
    <ol>
      <li>Image classification</li>
      <li>Object localization</li>
      <li>Object detection</li>
      <li>Object segmentation</li>
      <li>Object Identification</li>
      <li>Instance segmentation</li>
      <li>Panoptic segmentation</li>
    </ol>
  </li>
  <li>Reoresentation
    <blockquote>
      <p>self supervised learning (Jing &amp; Tian, 2019)</p>
    </blockquote>
  </li>
</ul>

<h4 id="video-as-visual-information">Video as visual information</h4>
<ul>
  <li>Utilization
    <ol>
      <li>knowing the tasks where videos are used as inputs</li>
      <li>the representation of a video</li>
    </ol>
  </li>
  <li>Takss
    <ol>
      <li>Object tracking</li>
      <li>Action classification</li>
      <li>Emotion detection</li>
      <li>Scene detection</li>
      <li>Automated editing</li>
    </ol>
  </li>
</ul>

<h3 id="nlp-tasks">NLP tasks</h3>
<ul>
  <li>Tasks
    <ol>
      <li>understanding language</li>
      <li>generating language
        <blockquote>
          <p>Some of the classical NLP tasks, that are used to comprehend language, are shallow parsing, syntax parsing, semantic role labeling, named entity recognition, entity linking, co-reference resolution, etc.</p>
        </blockquote>
      </li>
    </ol>
  </li>
  <li>Representation</li>
</ul>

<h3 id="cv-and-nlp-integration-tasks">CV and NLP integration tasks</h3>
<p><img src="vl_framework.png" width="600" /></p>

<h4 id="extension-of-nlp-tasks">Extension of NLP tasks</h4>
<ul>
  <li>Visual Description Generation: The goal is to generate a human-readable text snippt that describes the input</li>
  <li>Visual Storytelling: A sequence of visual inputs is used to generate a narrative summary based on text aligned with them</li>
  <li>Visual Question Answering: Answer questions about a visual input.</li>
  <li>Visual Dialog: Aim at creating a meaningful dislog in a natural and conversational language about a visual content.</li>
  <li>Visual Reeferring Expression:</li>
  <li>Visual Entailment: An inference task for predicting whether the image semantically entails the text.</li>
  <li>Multimodal Machine Traslation: Translate from source language to target language by leveraging the visual information as auxiliary modality along with the natural language text in source language.</li>
</ul>

<h4 id="extension-of-cv-tasks">Extension of CV tasks</h4>
<ul>
  <li>Visual generation: generate visual content by conditioning on input text from a chosen natural language.</li>
  <li>Visual resoning: It is expected to output a relationship between detected objects by generating an entire visual scene graph. The scene graph is leverage to reason and answer questions about visual content. It can also be used to reason about whether a natural language statement is true or not regarding a visual input.</li>
</ul>

<h4 id="extension-of-both-nlp-and-cv-tasks">Extension of both NLP and CV tasks</h4>
<ul>
  <li>Vision-and-Language Navigation: natural language navigation should be interpreted based on visual input. Combine both vision and language.</li>
</ul>

<hr />
<h2 id="visual-description-generation-and-storytelling">Visual Description Generation and Storytelling</h2>
<blockquote>
  <p>generate a textual description when conditioned on visual input</p>
</blockquote>

<h3 id="visual-description-generation">Visual description generation</h3>
<blockquote>
  <p>Aim to generate either a global descriptionor dense captions for a given visual input.</p>
  <h4 id="image-description-generation">Image description generation</h4>
  <ul>
    <li>Standard image description generation: generate a sentence-level description of the scene in a given image</li>
    <li>Dense image description generation: create descriptions at the local object-level in a given image.</li>
    <li>Image paragraph generation: create paragraphs instead of generating a single simple description. Generated paragraphs are expected to be coherent and contain fine-grained natural language descriptions.</li>
    <li>Spoken language image description generation: expand the description generation task to work with spoken language, instead o flimiting to only the written forms of language.</li>
    <li>Stylistic image description generation: add style to the standard image description generation, where the generated descriptions adhere to a specific style.</li>
    <li>Unseen objects image description generation: leverage images which lack paired descriptions. Generate descriptions for visual object categories previously unseen in image-description corpora.</li>
    <li>Diverse image description generation: incorporate variety and diversity in the generated captions.</li>
    <li>Controllable image description generation: select specific objects in an image, defined by a control signal, to generate descriptions.</li>
    <li>Image caption emendation as generation: build a model to emend both syntactic and semantic errors in the captions.
      <h4 id="video-description-generation">Video description generation</h4>
    </li>
    <li>Global description generation:
      <ol>
        <li>ground sentences that describe actions in the visual information extracted from videos (Motwani &amp; Mooney, 2012; Regneri et al., 2013)</li>
        <li>Generate global natural language descriptions for videos with various approaches: <em>leveraging latent topics</em> (Das et al., 2013), <em>corpora knowledge</em> (Krishnamoorthy et al., 2013), <em>graphical models</em> (Rohrbach et al., 2013), and <em>sequence-to-sequence</em> learning (Venugopalan et al., 2015b, 2015a; Donahue et al., 2015; Srivastava et al., 2015; Xu et al., 2016; Ramanishka et al., 2016; Jin et al., 2016), <em>factor graph</em>(Thomason et al., 2014) combines visual detection with language statistics.</li>
        <li>Seq2seeq based approaches: extra corpora (Venugopalan et al., 2016), soft-attention (Yao et al., 2015), multimodal fusion (Hori et al., 2017), temporal attention (Song et al., 2017), semantic consistency (Gao et al., 2017), residual connections(Li et al., 2019). Incorporation of semantic attributes learned from videos, ensembled-based description generator networks (Shetty et al., 2018), encoder-decoder reconstructors (Wang et al., 2018).</li>
        <li>Other approaches: combined with entailment generation task (Pasunuru &amp; Bansal, 2017a), multiple fine-grained actions (Wang et al., 2018b), reinforcement learning (Pasunuru &amp; Bansal, 2017b), Visual Text correction system (Mazaheri &amp; Shah, 2018), object relational graph baed encoder, language model decoder (Zhang et al., 2020).</li>
      </ol>
    </li>
  </ul>
</blockquote>

<ul>
  <li>Dense video description generation:
    <blockquote>
      <p>achieve fine-grained video understanding by addressing two sub-problems: (1) localizing events in a video, and (2) generating captions for these localized events. the core challenge, namely the automatic evaluation of video captioning, is still unsolved.</p>
    </blockquote>
  </li>
  <li>Movie description generation: input movie clips, align books to movies (Tapaswi et al., 2015; Zhu et al., 2015), movie descriptions (Rohrbach et al., 2015).</li>
</ul>

<h3 id="visual-storytelling">Visual storytelling</h3>
<blockquote>
  <p>The task of visual storytelling aims to encode a sequence of images or frames (in the video) to generate a paragraph which is story-like.</p>
  <h4 id="image-storytelling">Image storytelling</h4>
  <p>The aim of image storytelling is to generate stories from a sequence of images.</p>
  <ul>
    <li>semantic coherence is captured in a photo stream.</li>
    <li>discover semantic embeddings correlations (Yu et al., 2017), incorporated with reinforcement learning (Wang et al., 2018), hierarchically structured reinforced training (Huang et al., 2019), adversarial reward learning Wang et al. (2018a).</li>
    <li>suffer from <em>repetitiveness</em>, the same objects/events undermine a good story structure. -&gt; inter-sentence diversity was explored with diverse beam search (Hsu et al., 2018).
      <h4 id="video-storytelling-less-explored">Video storytelling (less explored)</h4>
      <p>In comparison to image storytelling, which only deals with a small sequence of images, the aim of video storytelling is to generate coherent and succinct stories for long videos.</p>
    </li>
    <li>Pinoeer Li et al. (2020) address challenges like diversity in the story and the inherent complexity of video.</li>
    <li>goal: offer support to people with visual disabilities or technical issues like internet bandwidth limitations.</li>
  </ul>
</blockquote>

<hr />
<h2 id="visual-referring-expression-comprehension-and-generation">Visual Referring Expression Comprehension and Generation</h2>
<blockquote>
  <p>The objective of the task is to ground a natural language expression (e.g. a noun phrase or a longer piece of text) to objects in a visual input.</p>
  <h3 id="image-referring-expression-comprehension-and-generation">Image referring expression comprehension and generation</h3>
  <p>In a natural environment, people use referring expressions to unambiguously identify, indicate, or point to particular objects. This is usually done with a simple phrase or within a larger context (e.g. a sentence). Having  a larger context provides better scope for avoiding ambiguity and allows the referential expression to easily map to the target object. However, there can also be other possibilities in which people are asked to describe a target object based on its surrounding objects.</p>
  <ul>
    <li>used to generate, an algorithm generates a referring expression for a given target object which is present in a visual scene
      <ul>
        <li>approaches: (FitzGerald et al., 2013) tackled the problem from the perspective of density estimation, learn distributions over logical exprssions identifying sets of objects in the world.</li>
      </ul>
    </li>
    <li>used to perform comprehension, an algorithm locates in an image the object described by a given referring expression.
      <ul>
        <li>approaches: Nagaraja et al. (2016) integrate contexts between objects. Multiple Instance Learning (MIL). Hu et al. (2016) leverage a NLP query of the object to localize a target object, integrating spatial configurations and global scene-level contextual information. (Yu et al., 2018) subject appearence, location, and relationship to other objects. (Cirik et al., 2018a) syntactic analysis, build a dynamic computation graph. variational model (Zhang et al., 2018)</li>
        <li>cross-modal: (Yang et al., 2019) cross-modal relationship inference， 1. highlight objects and relationships connected with a referring, 2. multi-modal semantic contexts. (可以将空间的referring用到depth estimation里面吗)。Recursive Grounding Tree (Hong et al., 2019) binary tree to parse referring expression-&gt; visual reasoning. (Liu et al., 2019), combining visual reasoning with referential expressions. object segmentation based referring expression (Liu et al., 2017).</li>
      </ul>
    </li>
  </ul>
</blockquote>

<h3 id="image-referring-expression-comprehension-and-generation-combination">Image referring expression comprehension and generation combination</h3>
<ul>
  <li>(Mao et al., 2016; Yu et al., 2016) find visual comparison to other objects within an image. <strong>(Yu et al., 2017a)</strong> a speaker, a listener, and a reinforcer. The speaker generate referring expressions, the listner comprehend referring expressions, the reinforce use a reward function to guide sampling of more discriminative expressions.</li>
</ul>

<h3 id="video-referring-expression-comprehension-and-generation">Video referring expression comprehension and generation</h3>
<ul>
  <li>Vasudevan et al. (2018) stereo videos to explore temporal-spatial contextual information. Khoreva et al. (2018) language referring expressions to achieve object segmentation. Wang et al. (2020) video grounding with contextual information.</li>
</ul>

<hr />
<h2 id="visual-question-answering-reasoning-and-entailment">Visual question answering, reasoning, and entailment</h2>
<blockquote>
  <p>they share the common intention of answering questions when conditioned on a visual input</p>
</blockquote>

<h3 id="visual-question-answering">Visual question answering</h3>
<blockquote>
  <p>The goal of Visual Question Answering (VQA) is to learn a model that comprehends visual content at both the global and local level for finding an association with pairs of questions and answers in the natural language form.</p>
  <ul>
    <li>Image Q&amp;A as Visual Turing Test (Malinowski &amp; Fritz, 2014; Malinowski et al., 2015; Geman et al., 2015). fill-in-the-blank tasks (Yu et al., 2015), multiple-choice question-answering for images. Address open-ended Image Q&amp;A (Antol et al., 2015; Agrawal et al., 2017), ask free-form natural language question. Binary image Q&amp;A (Zhang et al., 2016). Relate local regions in the images (Zhu et al., 2016) by addressing object-level grounding</li>
    <li>Interpretability or explainability by overcoming priors (Agrawal et al., 2018). Generate human-interpretable rules that provide better insights <strong>(Manjunatha et al., 2019)</strong>. cycle-consistency (Shah et al., 2019a). outside knowledge (Marino et al., 2019)(Shah et al., 2019b).</li>
    <li>Multi-task learning, federated learning.</li>
  </ul>
</blockquote>

<h3 id="video-question-answering">Video question answering</h3>
<blockquote>
  <p>is to answer natural language questions about videos.</p>
  <ul>
    <li>(Tu et al., 2014) jointly parsing videos with corresponding text to answer queries. open-ended movie Q&amp;A (Tapaswi et al., 2016). fill-in-the-blank questions (Zhu et al., 2017; Mazaheri et al., 2017). (Zeng et al., 2017) free-form Q&amp;A. High-level concept words (Yu et al., 2017b). Attention (Jang et al., 2017). <strong>spatio-temporal grounding (Lei et al., 2020)</strong></li>
    <li>STAGE (Lei et al., 2020), aligned fusion is essential for improving Video Q&amp;A.</li>
  </ul>
</blockquote>

<h3 id="visual-reasoning">Visual reasoning</h3>
<blockquote>
  <p>is to learn a model that comprehends the visual content by reasoning about it.</p>
</blockquote>

<h4 id="image-reasoning">Image reasoning</h4>
<blockquote>
  <p>s to answer sophisticated queries by reasoning about the visual world.</p>
  <ul>
    <li>(Johnson et al., 2017a) design diagnostic tests going beyond benchmarks. VQA struggle with comparing the attributes of objects or novel attribute combinations. (Johnson et al., 2017b) program generator. (Hu et al., 2017) predict instance-specific network layouts. Santoro et al. (2017) relation-aware visual features. (Cao et al., 2018) global context reasoning.</li>
    <li><strong>Mascharka et al. (2018) proposed a set of visual-reasoning primitives.</strong> Learning-By-Asking (LBA) (Misra et al., 2018b), mimic natural learning with the goal to make it more data efficient. compositional attention networks (Hudson &amp; Manning, 2018) explicit and expressive reasoning.</li>
    <li><strong>neural-symbolic visual question answering (Yi et al., 2018)</strong>, recover structural scene representation from the image and a program trace from the question. <strong>Neuro-Symbolic Concept Learner (NS-CL) (Mao et al., 2019)</strong> learns visual concepts, word, and semantic parsing of sentences without explicit supervision. It learns by simply looking at images and reading paired questions and answers. multimodal relation network (Cadène et al., 2019) learn end-to-end reasoning over real images. Aditya et al. (2019) used spatial knowledge to aid visual reasoning, knowledge distillation, relational resoning, probabilistic logical languages. Explainable and explicit neurla modules (Shi et al., 2019) scene graph.</li>
    <li><strong>Andreas et al. (2016a, 2016b)</strong> exploit the compositional linguistic structure of complex questions by forming  neural module networks which query about the abstract shapes observed in an image. Compositional question answering (Hudson &amp; Manning, 2019). Zellers et al. (2019) commonsense knowledge. NLVR (Suhr et al., 2017) and NLVR2 (Suhr et al., 2019) true or false.</li>
  </ul>
</blockquote>

<h4 id="video-reasoning">Video reasoning</h4>
<blockquote>
  <p>The goal of COG, Yang et al. (2018), is to address problems related to visual and logical reasoning and memory</p>
</blockquote>

<h3 id="visual-entailment">Visual entailment</h3>
<blockquote>
  <p>is to learn a model that predicts whether the visual content entails the augmented text along with hypothesis.</p>
  <h4 id="image-entailment">Image entailment</h4>
  <ul>
    <li>Vu et al. (2018), a visually grounded version of the textual entailment where an image is augmented with textual premise and hypothesis. Xie et al. (2019) predicts whether the image semantically entails the text, given image-sentence pairs, the premise is defined by an image.
      <h4 id="video-entailment">Video entailment</h4>
      <p>(Liu et al., 2020) to infer whether the natural language hypothesis is entailed or contradicted when given a video clip aligned with the subtitles information</p>
    </li>
  </ul>
</blockquote>

<hr />
<h2 id="visual-dialog">Visual Dialog</h2>
<blockquote>
  <p>involves a complex interaction between a human and an artificial agent.</p>
  <h3 id="image-dialog">Image Dialog</h3>
  <p>is to create AI agents that can hold dialog with humans in a natural language of choice about a visual content (Das et al., 2017a), represented by an image. To be more specific, given an image, a history of dialogs, and a question about the image, the goal of an AI agent is to ground the question in the image, infer the context from the history, and then answer the question accurately</p>
  <ul>
    <li>(de Vries et al., 2017) locate an unkown object in the image by asking a swquence of questions. (Mostafazadeh et al., 2017). hold natural-sounding conversations about a shared image.</li>
    <li>(Lu et al., 2017a) transfer from dialog generation. Seo et al. (2017) attentive memory. (Wu et al., 2018) reinforcement learning and GAN. (Kottur et al., 2018) form explicit and grounded co-reference between nouns and pronouns. (Niu et al., 2019) recursive visual attention. (Zheng et al., 2019) graphical model inference. Guo et al. (2019) builds an image-question-answer synergistic network. (Shekhar et al., 2019) a visually grounded encoder guessing and asking questions.
      <h3 id="video-dialog">Video dialog</h3>
      <p>is to leverage scene information containing both audio (which can be transcribed as subtitles) and visual frames to hold a dialog (i.e., an exchange) with humans in a natural language of choice about the multimedia content (Alamri et al., 2019b 2019a).</p>
    </li>
    <li>multimodal-based video description (Hori et al., 2019).</li>
  </ul>
</blockquote>

<hr />
<h2 id="multimodal-machine-translation-mmt">Multimodal Machine Translation (MMT)</h2>
<blockquote>
  <p>is to translate natural language sentences that describe visual content (e.g. image) in a source language into a target language by taking the visual content as an additional input to the source language sentences.</p>
  <h3 id="machine-tanslation-with-image">Machine tanslation with image</h3>
  <p>is to translate sentences, that describe an image, in a source language into equivalent sentences in a target language.</p>
  <ul>
    <li>single source translation, multisource MMT.</li>
    <li>multimodal attention (Huang et al., 2016). doubly-attentive decoder incorporated visual features (Calixto et al., 2017). learning to translate, and learning visually grounded representations (Elliott &amp; Kádár, 2017). noisy image captions for MMT (Schamoni et al., 2018).
      <h3 id="machine-translation-with-video">Machine translation with video</h3>
      <p>(Wang et al., 2019b) is to translate a source language description into the target language equivalent using the video information as additional spatio-temporal context.</p>
    </li>
  </ul>
</blockquote>

<hr />
<h2 id="language-to-vision-generation">Language-to-Vision Generation</h2>
<blockquote>
  <p>is to generate visual content given their natural language descriptions.</p>
  <h3 id="language-to-image-generation">Language-to-Image Generation</h3>
  <ul>
    <li>sentence-level language-to-image generation: generate images cnditioned on the natural language descriptions. (Mansimov et al., 2016) iteratively draw patches on a canvas, attending to the relavant words in the description. (Reed et al., 2016b) visual concepts could be translated from characters to pixels with conditional gan. (Reed et al., 2016a) what content should be drawn in which location for high quality image generation. (Nguyen et al., 2017) conditioned on image classes. (Dash et al., 2017) condition on both sentence and class information. stackGAN (Zhang et al., 2017, 2019). attention-based GAN (Xu et al., 2018)</li>
    <li>(Hong et al., 2018) infer the semantic layout of the image. Johnson et al. (2018) used image-specific scene graphs enabling explicitly reasoning about objects and their relationships.
      <h4 id="image-manipulation">Image manipulation</h4>
    </li>
    <li>TAGAN (Nam et al., 2018) generate semantically manipulated images while preserving text-irrelevant contents. Only regions correspond to the given text are modified. (Zhu et al., 2019) attention generator, discriminator. (Li et al., 2020) designed error correction modules to rectify mismatched attributes and complete the missing contents.
      <h4 id="fine-grained-image-generation">Fine-grained image generation</h4>
    </li>
    <li>(El-Nouby et al., 2018) recurrent image generation, output up to the current step &amp; past instructions. (Hinz et al., 2019) control location of objects by adding a pathway in an iterative manner.
      <h4 id="sequential-image-generation">Sequential image generation</h4>
    </li>
    <li>StoryGAN (Li et al., 2019b) opposite to storytelling.</li>
  </ul>
</blockquote>

<h3 id="language-to-video-generation">Language-to-Video generation</h3>
<ul>
  <li>(Li et al., 2018) conditional generative model</li>
</ul>

<hr />
<h2 id="vision-and-language-navigation">Vision-and-Language Navigation</h2>
<blockquote>
  <p>is to carry out navigation in an environment by interpreting natural language instructions</p>
  <h3 id="image-and-language-navigation">Image-and-Language Navigation</h3>
  <ul>
    <li>(Anderson et al., 2018b) an autonomous agent navigate in an environment by interpreting natural language instructions. (Wang et al., 2019a), reinforced cross-modal matching. (Fried et al., 2018) train an action space with an embedded speaker model.</li>
    <li>Embodied Question Answering (Das et al., 2018a, 2018b). interactive question answering (Gordon et al., 2018). grounded dialog (de Vries et al., 2018)</li>
  </ul>
</blockquote>

<hr />
<h2 id="vison-and-language-pretraining">Vison-and-Language pretraining</h2>
<blockquote>
  <p>To jointly learn representations using both visual and textual content</p>
  <h3 id="single-stream-architectures">Single-stream architectures</h3>
  <ul>
    <li>BERT-like (Devlin et al., 2019). VideoBERT (Sun et al., 2019). Bounding Boxes in Text Transformer (B2T2) (Alberti et al., 2019). Unicoder-VL (Li et al., 2020). VL-BERT (Su et al., 2020), VLP (Zhou et al., 2020), OSCAR (Li et al., 202), VinVL (Zhang et al., 2021) can jointly understand and generate from cross-modal data. (Cao et al., 2020) probe.
      <h3 id="two-stream-architectures">Two-stream architectures</h3>
    </li>
    <li>two independent encoders for learning visual and text representations. ViLBERT (Lu et al., 2019) and LXMERT (Tan &amp; Bansal, 2019).</li>
  </ul>
</blockquote>

<p>-neuro-symbolic reasoning systems (Yi et al., 2018; Vedantam et al., 2019).</p>
