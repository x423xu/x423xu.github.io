<h1 id="2022331--202246">2022/3/31 ~ 2022/4/6</h1>
<h2 id="order-embeddings-of-images-and-language">Order-Embeddings of images and Language</h2>
<h3 id="core-idea">Core idea</h3>
<ul>
  <li>Explicitly modeling the partial order structure of the hierarchy over language and image -&gt; <strong>Visual sematic hierarchy</strong></li>
</ul>

<h3 id="how-to-do-it">How to do it</h3>
<ul>
  <li>Penalize order violations
\(E(x,y) = ||max(0,y-x)||^2\)
where $E(x,y)=0 \Leftrightarrow x \preceq y$</li>
</ul>

<h2 id="modeling-heterogeneous-hierarchies-with-relation-specific-hyperbolic-cones">Modeling heterogeneous hierarchies with relation-specific hyperbolic cones</h2>
<h3 id="core-idea-1">Core idea</h3>
<ul>
  <li>Embeds entities into hyperbolic cones &amp; models relations as transformations between the cones</li>
</ul>

<h3 id="how-to-do-it-1">How to do it</h3>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Poincare entailment cone at apex $x$ $$\zeta_x = {y\in \Beta^d</td>
          <td>\angle_xy\leq\sin^{-1}(K\frac{1-</td>
          <td> </td>
          <td>x</td>
          <td> </td>
          <td>^2}{</td>
          <td> </td>
          <td>x</td>
          <td> </td>
          <td>})}$$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>embed entity: $h=(h_1,h_2,\cdots,h_{d})$, where $h_i\in\Beta^2$ is the apex of the $i-$th 2D hyperbolic cone.</li>
  <li>Transformation:
    <ol>
      <li>Rotation transformation: $f_1(h,r) = \exp_o(\mathbf{G}(\theta)\log_o(h))$, where $\mathbf{G(\theta)=\left[\begin{matrix}cos(\theta)&amp;-sin(\theta)\sin(\theta)&amp;cos(\theta)\end{matrix}\right]}$</li>
      <li>Restricted rotation transformation:$f_2(h,r)=\exp_h(s\cdot \mathbf{G}(\theta\frac{\phi}{\pi})\bar h), r=(s,\theta)$</li>
    </ol>
  </li>
  <li>Loss function:
    <ol>
      <li>
        <table>
          <tbody>
            <tr>
              <td>$\ell_d=-\log\sigma(\psi(h,r,t))-\sum_{t^{‘}}\frac{1}{</td>
              <td>T</td>
              <td>}\log\sigma(-\psi(h,r,t^{‘}))$</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>$\ell_a(h,r,t)=m\cdot(max(0,\angle_{h_i}t_i-\phi(h_i)))$</li>
    </ol>
  </li>
</ul>

<h2 id="learning-to-detect-unseen-object-classes-by-between-class-attribute-transfer">Learning to detect unseen object classes by between-class attribute transfer</h2>
<h3 id="core-idea-2">Core idea</h3>
<ul>
  <li>Decompose an image/object into high-level semantic attributes such as shape, color, geographic information. Then detect new classes based on their attribute representations.</li>
</ul>

<h3 id="how-to-do-it-2">How to do it</h3>
<ul>
  <li>Direct attribute prediction:
    <ol>
      <li>decouple training images into attribute variables $\alpha_m$</li>
      <li>per-attribute parameters $\beta_m$</li>
      <li>at test: image $x\rightarrow\beta_m\rightarrow \alpha_m\rightarrow class$
 \(p(z|x)=\sum_{a\in\{0,1\}^M}p(z|a)p(a|x) = \frac{p(z)}{p(a^z)}\prod_{m=1}^{M}p(a_m^z|x)\)</li>
    </ol>
  </li>
  <li>Indirect attribute prediction
    <ol>
      <li>add an attribute layer between two label layers</li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>for the indirect one, $p(a_m</td>
              <td>x)=\sum_{k=1}^K p(a_m</td>
              <td>y_k)p(y_k</td>
              <td>x)$</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ol>
  </li>
</ul>

<h2 id="zero-shot-learning-via-joint-similarity-embedding">Zero-shot learning via joint similarity embedding</h2>

<h3 id="core-idea-3">Core idea</h3>
<ul>
  <li>To test if source-target pair matches each other</li>
</ul>

<h2 id="multi-grained-vision-language-pre-training-aligning-texts-with-visual-concepts">Multi-grained vision language pre-training: aligning texts with visual concepts</h2>
<h3 id="core-idea-4">Core idea</h3>
<ul>
  <li>Locate visual concepts in the image given the associated texts &amp; align the texts with the visual concepts.</li>
</ul>

<h3 id="how-to-do-it-3">How to do it</h3>
<ul>
  <li>Bounding box loss:
\(\ell_{bbox} = \mathbb{E}_{(V^j,T^j)\sim I;I\sim D}\left[\ell_{iou}(b_j,\hat{b}_j)+||b_j-\hat{b}_j||_1\right]\)</li>
  <li>Contrastive learning:
\(p^{v2t}(V) = \frac{\exp(s(V,t)/\tau)}{\sum_{i=1}^{N}\exp(s(V,T^i)/\tau)}\)
\(p^{t2v}(T) = \frac{\exp(s(V,t)/\tau)}{\sum_{i=1}^{N}\exp(s(V^i,T)/\tau)}\)
\(\ell_{cl}=\frac{1}{2}\mathbb{E}_{V,T\sim D}\left[H(y^{v2t}(V),p^{v2t}(V))+H(y^{t2v}(T),p^{t2v}(T))\right]\)</li>
  <li>Match prediction: whether a pair of visual concept and text is matched.
\(\ell_{match} = \mathbb{E}_{V,T\sim D}H(y^{match},p^{match}(V,T))\)</li>
  <li>Masked Language Modeling: predict the masked words in the text based on the visual concept
\(\ell_{mlm} = \mathbb{E}_{t_j\sim \hat{T};(V,\hat{T}\sim D)}H(y^j, p^j(V,\hat{T}))\)</li>
</ul>
